{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN심화.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtcykqFoGSvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26056cfa-422b-4512-bfe0-16de7108bf78"
      },
      "source": [
        "# gdrive에 mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tUkhqtbGqMR"
      },
      "source": [
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/tobigs16_강의자료/9주차/정규세션_CNN심화')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWaSs_ZXGqku"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder \n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Pytorch --> CNN\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchsummary import summary "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upd3wp1mGsI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e82da0a-3b73-4c05-fcd7-b72a871f4814"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device('cuda')\n",
        "else:\n",
        "  DEVICE = torch.device('cpu')\n",
        "print(DEVICE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVVjiMe3Gtcp"
      },
      "source": [
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/tobigs16_강의자료/9주차/정규세션_CNN심화/data/'                   # 기존의 데이터\n",
        "classes_list = os.listdir(original_dataset_dir) \n",
        " \n",
        "base_dir = '/splitted'                           # train-validation-test 나누기\n",
        "os.makedirs(base_dir,exist_ok=True)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') \n",
        "os.makedirs(train_dir,exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.makedirs(validation_dir,exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir,exist_ok=True)\n",
        "\n",
        "for cls in classes_list:     \n",
        "    os.makedirs(os.path.join(train_dir, cls),exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_dir, cls),exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, cls),exist_ok=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxS-TgrMGvhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a89729b-0c66-45a2-f7cb-bbba5d90a7a0"
      },
      "source": [
        "## 데이터 분할 후 클래스별 데이터 수 확인\n",
        "\n",
        "import math\n",
        " \n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        " \n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size( dog ):  197\n",
            "Validation size( dog ):  65\n",
            "Test size( dog ):  65\n",
            "Train size( horse ):  90\n",
            "Validation size( horse ):  30\n",
            "Test size( horse ):  30\n",
            "Train size( house ):  147\n",
            "Validation size( house ):  49\n",
            "Test size( house ):  49\n",
            "Train size( person ):  239\n",
            "Validation size( person ):  79\n",
            "Test size( person ):  79\n",
            "Train size( giraffe ):  141\n",
            "Validation size( giraffe ):  47\n",
            "Test size( giraffe ):  47\n",
            "Train size( guitar ):  80\n",
            "Validation size( guitar ):  26\n",
            "Test size( guitar ):  26\n",
            "Train size( elephant ):  123\n",
            "Validation size( elephant ):  41\n",
            "Test size( elephant ):  41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDX_4JY7Gy3X"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCH = 30 "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ao01wYlfbPQ"
      },
      "source": [
        "transform_base = transforms.Compose([transforms.ToTensor()]) \n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-3md6CGzeo"
      },
      "source": [
        "'''transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()]) \n",
        "basic_ = '/content/drive/MyDrive/tobigs16_강의자료/9주차/정규세션_CNN심화'\n",
        "train_dataset = ImageFolder(root=basic_+'/splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root=basic_+'/splitted/val', transform=transform_base)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RZcF6AaG1Rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3f5bcc-6921-458c-bcf7-b8a661ad063f"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ71-FEuIQnd"
      },
      "source": [
        "Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogzq919tRTtM",
        "outputId": "6dc17bd0-2dbe-44e1-f80a-31c940daa126"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes = 7):\n",
        "        super(Net, self).__init__()\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, padding=0, stride=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \n",
        "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \n",
        "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fclayer = nn.Sequential(\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.convnet(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fclayer(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "model_base = Net().to(DEVICE)\n",
        "print(model_base)\n",
        "summary(model_base, (3, 227, 227)) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (convnet): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fclayer): Sequential(\n",
            "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
            "              ReLU-2           [-1, 96, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
            "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
            "              ReLU-5          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
            "             ReLU-10          [-1, 384, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "           Linear-14                 [-1, 4096]      37,752,832\n",
            "             ReLU-15                 [-1, 4096]               0\n",
            "          Dropout-16                 [-1, 4096]               0\n",
            "           Linear-17                 [-1, 4096]      16,781,312\n",
            "             ReLU-18                 [-1, 4096]               0\n",
            "          Dropout-19                 [-1, 4096]               0\n",
            "           Linear-20                    [-1, 7]          28,679\n",
            "================================================================\n",
            "Total params: 58,310,023\n",
            "Trainable params: 58,310,023\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.59\n",
            "Forward/backward pass size (MB): 11.04\n",
            "Params size (MB): 222.44\n",
            "Estimated Total Size (MB): 234.07\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkeg1mA1G4Lh"
      },
      "source": [
        "# Optimizer, Loss function 은 편하신 대로 변경하시면 됩니다 :) \n",
        "\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001) \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGlQr3LqG5dj"
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()                         # 모델 train 상태로\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)   # data, target 값 DEVICE에 할당\n",
        "        optimizer.zero_grad()                                             # optimizer gradient 값 초기화\n",
        "        output = model(data)                                              # 할당된 데이터로 output 계산\n",
        "        loss = criterion(output,target)                                               # Cross Entropy Loss 사용해 loss 계산\n",
        "        loss.backward()                                               # 계산된 loss back propagation\n",
        "        optimizer.step()                                    # parameter update"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOV4W-0kG7F3"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()      # 모델 평가 상태로\n",
        "    test_loss = 0     # test_loss 초기화\n",
        "    correct = 0       # 맞게 예측한 0 값으로 초기화\n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)     # data, target DEVICE에 할당\n",
        "            output = model(data)                                                   # output 계산\n",
        "            test_loss += criterion(output, target).item()         # loss 계산(총 loss 에 더해주기)\n",
        "            pred = output.max(1, keepdim=True)[1]                 # 계산된 벡터값 중 가장 큰 값 가지는 class 예측\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # 맞게 예측한 값 세기\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset)                         # 평균 loss\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)     # test(validation) 데이터 정확도\n",
        "    return test_loss, test_accuracy  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykWQo5DdILxx"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIVPLcIpG8p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cd6aa4-5abf-49de-c90b-3b0d231c8f7f"
      },
      "source": [
        "import time\n",
        "import copy\n",
        " \n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  # beset accuracy 초기화\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()                                     # 학습 시간 계산\n",
        "        train(model, train_loader, optimizer)                   # train 데이터로 학습\n",
        "        train_loss, train_acc = evaluate(model, train_loader)   # train_loss, train_acc 계산\n",
        "        val_loss, val_acc = evaluate(model, val_loader)         # valid_loss, valid_acc 계산\n",
        "        \n",
        "        if val_acc>best_acc:  # update best accuracy\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since # 학습 시간 출력\n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "\n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model\n",
        "\n",
        "base = train_baseline(model_base ,train_loader, val_loader, optimizer)  \t# 모델 학습시키기\n",
        "torch.save(base,'AlexNet.pt')                                             # 모델 저장"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.0299, Accuracy: 23.50%\n",
            "val Loss: 0.0336, Accuracy: 23.44%\n",
            "Completed in 1m 2s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.0296, Accuracy: 23.50%\n",
            "val Loss: 0.0336, Accuracy: 23.44%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.0297, Accuracy: 21.14%\n",
            "val Loss: 0.0337, Accuracy: 20.77%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.0286, Accuracy: 28.32%\n",
            "val Loss: 0.0330, Accuracy: 27.89%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.0296, Accuracy: 21.73%\n",
            "val Loss: 0.0333, Accuracy: 20.77%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.0298, Accuracy: 23.50%\n",
            "val Loss: 0.0353, Accuracy: 23.44%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.0290, Accuracy: 26.25%\n",
            "val Loss: 0.0330, Accuracy: 25.82%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.0280, Accuracy: 28.71%\n",
            "val Loss: 0.0326, Accuracy: 28.19%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.0279, Accuracy: 30.19%\n",
            "val Loss: 0.0342, Accuracy: 31.45%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.0284, Accuracy: 27.53%\n",
            "val Loss: 0.0329, Accuracy: 22.85%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.0271, Accuracy: 31.17%\n",
            "val Loss: 0.0320, Accuracy: 28.19%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.0268, Accuracy: 32.84%\n",
            "val Loss: 0.0331, Accuracy: 27.30%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.0265, Accuracy: 33.33%\n",
            "val Loss: 0.0326, Accuracy: 30.56%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.0257, Accuracy: 35.50%\n",
            "val Loss: 0.0320, Accuracy: 30.86%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.0260, Accuracy: 35.89%\n",
            "val Loss: 0.0339, Accuracy: 33.83%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.0256, Accuracy: 37.46%\n",
            "val Loss: 0.0322, Accuracy: 27.60%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.0247, Accuracy: 37.46%\n",
            "val Loss: 0.0320, Accuracy: 32.34%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.0247, Accuracy: 39.72%\n",
            "val Loss: 0.0319, Accuracy: 29.38%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.0234, Accuracy: 41.89%\n",
            "val Loss: 0.0338, Accuracy: 32.34%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.0233, Accuracy: 42.08%\n",
            "val Loss: 0.0361, Accuracy: 32.05%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.0232, Accuracy: 41.99%\n",
            "val Loss: 0.0352, Accuracy: 28.78%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.0220, Accuracy: 46.51%\n",
            "val Loss: 0.0363, Accuracy: 33.53%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0202, Accuracy: 50.05%\n",
            "val Loss: 0.0353, Accuracy: 31.45%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0199, Accuracy: 52.61%\n",
            "val Loss: 0.0344, Accuracy: 31.75%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0186, Accuracy: 57.23%\n",
            "val Loss: 0.0373, Accuracy: 31.75%\n",
            "Completed in 0m 9s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0183, Accuracy: 56.83%\n",
            "val Loss: 0.0350, Accuracy: 31.16%\n",
            "Completed in 0m 10s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0170, Accuracy: 59.59%\n",
            "val Loss: 0.0417, Accuracy: 32.34%\n",
            "Completed in 0m 10s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0162, Accuracy: 59.49%\n",
            "val Loss: 0.0435, Accuracy: 31.75%\n",
            "Completed in 0m 10s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0147, Accuracy: 66.57%\n",
            "val Loss: 0.0442, Accuracy: 31.16%\n",
            "Completed in 0m 10s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0133, Accuracy: 71.39%\n",
            "val Loss: 0.0413, Accuracy: 33.83%\n",
            "Completed in 0m 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GlfgvWvIHaO"
      },
      "source": [
        "Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJM-NC5mG-Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7557da-5d42-435f-ecaa-428d7114036e"
      },
      "source": [
        "# Test Data Classification 하기\n",
        "transform_base = transforms.Compose([transforms.ToTensor()]) \n",
        "test_base = ImageFolder(root='./splitted/test',transform=transform_base)  \n",
        "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd655OpGlpck",
        "outputId": "b3d0bf37-3d75-4fac-c7a3-cce585f12faf"
      },
      "source": [
        "tmp = []\n",
        "for batch_idx, (data,target) in enumerate(test_loader_base):\n",
        "  data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "  output = model_base(data)\n",
        "  tmp.append(output)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3SaqVFiG_Ne"
      },
      "source": [
        "def predicit_test(model, data_loader):\n",
        "    #### Test 데이터의 class 를 분류하는 함수를 만들어 주세요 ####\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data,target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data,1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "    accuracy = 100*correct/total\n",
        "    print('accuracy:',accuracy)\n",
        "    return accuracy"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMMpEASynwnO",
        "outputId": "a34dea89-4fc4-4e77-885b-17941258ce66"
      },
      "source": [
        "predicit_test(model_base,test_loader_base)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 34.4213649851632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.4213649851632"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf04nMFzIE6f"
      },
      "source": [
        "Transfer Learning with 모델 학습\n",
        "Transfer Learning이 익숙하지 않으신 분들은 PyTorch에서 제공하는 https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/transfer_learning_tutorial.html 을 참고하세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYIEj4KTHAnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41bd40f-c32b-4666-c547-8d561dd0684e"
      },
      "source": [
        "# Data Augmentation \n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        ### 데이터 전처리를 진행해 주세요 ###\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.ToTensor()]),\n",
        "    \n",
        "    'val': transforms.Compose([\n",
        "        ### 데이터 전처리를 진행해 주세요 ###\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.ToTensor()])\n",
        "}\n",
        "\n",
        "train_dataset_resnet = ImageFolder(root='./splitted/train', transform = data_transforms['train'])\n",
        "val_dataset_resnet = ImageFolder(root='./splitted/val', transform = data_transforms['val'])\n",
        "\n",
        "train_loader_resnet = torch.utils.data.DataLoader(train_dataset_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader_resnet = torch.utils.data.DataLoader(val_dataset_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjJ4Ua0wHB4F"
      },
      "source": [
        "from torchvision import models\n",
        " \n",
        "resnet = models.resnet50(pretrained=True)   # resnet50 불러오기, pretrained = True로 학습된 파라미터 값들 불러오기\n",
        "num_ftrs = resnet.fc.in_features            # resnet50의 fully connected layer의 input 노드 수\n",
        "resnet.fc = nn.Linear(num_ftrs, 7)         # input 노드 수를 이용해 새로운 layer 추가\n",
        "resnet = resnet.to(DEVICE)                  # resnet 모델 DEVICE에 할당\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()                                                           # CrossEntropyLoss 로 loss 계산\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001) # optimizer -> adam"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rarj6VrnHC6Q"
      },
      "source": [
        "# Pretrained Model의 일부 layer freeze 하기\n",
        "ct = 0 \n",
        "for child in resnet.children():  \n",
        "    ct+= 1  \n",
        "    if ct < 7: \n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfJUiAdEICfA"
      },
      "source": [
        "Fine Tuning을 진행해 주세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ola7YaZzHD2y"
      },
      "source": [
        "def train_resnet(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  # beset accuracy 초기화\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()                                     # 학습 시간 계산\n",
        "        train(model, train_loader, optimizer)                   # train 데이터로 학습\n",
        "        train_loss, train_acc = evaluate(model, train_loader)   # train_loss, train_acc 계산\n",
        "        val_loss, val_acc = evaluate(model, val_loader)         # valid_loss, valid_acc 계산\n",
        "        \n",
        "        if val_acc>best_acc:  # update best accuracy\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = tㅈㄱㄱime.time() - since # 학습 시간 출력\n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "\n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q557C8jMHGFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819081ee-4912-4130-ac85-ea0c5ddd1c4d"
      },
      "source": [
        "resnet_train = train_resnet(resnet, train_loader_resnet, val_loader_resnet, optimizer_ft)  # resnet transfer learning 진행\n",
        "torch.save(resnet_train,'resNet.pt')   "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.5647, Accuracy: 37.76%\n",
            "val Loss: 0.6108, Accuracy: 31.16%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.0097, Accuracy: 85.55%\n",
            "val Loss: 0.0221, Accuracy: 70.33%\n",
            "Completed in 0m 47s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.0025, Accuracy: 95.38%\n",
            "val Loss: 0.0130, Accuracy: 81.01%\n",
            "Completed in 0m 28s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.0019, Accuracy: 95.97%\n",
            "val Loss: 0.0154, Accuracy: 80.42%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.0153, Accuracy: 81.71%\n",
            "val Loss: 0.0344, Accuracy: 67.36%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.0019, Accuracy: 96.56%\n",
            "val Loss: 0.0128, Accuracy: 77.45%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.0034, Accuracy: 93.81%\n",
            "val Loss: 0.0149, Accuracy: 75.07%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.0004, Accuracy: 99.12%\n",
            "val Loss: 0.0127, Accuracy: 83.98%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.0081, Accuracy: 89.87%\n",
            "val Loss: 0.0223, Accuracy: 73.29%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.0015, Accuracy: 96.76%\n",
            "val Loss: 0.0156, Accuracy: 79.82%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.0024, Accuracy: 96.07%\n",
            "val Loss: 0.0155, Accuracy: 79.53%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.0017, Accuracy: 96.17%\n",
            "val Loss: 0.0178, Accuracy: 76.56%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.0015, Accuracy: 96.95%\n",
            "val Loss: 0.0151, Accuracy: 78.04%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.0029, Accuracy: 94.99%\n",
            "val Loss: 0.0128, Accuracy: 80.42%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.0008, Accuracy: 98.43%\n",
            "val Loss: 0.0150, Accuracy: 79.53%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.0001, Accuracy: 99.90%\n",
            "val Loss: 0.0151, Accuracy: 80.42%\n",
            "Completed in 0m 28s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.0046, Accuracy: 90.76%\n",
            "val Loss: 0.0250, Accuracy: 69.44%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.0057, Accuracy: 91.84%\n",
            "val Loss: 0.0278, Accuracy: 71.81%\n",
            "Completed in 0m 28s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.0005, Accuracy: 99.12%\n",
            "val Loss: 0.0145, Accuracy: 78.04%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.0001, Accuracy: 100.00%\n",
            "val Loss: 0.0111, Accuracy: 81.31%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0113, Accuracy: 83.38%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.0002, Accuracy: 99.80%\n",
            "val Loss: 0.0137, Accuracy: 78.93%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0002, Accuracy: 99.71%\n",
            "val Loss: 0.0159, Accuracy: 78.64%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0002, Accuracy: 99.71%\n",
            "val Loss: 0.0105, Accuracy: 84.87%\n",
            "Completed in 0m 28s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0001, Accuracy: 99.90%\n",
            "val Loss: 0.0121, Accuracy: 82.49%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0142, Accuracy: 83.38%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0187, Accuracy: 78.93%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0134, Accuracy: 83.38%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0096, Accuracy: 86.35%\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0126, Accuracy: 86.94%\n",
            "Completed in 0m 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHm2KUF3HIGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61a3536-0387-4297-bd80-799648bea47a"
      },
      "source": [
        "# Classification 하기\n",
        "transform_res = transforms.Compose([transforms.ToTensor()]) \n",
        "test_res = ImageFolder(root='./splitted/test',transform=transform_res)  \n",
        "test_loader_res = torch.utils.data.DataLoader(test_res, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bogJjztv1L8r",
        "outputId": "8060c6b7-c017-417b-997f-c597048a61a2"
      },
      "source": [
        "def prediction(model, test_loader):\n",
        "    ### FineTuning을 진행한 모델을 가지고 예측을 진행해 주세요    \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data,target) in enumerate(test_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data,1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "    accuracy = 100*correct/total\n",
        "    print('accuracy:',accuracy)\n",
        "    return accuracy\n",
        "prediction(resnet,test_loader_res)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 84.86646884272997\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84.86646884272997"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRaII3jH-sI"
      },
      "source": [
        "모델 평가\n",
        "모델 평가를 진행할 때 accuracy, precision, recall f1-score 등 다양한 평가지표를 가지고 진행해 주세요 ~!"
      ]
    }
  ]
}