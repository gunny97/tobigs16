{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 16기 3주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67283132, 0.72333819, 0.38833773])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.547465596165016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{(1+e^{-x\\theta})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters) \n",
    "    p = 1/(1+np.exp(-z)) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8245473837083176"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "\n",
    "## $l(p) = -\\Sigma[y_ilogp_i + (1-y_i)log(1-p_i)]$\n",
    "+) $p_i = \\frac{1}{(1+e^{-x\\theta})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X,parameters)\n",
    "    loss = -(y*np.log(p)+(1-y)*np.log(1-p))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X,parameters)\n",
    "    loss = (y-y_hat)*1/2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X,y,parameters)\n",
    "    loss = loss/n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42258188597829444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, mse_i, len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2098724687527542"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)= -\\Sigma(y_i-\\theta^{T}x_i)x_{ij}$ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma[(y_i-P_i)x_{ij}]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X,parameters)\n",
    "        gradient = -(y-y_hat)*X[j]\n",
    "    else:\n",
    "        p = logistic(X,parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11936066563861701"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07700087687125563"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] = get_gradient_ij(X,y,parameters,j,model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7924411755363063, 0.4312472456715538, 0.5575542745201951]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: batch_size에 맞게끔 학습에 들어갈 데이터를 batch 크기 만큼 나눠주는 역할을 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= -learning_rate*1/n\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67288415, 0.72336694, 0.3883749 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 모델을 학습 시키는 횟수\n",
    "- num_epoch:  hyperaparmeter로 받는 값\n",
    "<br>\n",
    "\n",
    "BGD: batch gradient descent -> 전체 데이터에 대해 업데이트가 한번에 이루어짐\n",
    "SGD: stochastic gradient descent -> 개별 데이터 한 개에 대해서 error gradeint 계산\n",
    "MGD: mini-batch gradient descent -> mini-batch안의 데이터 m개에 대해서 각 데이터에 대한 기울기를 m개 구한 뒤, 그것의 평균 기울기를 통해 모델을 업데이트 함\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> sgd  \n",
    "batch_size=k -> mgd  \n",
    "batch_size=whole -> bgd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 과제 form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "                \n",
    "        if stopper:\n",
    "            break\n",
    "            \n",
    "        \n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch,y_batch,parameters,model)\n",
    "            parameters = step(parameters,gradients,learning_rate,batch_size)\n",
    "            new_loss = batch_loss(X_batch,y_batch,parameters,loss_function,batch_size)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중단 조건 바꾼 form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    train_loss_min = np.Inf\n",
    "    epoch_loss = 0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        \n",
    "        \n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch,y_batch,parameters,model)\n",
    "            parameters = step(parameters,gradients,learning_rate,batch_size)\n",
    "            new_loss = batch_loss(X_batch,y_batch,parameters,loss_function,batch_size)\n",
    "\n",
    "            train_loss += new_loss\n",
    "        \n",
    "        train_loss = train_loss * 1/batch_size\n",
    "        \n",
    "        if epoch_loss >= train_loss:\n",
    "            break\n",
    "        \n",
    "        epoch_loss = train_loss\n",
    "        \n",
    "        if train_loss <= train_loss_min:\n",
    "            print(f\"epoch: {epoch}  loss: {train_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.006771793730499139  params: [0.45229354 0.04655559 0.98251211]  gradients: [-5.086216425133753e-05, -2.7679238433614866e-05, -3.578614787462194e-05]\n",
      "epoch: 1  loss: 0.006771975298873439  params: [0.4523444  0.04658327 0.98254789]  gradients: [-5.086326273633355e-05, -2.767983623020037e-05, -3.578692075849514e-05]\n",
      "epoch: 2  loss: 0.0067721568768438255  params: [0.45239526 0.04661095 0.98258368]  gradients: [-5.0864361192423457e-05, -2.768043401105514e-05, -3.5787693622030256e-05]\n",
      "epoch: 3  loss: 0.006772338464410203  params: [0.45244613 0.04663863 0.98261947]  gradients: [-5.086545961960245e-05, -2.7681031776176554e-05, -3.578846646522393e-05]\n",
      "epoch: 4  loss: 0.006772520061572475  params: [0.452497   0.04666631 0.98265526]  gradients: [-5.0866558017865704e-05, -2.7681629525561998e-05, -3.5789239288072754e-05]\n",
      "epoch: 5  loss: 0.006772701668330564  params: [0.45254786 0.046694   0.98269105]  gradients: [-5.08676563872084e-05, -2.7682227259208848e-05, -3.5790012090573335e-05]\n",
      "epoch: 6  loss: 0.006772883284684369  params: [0.45259873 0.04672168 0.98272684]  gradients: [-5.086875472762575e-05, -2.76828249771145e-05, -3.579078487272232e-05]\n",
      "epoch: 7  loss: 0.006773064910633809  params: [0.4526496  0.04674936 0.98276263]  gradients: [-5.0869853039112916e-05, -2.7683422679276314e-05, -3.5791557634516284e-05]\n",
      "epoch: 8  loss: 0.006773246546178792  params: [0.45270047 0.04677705 0.98279842]  gradients: [-5.087095132166512e-05, -2.76840203656917e-05, -3.579233037595189e-05]\n",
      "epoch: 9  loss: 0.006773428191319221  params: [0.45275134 0.04680473 0.98283422]  gradients: [-5.0872049575277535e-05, -2.768461803635802e-05, -3.5793103097025716e-05]\n",
      "epoch: 10  loss: 0.006773609846055016  params: [0.45280222 0.04683242 0.98287001]  gradients: [-5.0873147799945364e-05, -2.7685215691272667e-05, -3.579387579773441e-05]\n",
      "epoch: 11  loss: 0.006773791510386089  params: [0.45285309 0.0468601  0.9829058 ]  gradients: [-5.0874245995663795e-05, -2.7685813330433025e-05, -3.579464847807457e-05]\n",
      "epoch: 12  loss: 0.006773973184312343  params: [0.45290397 0.04688779 0.9829416 ]  gradients: [-5.0875344162428025e-05, -2.768641095383648e-05, -3.5795421138042827e-05]\n",
      "epoch: 13  loss: 0.006774154867833689  params: [0.45295484 0.04691547 0.9829774 ]  gradients: [-5.087644230023323e-05, -2.7687008561480407e-05, -3.579619377763578e-05]\n",
      "epoch: 14  loss: 0.006774336560950031  params: [0.45300572 0.04694316 0.98301319]  gradients: [-5.087754040907463e-05, -2.7687606153362205e-05, -3.5796966396850064e-05]\n",
      "epoch: 15  loss: 0.006774518263661292  params: [0.4530566  0.04697085 0.98304899]  gradients: [-5.087863848894739e-05, -2.7688203729479237e-05, -3.5797738995682285e-05]\n",
      "epoch: 16  loss: 0.0067746999759673784  params: [0.45310748 0.04699854 0.98308479]  gradients: [-5.0879736539846724e-05, -2.7688801289828905e-05, -3.5798511574129065e-05]\n",
      "epoch: 17  loss: 0.006774881697868191  params: [0.45315836 0.04702623 0.98312059]  gradients: [-5.0880834561767825e-05, -2.7689398834408593e-05, -3.579928413218704e-05]\n",
      "epoch: 18  loss: 0.00677506342936366  params: [0.45320924 0.04705392 0.98315639]  gradients: [-5.08819325547059e-05, -2.7689996363215686e-05, -3.5800056669852814e-05]\n",
      "epoch: 19  loss: 0.006775245170453675  params: [0.45326013 0.04708161 0.98319219]  gradients: [-5.0883030518656105e-05, -2.7690593876247557e-05, -3.5800829187123e-05]\n",
      "epoch: 20  loss: 0.0067754269211381425  params: [0.45331101 0.0471093  0.98322799]  gradients: [-5.0884128453613675e-05, -2.769119137350161e-05, -3.580160168399423e-05]\n",
      "epoch: 21  loss: 0.006775608681416989  params: [0.4533619  0.04713699 0.98326379]  gradients: [-5.0885226359573777e-05, -2.7691788854975204e-05, -3.580237416046312e-05]\n",
      "epoch: 22  loss: 0.0067757904512901175  params: [0.45341278 0.04716468 0.9832996 ]  gradients: [-5.088632423653163e-05, -2.7692386320665757e-05, -3.580314661652628e-05]\n",
      "epoch: 23  loss: 0.006775972230757434  params: [0.45346367 0.04719238 0.9833354 ]  gradients: [-5.088742208448243e-05, -2.769298377057064e-05, -3.5803919052180363e-05]\n",
      "epoch: 24  loss: 0.006776154019818857  params: [0.45351456 0.04722007 0.9833712 ]  gradients: [-5.088851990342136e-05, -2.7693581204687233e-05, -3.5804691467421954e-05]\n",
      "epoch: 25  loss: 0.0067763358184742845  params: [0.45356545 0.04724777 0.98340701]  gradients: [-5.088961769334363e-05, -2.769417862301293e-05, -3.580546386224769e-05]\n",
      "epoch: 26  loss: 0.006776517626723635  params: [0.45361634 0.04727546 0.98344282]  gradients: [-5.089071545424443e-05, -2.7694776025545117e-05, -3.580623623665419e-05]\n",
      "epoch: 27  loss: 0.006776699444566812  params: [0.45366723 0.04730316 0.98347862]  gradients: [-5.089181318611895e-05, -2.7695373412281183e-05, -3.580700859063807e-05]\n",
      "epoch: 28  loss: 0.006776881272003732  params: [0.45371812 0.04733085 0.98351443]  gradients: [-5.08929108889624e-05, -2.7695970783218507e-05, -3.5807780924195966e-05]\n",
      "epoch: 29  loss: 0.006777063109034292  params: [0.45376902 0.04735855 0.98355024]  gradients: [-5.0894008562769984e-05, -2.769656813835448e-05, -3.580855323732448e-05]\n",
      "epoch: 30  loss: 0.006777244955658409  params: [0.45381991 0.04738624 0.98358605]  gradients: [-5.0895106207536885e-05, -2.7697165477686488e-05, -3.5809325530020246e-05]\n",
      "epoch: 31  loss: 0.006777426811875998  params: [0.45387081 0.04741394 0.98362186]  gradients: [-5.0896203823258336e-05, -2.7697762801211935e-05, -3.58100978022799e-05]\n",
      "epoch: 32  loss: 0.006777608677686955  params: [0.45392171 0.04744164 0.98365767]  gradients: [-5.089730140992948e-05, -2.7698360108928182e-05, -3.581087005410003e-05]\n",
      "epoch: 33  loss: 0.006777790553091203  params: [0.4539726  0.04746934 0.98369348]  gradients: [-5.089839896754557e-05, -2.7698957400832636e-05, -3.581164228547729e-05]\n",
      "epoch: 34  loss: 0.00677797243808864  params: [0.4540235  0.04749704 0.98372929]  gradients: [-5.0899496496101795e-05, -2.7699554676922683e-05, -3.58124144964083e-05]\n",
      "epoch: 35  loss: 0.00677815433267918  params: [0.4540744  0.04752474 0.98376511]  gradients: [-5.090059399559334e-05, -2.7700151937195704e-05, -3.5813186686889666e-05]\n",
      "epoch: 36  loss: 0.006778336236862727  params: [0.45412531 0.04755244 0.98380092]  gradients: [-5.09016914660154e-05, -2.7700749181649083e-05, -3.581395885691802e-05]\n",
      "epoch: 37  loss: 0.0067785181506391946  params: [0.45417621 0.04758014 0.98383674]  gradients: [-5.090278890736321e-05, -2.7701346410280233e-05, -3.581473100649e-05]\n",
      "epoch: 38  loss: 0.00677870007400849  params: [0.45422711 0.04760784 0.98387255]  gradients: [-5.090388631963194e-05, -2.7701943623086517e-05, -3.58155031356022e-05]\n",
      "epoch: 39  loss: 0.006778882006970524  params: [0.45427802 0.04763555 0.98390837]  gradients: [-5.0904983702816814e-05, -2.7702540820065336e-05, -3.581627524425128e-05]\n",
      "epoch: 40  loss: 0.006779063949525204  params: [0.45432892 0.04766325 0.98394419]  gradients: [-5.090608105691303e-05, -2.770313800121408e-05, -3.5817047332433836e-05]\n",
      "epoch: 41  loss: 0.00677924590167244  params: [0.45437983 0.04769095 0.98398   ]  gradients: [-5.090717838191577e-05, -2.770373516653013e-05, -3.58178194001465e-05]\n",
      "epoch: 42  loss: 0.0067794278634121284  params: [0.45443074 0.04771866 0.98401582]  gradients: [-5.0908275677820274e-05, -2.770433231601089e-05, -3.5818591447385915e-05]\n",
      "epoch: 43  loss: 0.0067796098347442  params: [0.45448165 0.04774636 0.98405164]  gradients: [-5.0909372944621715e-05, -2.7704929449653734e-05, -3.581936347414868e-05]\n",
      "epoch: 44  loss: 0.006779791815668547  params: [0.45453256 0.04777407 0.98408746]  gradients: [-5.091047018231532e-05, -2.7705526567456068e-05, -3.5820135480431445e-05]\n",
      "epoch: 45  loss: 0.0067799738061850795  params: [0.45458347 0.04780177 0.98412328]  gradients: [-5.091156739089628e-05, -2.7706123669415266e-05, -3.5820907466230814e-05]\n",
      "epoch: 46  loss: 0.006780155806293709  params: [0.45463438 0.04782948 0.9841591 ]  gradients: [-5.09126645703598e-05, -2.7706720755528732e-05, -3.5821679431543424e-05]\n",
      "epoch: 47  loss: 0.006780337815994349  params: [0.4546853  0.04785719 0.98419493]  gradients: [-5.091376172070109e-05, -2.770731782579385e-05, -3.5822451376365914e-05]\n",
      "epoch: 48  loss: 0.006780519835286893  params: [0.45473621 0.0478849  0.98423075]  gradients: [-5.091485884191535e-05, -2.7707914880208017e-05, -3.582322330069489e-05]\n",
      "epoch: 49  loss: 0.006780701864171264  params: [0.45478713 0.0479126  0.98426657]  gradients: [-5.091595593399779e-05, -2.7708511918768612e-05, -3.582399520452699e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50  loss: 0.006780883902647364  params: [0.45483804 0.04794031 0.9843024 ]  gradients: [-5.091705299694362e-05, -2.7709108941473035e-05, -3.582476708785883e-05]\n",
      "epoch: 51  loss: 0.0067810659507151  params: [0.45488896 0.04796802 0.98433822]  gradients: [-5.091815003074805e-05, -2.7709705948318684e-05, -3.5825538950687054e-05]\n",
      "epoch: 52  loss: 0.0067812480083743855  params: [0.45493988 0.04799573 0.98437405]  gradients: [-5.091924703540627e-05, -2.771030293930294e-05, -3.582631079300828e-05]\n",
      "epoch: 53  loss: 0.006781430075625116  params: [0.4549908  0.04802344 0.98440988]  gradients: [-5.092034401091349e-05, -2.7710899914423193e-05, -3.5827082614819134e-05]\n",
      "epoch: 54  loss: 0.006781612152467214  params: [0.45504172 0.04805116 0.9844457 ]  gradients: [-5.092144095726494e-05, -2.7711496873676846e-05, -3.5827854416116256e-05]\n",
      "epoch: 55  loss: 0.006781794238900578  params: [0.45509265 0.04807887 0.98448153]  gradients: [-5.092253787445581e-05, -2.771209381706128e-05, -3.582862619689626e-05]\n",
      "epoch: 56  loss: 0.0067819763349251175  params: [0.45514357 0.04810658 0.98451736]  gradients: [-5.0923634762481306e-05, -2.7712690744573896e-05, -3.582939795715578e-05]\n",
      "epoch: 57  loss: 0.0067821584405407505  params: [0.45519449 0.04813429 0.98455319]  gradients: [-5.092473162133665e-05, -2.7713287656212083e-05, -3.583016969689144e-05]\n",
      "epoch: 58  loss: 0.006782340555747366  params: [0.45524542 0.04816201 0.98458902]  gradients: [-5.092582845101704e-05, -2.7713884551973234e-05, -3.583094141609989e-05]\n",
      "epoch: 59  loss: 0.0067825226805448835  params: [0.45529635 0.04818972 0.98462486]  gradients: [-5.092692525151769e-05, -2.7714481431854746e-05, -3.583171311477774e-05]\n",
      "epoch: 60  loss: 0.006782704814933206  params: [0.45534728 0.04821744 0.98466069]  gradients: [-5.0928022022833804e-05, -2.7715078295853997e-05, -3.583248479292161e-05]\n",
      "epoch: 61  loss: 0.0067828869589122535  params: [0.4553982  0.04824515 0.98469652]  gradients: [-5.0929118764960614e-05, -2.771567514396841e-05, -3.5833256450528166e-05]\n",
      "epoch: 62  loss: 0.00678306911248191  params: [0.45544913 0.04827287 0.98473236]  gradients: [-5.09302154778933e-05, -2.7716271976195346e-05, -3.583402808759401e-05]\n",
      "epoch: 63  loss: 0.006783251275642107  params: [0.45550007 0.04830059 0.98476819]  gradients: [-5.09313121616271e-05, -2.771686879253222e-05, -3.583479970411578e-05]\n",
      "epoch: 64  loss: 0.006783433448392741  params: [0.455551   0.0483283  0.98480403]  gradients: [-5.093240881615721e-05, -2.7717465592976424e-05, -3.5835571300090106e-05]\n",
      "epoch: 65  loss: 0.0067836156307337124  params: [0.45560193 0.04835602 0.98483986]  gradients: [-5.0933505441478834e-05, -2.771806237752534e-05, -3.583634287551361e-05]\n",
      "epoch: 66  loss: 0.006783797822664944  params: [0.45565287 0.04838374 0.9848757 ]  gradients: [-5.093460203758721e-05, -2.7718659146176376e-05, -3.583711443038295e-05]\n",
      "epoch: 67  loss: 0.006783980024186328  params: [0.4557038  0.04841146 0.98491154]  gradients: [-5.093569860447752e-05, -2.7719255898926916e-05, -3.583788596469473e-05]\n",
      "epoch: 68  loss: 0.006784162235297777  params: [0.45575474 0.04843918 0.98494738]  gradients: [-5.0936795142145e-05, -2.7719852635774362e-05, -3.5838657478445586e-05]\n",
      "epoch: 69  loss: 0.00678434445599921  params: [0.45580568 0.0484669  0.98498322]  gradients: [-5.093789165058486e-05, -2.772044935671611e-05, -3.583942897163216e-05]\n",
      "epoch: 70  loss: 0.006784526686290522  params: [0.45585662 0.04849462 0.98501906]  gradients: [-5.0938988129792304e-05, -2.7721046061749546e-05, -3.584020044425109e-05]\n",
      "epoch: 71  loss: 0.006784708926171615  params: [0.45590756 0.04852234 0.9850549 ]  gradients: [-5.094008457976255e-05, -2.7721642750872076e-05, -3.584097189629899e-05]\n",
      "epoch: 72  loss: 0.006784891175642403  params: [0.4559585  0.04855007 0.98509074]  gradients: [-5.094118100049082e-05, -2.772223942408109e-05, -3.58417433277725e-05]\n",
      "epoch: 73  loss: 0.006785073434702796  params: [0.45600944 0.04857779 0.98512658]  gradients: [-5.094227739197232e-05, -2.7722836081373984e-05, -3.584251473866827e-05]\n",
      "epoch: 74  loss: 0.006785255703352696  params: [0.45606038 0.04860551 0.98516242]  gradients: [-5.094337375420226e-05, -2.7723432722748156e-05, -3.58432861289829e-05]\n",
      "epoch: 75  loss: 0.006785437981592013  params: [0.45611133 0.04863324 0.98519827]  gradients: [-5.0944470087175877e-05, -2.7724029348201e-05, -3.584405749871305e-05]\n",
      "epoch: 76  loss: 0.006785620269420646  params: [0.45616227 0.04866096 0.98523411]  gradients: [-5.094556639088836e-05, -2.7724625957729915e-05, -3.5844828847855344e-05]\n",
      "epoch: 77  loss: 0.00678580256683851  params: [0.45621322 0.04868869 0.98526996]  gradients: [-5.0946662665334935e-05, -2.7725222551332292e-05, -3.5845600176406423e-05]\n",
      "epoch: 78  loss: 0.0067859848738455126  params: [0.45626417 0.04871641 0.9853058 ]  gradients: [-5.094775891051082e-05, -2.7725819129005537e-05, -3.58463714843629e-05]\n",
      "epoch: 79  loss: 0.006786167190441555  params: [0.45631512 0.04874414 0.98534165]  gradients: [-5.094885512641123e-05, -2.7726415690747034e-05, -3.584714277172144e-05]\n",
      "epoch: 80  loss: 0.006786349516626545  params: [0.45636607 0.04877186 0.9853775 ]  gradients: [-5.0949951313031405e-05, -2.7727012236554198e-05, -3.5847914038478665e-05]\n",
      "epoch: 81  loss: 0.006786531852400396  params: [0.45641702 0.04879959 0.98541335]  gradients: [-5.095104747036652e-05, -2.7727608766424407e-05, -3.58486852846312e-05]\n",
      "epoch: 82  loss: 0.006786714197763  params: [0.45646797 0.04882732 0.9854492 ]  gradients: [-5.095214359841184e-05, -2.772820528035508e-05, -3.58494565101757e-05]\n",
      "epoch: 83  loss: 0.006786896552714271  params: [0.45651892 0.04885505 0.98548505]  gradients: [-5.095323969716253e-05, -2.772880177834359e-05, -3.585022771510877e-05]\n",
      "epoch: 84  loss: 0.006787078917254119  params: [0.45656988 0.04888278 0.9855209 ]  gradients: [-5.0954335766613854e-05, -2.7729398260387354e-05, -3.585099889942708e-05]\n",
      "epoch: 85  loss: 0.006787261291382445  params: [0.45662083 0.04891051 0.98555675]  gradients: [-5.0955431806761004e-05, -2.7729994726483764e-05, -3.585177006312725e-05]\n",
      "epoch: 86  loss: 0.006787443675099166  params: [0.45667179 0.04893824 0.9855926 ]  gradients: [-5.0956527817599216e-05, -2.773059117663022e-05, -3.585254120620592e-05]\n",
      "epoch: 87  loss: 0.00678762606840417  params: [0.45672275 0.04896597 0.98562846]  gradients: [-5.0957623799123704e-05, -2.7731187610824116e-05, -3.585331232865972e-05]\n",
      "epoch: 88  loss: 0.006787808471297378  params: [0.4567737  0.0489937  0.98566431]  gradients: [-5.095871975132967e-05, -2.7731784029062846e-05, -3.585408343048528e-05]\n",
      "epoch: 89  loss: 0.006787990883778687  params: [0.45682466 0.04902143 0.98570017]  gradients: [-5.0959815674212375e-05, -2.7732380431343828e-05, -3.585485451167926e-05]\n",
      "epoch: 90  loss: 0.006788173305848009  params: [0.45687563 0.04904917 0.98573602]  gradients: [-5.0960911567767e-05, -2.7732976817664446e-05, -3.5855625572238286e-05]\n",
      "epoch: 91  loss: 0.00678835573750525  params: [0.45692659 0.0490769  0.98577188]  gradients: [-5.096200743198879e-05, -2.7733573188022105e-05, -3.5856396612159e-05]\n",
      "epoch: 92  loss: 0.006788538178750311  params: [0.45697755 0.04910464 0.98580773]  gradients: [-5.096310326687295e-05, -2.7734169542414195e-05, -3.585716763143802e-05]\n",
      "epoch: 93  loss: 0.0067887206295831025  params: [0.45702851 0.04913237 0.98584359]  gradients: [-5.096419907241473e-05, -2.7734765880838132e-05, -3.5857938630072016e-05]\n",
      "epoch: 94  loss: 0.006788903090003524  params: [0.45707948 0.04916011 0.98587945]  gradients: [-5.096529484860931e-05, -2.7735362203291297e-05, -3.58587096080576e-05]\n",
      "epoch: 95  loss: 0.00678908556001149  params: [0.45713045 0.04918784 0.98591531]  gradients: [-5.096639059545195e-05, -2.773595850977111e-05, -3.585948056539142e-05]\n",
      "epoch: 96  loss: 0.006789268039606899  params: [0.45718141 0.04921558 0.98595117]  gradients: [-5.096748631293785e-05, -2.773655480027496e-05, -3.5860251502070125e-05]\n",
      "epoch: 97  loss: 0.006789450528789661  params: [0.45723238 0.04924331 0.98598703]  gradients: [-5.096858200106225e-05, -2.773715107480024e-05, -3.5861022418090334e-05]\n",
      "epoch: 98  loss: 0.006789633027559674  params: [0.45728335 0.04927105 0.98602289]  gradients: [-5.096967765982036e-05, -2.773774733334437e-05, -3.5861793313448706e-05]\n",
      "epoch: 99  loss: 0.006789815535916856  params: [0.45733432 0.04929879 0.98605876]  gradients: [-5.097077328920741e-05, -2.7738343575904734e-05, -3.586256418814186e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100  loss: 0.006789998053861105  params: [0.45738529 0.04932653 0.98609462]  gradients: [-5.0971868889218615e-05, -2.7738939802478743e-05, -3.5863335042166454e-05]\n",
      "epoch: 101  loss: 0.006790180581392325  params: [0.45743627 0.04935427 0.98613048]  gradients: [-5.0972964459849216e-05, -2.7739536013063795e-05, -3.586410587551912e-05]\n",
      "epoch: 102  loss: 0.006790363118510424  params: [0.45748724 0.04938201 0.98616635]  gradients: [-5.097406000109444e-05, -2.7740132207657296e-05, -3.586487668819651e-05]\n",
      "epoch: 103  loss: 0.00679054566521531  params: [0.45753822 0.04940975 0.98620221]  gradients: [-5.0975155512949495e-05, -2.774072838625664e-05, -3.586564748019524e-05]\n",
      "epoch: 104  loss: 0.006790728221506881  params: [0.45758919 0.04943749 0.98623808]  gradients: [-5.097625099540963e-05, -2.7741324548859234e-05, -3.586641825151199e-05]\n",
      "epoch: 105  loss: 0.0067909107873850485  params: [0.45764017 0.04946523 0.98627395]  gradients: [-5.097734644847004e-05, -2.7741920695462477e-05, -3.586718900214336e-05]\n",
      "epoch: 106  loss: 0.006791093362849713  params: [0.45769115 0.04949298 0.98630982]  gradients: [-5.0978441872125965e-05, -2.774251682606377e-05, -3.5867959732086e-05]\n",
      "epoch: 107  loss: 0.006791275947900786  params: [0.45774213 0.04952072 0.98634568]  gradients: [-5.097953726637264e-05, -2.7743112940660522e-05, -3.5868730441336576e-05]\n",
      "epoch: 108  loss: 0.006791458542538171  params: [0.45779311 0.04954846 0.98638155]  gradients: [-5.0980632631205285e-05, -2.774370903925013e-05, -3.5869501129891705e-05]\n",
      "epoch: 109  loss: 0.006791641146761768  params: [0.45784409 0.04957621 0.98641742]  gradients: [-5.098172796661914e-05, -2.7744305121829997e-05, -3.5870271797748046e-05]\n",
      "epoch: 110  loss: 0.006791823760571482  params: [0.45789507 0.04960395 0.9864533 ]  gradients: [-5.098282327260941e-05, -2.7744901188397528e-05, -3.5871042444902223e-05]\n",
      "epoch: 111  loss: 0.006792006383967226  params: [0.45794606 0.0496317  0.98648917]  gradients: [-5.0983918549171346e-05, -2.774549723895013e-05, -3.5871813071350904e-05]\n",
      "epoch: 112  loss: 0.006792189016948895  params: [0.45799704 0.04965944 0.98652504]  gradients: [-5.098501379630016e-05, -2.7746093273485203e-05, -3.5872583677090714e-05]\n",
      "epoch: 113  loss: 0.006792371659516401  params: [0.45804803 0.04968719 0.98656091]  gradients: [-5.09861090139911e-05, -2.7746689292000155e-05, -3.58733542621183e-05]\n",
      "epoch: 114  loss: 0.00679255431166964  params: [0.45809902 0.04971494 0.98659679]  gradients: [-5.098720420223938e-05, -2.7747285294492377e-05, -3.58741248264303e-05]\n",
      "epoch: 115  loss: 0.006792736973408527  params: [0.45815    0.04974269 0.98663266]  gradients: [-5.098829936104022e-05, -2.7747881280959284e-05, -3.5874895370023366e-05]\n",
      "epoch: 116  loss: 0.0067929196447329655  params: [0.45820099 0.04977043 0.98666854]  gradients: [-5.098939449038887e-05, -2.7748477251398275e-05, -3.587566589289414e-05]\n",
      "epoch: 117  loss: 0.006793102325642859  params: [0.45825198 0.04979818 0.98670441]  gradients: [-5.099048959028056e-05, -2.7749073205806757e-05, -3.5876436395039266e-05]\n",
      "epoch: 118  loss: 0.006793285016138098  params: [0.45830298 0.04982593 0.98674029]  gradients: [-5.099158466071052e-05, -2.774966914418214e-05, -3.5877206876455395e-05]\n",
      "epoch: 119  loss: 0.00679346771621861  params: [0.45835397 0.04985368 0.98677617]  gradients: [-5.099267970167397e-05, -2.7750265066521822e-05, -3.587797733713916e-05]\n",
      "epoch: 120  loss: 0.006793650425884281  params: [0.45840496 0.04988143 0.98681205]  gradients: [-5.099377471316615e-05, -2.7750860972823208e-05, -3.5878747777087204e-05]\n",
      "epoch: 121  loss: 0.006793833145135025  params: [0.45845596 0.04990919 0.98684793]  gradients: [-5.0994869695182285e-05, -2.775145686308371e-05, -3.587951819629619e-05]\n",
      "epoch: 122  loss: 0.0067940158739707435  params: [0.45850695 0.04993694 0.98688381]  gradients: [-5.0995964647717624e-05, -2.7752052737300727e-05, -3.588028859476275e-05]\n",
      "epoch: 123  loss: 0.00679419861239134  params: [0.45855795 0.04996469 0.98691969]  gradients: [-5.0997059570767386e-05, -2.7752648595471664e-05, -3.588105897248354e-05]\n",
      "epoch: 124  loss: 0.006794381360396719  params: [0.45860895 0.04999244 0.98695557]  gradients: [-5.0998154464326795e-05, -2.7753244437593925e-05, -3.588182932945518e-05]\n",
      "epoch: 125  loss: 0.006794564117986789  params: [0.45865995 0.0500202  0.98699145]  gradients: [-5.099924932839111e-05, -2.775384026366493e-05, -3.588259966567435e-05]\n",
      "epoch: 126  loss: 0.006794746885161454  params: [0.45871095 0.05004795 0.98702734]  gradients: [-5.1000344162955546e-05, -2.775443607368207e-05, -3.588336998113768e-05]\n",
      "epoch: 127  loss: 0.006794929661920612  params: [0.45876195 0.05007571 0.98706322]  gradients: [-5.100143896801535e-05, -2.775503186764276e-05, -3.588414027584182e-05]\n",
      "epoch: 128  loss: 0.006795112448264167  params: [0.45881295 0.05010346 0.98709911]  gradients: [-5.100253374356574e-05, -2.7755627645544402e-05, -3.588491054978342e-05]\n",
      "epoch: 129  loss: 0.006795295244192027  params: [0.45886396 0.05013122 0.98713499]  gradients: [-5.100362848960196e-05, -2.7756223407384402e-05, -3.588568080295911e-05]\n",
      "epoch: 130  loss: 0.00679547804970409  params: [0.45891496 0.05015898 0.98717088]  gradients: [-5.100472320611925e-05, -2.7756819153160173e-05, -3.588645103536556e-05]\n",
      "epoch: 131  loss: 0.006795660864800273  params: [0.45896597 0.05018673 0.98720677]  gradients: [-5.1005817893112846e-05, -2.775741488286912e-05, -3.5887221246999403e-05]\n",
      "epoch: 132  loss: 0.006795843689480461  params: [0.45901697 0.05021449 0.98724265]  gradients: [-5.100691255057797e-05, -2.7758010596508643e-05, -3.5887991437857305e-05]\n",
      "epoch: 133  loss: 0.006796026523744576  params: [0.45906798 0.05024225 0.98727854]  gradients: [-5.100800717850988e-05, -2.775860629407617e-05, -3.5888761607935896e-05]\n",
      "epoch: 134  loss: 0.006796209367592514  params: [0.45911899 0.05027001 0.98731443]  gradients: [-5.100910177690379e-05, -2.7759201975569082e-05, -3.5889531757231836e-05]\n",
      "epoch: 135  loss: 0.006796392221024176  params: [0.45917    0.05029777 0.98735032]  gradients: [-5.101019634575495e-05, -2.7759797640984803e-05, -3.5890301885741764e-05]\n",
      "epoch: 136  loss: 0.006796575084039461  params: [0.45922101 0.05032553 0.98738621]  gradients: [-5.10112908850586e-05, -2.776039329032074e-05, -3.589107199346233e-05]\n",
      "epoch: 137  loss: 0.006796757956638288  params: [0.45927202 0.05035329 0.98742211]  gradients: [-5.101238539480997e-05, -2.77609889235743e-05, -3.58918420803902e-05]\n",
      "epoch: 138  loss: 0.0067969408388205465  params: [0.45932304 0.05038105 0.987458  ]  gradients: [-5.10134798750043e-05, -2.7761584540742887e-05, -3.5892612146522e-05]\n",
      "epoch: 139  loss: 0.006797123730586147  params: [0.45937405 0.05040881 0.98749389]  gradients: [-5.101457432563684e-05, -2.7762180141823926e-05, -3.58933821918544e-05]\n",
      "epoch: 140  loss: 0.0067973066319349925  params: [0.45942507 0.05043658 0.98752979]  gradients: [-5.10156687467028e-05, -2.7762775726814804e-05, -3.589415221638403e-05]\n",
      "epoch: 141  loss: 0.006797489542866983  params: [0.45947608 0.05046434 0.98756568]  gradients: [-5.101676313819746e-05, -2.7763371295712947e-05, -3.5894922220107564e-05]\n",
      "epoch: 142  loss: 0.006797672463382026  params: [0.4595271  0.0504921  0.98760158]  gradients: [-5.1017857500116035e-05, -2.7763966848515757e-05, -3.589569220302164e-05]\n",
      "epoch: 143  loss: 0.00679785539348002  params: [0.45957812 0.05051987 0.98763747]  gradients: [-5.1018951832453755e-05, -2.7764562385220638e-05, -3.58964621651229e-05]\n",
      "epoch: 144  loss: 0.006798038333160865  params: [0.45962914 0.05054763 0.98767337]  gradients: [-5.102004613520589e-05, -2.776515790582501e-05, -3.589723210640801e-05]\n",
      "epoch: 145  loss: 0.006798221282424474  params: [0.45968016 0.0505754  0.98770927]  gradients: [-5.102114040836766e-05, -2.776575341032629e-05, -3.589800202687363e-05]\n",
      "epoch: 146  loss: 0.006798404241270745  params: [0.45973118 0.05060317 0.98774517]  gradients: [-5.102223465193431e-05, -2.776634889872187e-05, -3.589877192651638e-05]\n",
      "epoch: 147  loss: 0.00679858720969958  params: [0.45978221 0.05063093 0.98778107]  gradients: [-5.102332886590109e-05, -2.776694437100917e-05, -3.589954180533295e-05]\n",
      "epoch: 148  loss: 0.006798770187710887  params: [0.45983323 0.0506587  0.98781697]  gradients: [-5.102442305026323e-05, -2.7767539827185595e-05, -3.590031166331996e-05]\n",
      "epoch: 149  loss: 0.006798953175304566  params: [0.45988426 0.05068647 0.98785287]  gradients: [-5.102551720501598e-05, -2.7768135267248566e-05, -3.590108150047408e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150  loss: 0.006799136172480515  params: [0.45993528 0.05071424 0.98788877]  gradients: [-5.102661133015457e-05, -2.7768730691195483e-05, -3.590185131679195e-05]\n",
      "epoch: 151  loss: 0.006799319179238643  params: [0.45998631 0.05074201 0.98792467]  gradients: [-5.102770542567426e-05, -2.776932609902377e-05, -3.590262111227024e-05]\n",
      "epoch: 152  loss: 0.006799502195578853  params: [0.46003734 0.05076978 0.98796058]  gradients: [-5.102879949157029e-05, -2.776992149073083e-05, -3.59033908869056e-05]\n",
      "epoch: 153  loss: 0.006799685221501045  params: [0.46008837 0.05079755 0.98799648]  gradients: [-5.10298935278379e-05, -2.777051686631407e-05, -3.590416064069466e-05]\n",
      "epoch: 154  loss: 0.006799868257005119  params: [0.4601394  0.05082532 0.98803238]  gradients: [-5.103098753447233e-05, -2.777111222577091e-05, -3.590493037363411e-05]\n",
      "epoch: 155  loss: 0.006800051302090979  params: [0.46019043 0.05085309 0.98806829]  gradients: [-5.1032081511468834e-05, -2.7771707569098763e-05, -3.590570008572058e-05]\n",
      "epoch: 156  loss: 0.00680023435675853  params: [0.46024147 0.05088086 0.9881042 ]  gradients: [-5.1033175458822646e-05, -2.7772302896295032e-05, -3.590646977695073e-05]\n",
      "epoch: 157  loss: 0.006800417421007672  params: [0.4602925  0.05090863 0.9881401 ]  gradients: [-5.103426937652902e-05, -2.777289820735714e-05, -3.590723944732121e-05]\n",
      "epoch: 158  loss: 0.00680060049483831  params: [0.46034354 0.05093641 0.98817601]  gradients: [-5.1035363264583204e-05, -2.7773493502282493e-05, -3.5908009096828694e-05]\n",
      "epoch: 159  loss: 0.006800783578250345  params: [0.46039457 0.05096418 0.98821192]  gradients: [-5.103645712298044e-05, -2.777408878106851e-05, -3.590877872546981e-05]\n",
      "epoch: 160  loss: 0.006800966671243678  params: [0.46044561 0.05099196 0.98824783]  gradients: [-5.103755095171597e-05, -2.77746840437126e-05, -3.590954833324124e-05]\n",
      "epoch: 161  loss: 0.006801149773818218  params: [0.46049665 0.05101973 0.98828374]  gradients: [-5.1038644750785044e-05, -2.777527929021217e-05, -3.5910317920139616e-05]\n",
      "epoch: 162  loss: 0.0068013328859738535  params: [0.46054769 0.05104751 0.98831965]  gradients: [-5.1039738520182905e-05, -2.7775874520564645e-05, -3.59110874861616e-05]\n",
      "epoch: 163  loss: 0.006801516007710506  params: [0.46059873 0.05107528 0.98835556]  gradients: [-5.104083225990481e-05, -2.7776469734767434e-05, -3.5911857031303864e-05]\n",
      "epoch: 164  loss: 0.006801699139028059  params: [0.46064977 0.05110306 0.98839148]  gradients: [-5.1041925969946005e-05, -2.777706493281795e-05, -3.591262655556305e-05]\n",
      "epoch: 165  loss: 0.006801882279926428  params: [0.46070082 0.05113084 0.98842739]  gradients: [-5.104301965030174e-05, -2.777766011471361e-05, -3.5913396058935824e-05]\n",
      "epoch: 166  loss: 0.006802065430405502  params: [0.46075186 0.05115862 0.9884633 ]  gradients: [-5.104411330096725e-05, -2.777825528045182e-05, -3.591416554141882e-05]\n",
      "epoch: 167  loss: 0.00680224859046519  params: [0.4608029  0.0511864  0.98849922]  gradients: [-5.104520692193779e-05, -2.7778850430030004e-05, -3.591493500300872e-05]\n",
      "epoch: 168  loss: 0.006802431760105394  params: [0.46085395 0.05121418 0.98853513]  gradients: [-5.104630051320862e-05, -2.7779445563445575e-05, -3.591570444370218e-05]\n",
      "epoch: 169  loss: 0.006802614939326018  params: [0.460905   0.05124196 0.98857105]  gradients: [-5.104739407477498e-05, -2.778004068069595e-05, -3.591647386349585e-05]\n",
      "epoch: 170  loss: 0.0068027981281269636  params: [0.46095605 0.05126974 0.98860697]  gradients: [-5.1048487606632124e-05, -2.778063578177853e-05, -3.591724326238639e-05]\n",
      "epoch: 171  loss: 0.006802981326508125  params: [0.4610071  0.05129752 0.98864289]  gradients: [-5.1049581108775295e-05, -2.778123086669075e-05, -3.591801264037045e-05]\n",
      "epoch: 172  loss: 0.006803164534469415  params: [0.46105815 0.0513253  0.9886788 ]  gradients: [-5.1050674581199766e-05, -2.7781825935430014e-05, -3.591878199744471e-05]\n",
      "epoch: 173  loss: 0.0068033477520107275  params: [0.4611092  0.05135308 0.98871472]  gradients: [-5.105176802390075e-05, -2.7782420987993732e-05, -3.59195513336058e-05]\n",
      "epoch: 174  loss: 0.006803530979131964  params: [0.46116025 0.05138086 0.98875064]  gradients: [-5.105286143687353e-05, -2.7783016024379335e-05, -3.59203206488504e-05]\n",
      "epoch: 175  loss: 0.00680371421583303  params: [0.46121131 0.05140865 0.98878657]  gradients: [-5.105395482011335e-05, -2.7783611044584226e-05, -3.5921089943175165e-05]\n",
      "epoch: 176  loss: 0.0068038974621138235  params: [0.46126236 0.05143643 0.98882249]  gradients: [-5.105504817361546e-05, -2.7784206048605834e-05, -3.592185921657675e-05]\n",
      "epoch: 177  loss: 0.006804080717974248  params: [0.46131342 0.05146422 0.98885841]  gradients: [-5.105614149737511e-05, -2.7784801036441568e-05, -3.5922628469051824e-05]\n",
      "epoch: 178  loss: 0.006804263983414209  params: [0.46136447 0.051492   0.98889433]  gradients: [-5.105723479138756e-05, -2.7785396008088842e-05, -3.592339770059704e-05]\n",
      "epoch: 179  loss: 0.0068044472584335965  params: [0.46141553 0.05151979 0.98893026]  gradients: [-5.105832805564807e-05, -2.7785990963545084e-05, -3.5924166911209053e-05]\n",
      "epoch: 180  loss: 0.006804630543032322  params: [0.46146659 0.05154758 0.98896618]  gradients: [-5.105942129015186e-05, -2.778658590280769e-05, -3.592493610088453e-05]\n",
      "epoch: 181  loss: 0.006804813837210279  params: [0.46151765 0.05157536 0.98900211]  gradients: [-5.106051449489423e-05, -2.77871808258741e-05, -3.592570526962014e-05]\n",
      "epoch: 182  loss: 0.006804997140967373  params: [0.46156871 0.05160315 0.98903803]  gradients: [-5.1061607669870394e-05, -2.778777573274172e-05, -3.5926474417412535e-05]\n",
      "epoch: 183  loss: 0.006805180454303509  params: [0.46161978 0.05163094 0.98907396]  gradients: [-5.106270081507564e-05, -2.7788370623407973e-05, -3.592724354425837e-05]\n",
      "epoch: 184  loss: 0.006805363777218584  params: [0.46167084 0.05165873 0.98910989]  gradients: [-5.106379393050521e-05, -2.7788965497870275e-05, -3.592801265015432e-05]\n",
      "epoch: 185  loss: 0.006805547109712492  params: [0.46172191 0.05168652 0.98914582]  gradients: [-5.106488701615435e-05, -2.778956035612604e-05, -3.592878173509704e-05]\n",
      "epoch: 186  loss: 0.006805730451785143  params: [0.46177297 0.05171431 0.98918175]  gradients: [-5.1065980072018326e-05, -2.779015519817269e-05, -3.592955079908319e-05]\n",
      "epoch: 187  loss: 0.006805913803436442  params: [0.46182404 0.0517421  0.98921768]  gradients: [-5.10670730980924e-05, -2.7790750024007643e-05, -3.5930319842109445e-05]\n",
      "epoch: 188  loss: 0.006806097164666271  params: [0.46187511 0.05176989 0.98925361]  gradients: [-5.106816609437182e-05, -2.779134483362832e-05, -3.5931088864172455e-05]\n",
      "epoch: 189  loss: 0.006806280535474554  params: [0.46192618 0.05179768 0.98928954]  gradients: [-5.106925906085183e-05, -2.779193962703213e-05, -3.5931857865268885e-05]\n",
      "epoch: 190  loss: 0.006806463915861177  params: [0.46197725 0.05182547 0.98932547]  gradients: [-5.107035199752772e-05, -2.7792534404216508e-05, -3.59326268453954e-05]\n",
      "epoch: 191  loss: 0.006806647305826043  params: [0.46202832 0.05185327 0.98936141]  gradients: [-5.107144490439473e-05, -2.779312916517886e-05, -3.5933395804548665e-05]\n",
      "epoch: 192  loss: 0.006806830705369058  params: [0.46207939 0.05188106 0.98939734]  gradients: [-5.107253778144812e-05, -2.7793723909916616e-05, -3.5934164742725346e-05]\n",
      "epoch: 193  loss: 0.006807014114490112  params: [0.46213046 0.05190886 0.98943328]  gradients: [-5.107363062868314e-05, -2.7794318638427193e-05, -3.593493365992209e-05]\n",
      "epoch: 194  loss: 0.006807197533189113  params: [0.46218154 0.05193665 0.98946921]  gradients: [-5.107472344609506e-05, -2.7794913350708004e-05, -3.593570255613559e-05]\n",
      "epoch: 195  loss: 0.006807380961465966  params: [0.46223261 0.05196445 0.98950515]  gradients: [-5.107581623367913e-05, -2.7795508046756473e-05, -3.593647143136249e-05]\n",
      "epoch: 196  loss: 0.006807564399320561  params: [0.46228369 0.05199224 0.98954109]  gradients: [-5.107690899143062e-05, -2.779610272657002e-05, -3.593724028559945e-05]\n",
      "epoch: 197  loss: 0.006807747846752807  params: [0.46233477 0.05202004 0.98957702]  gradients: [-5.107800171934479e-05, -2.779669739014607e-05, -3.593800911884315e-05]\n",
      "epoch: 198  loss: 0.006807931303762594  params: [0.46238585 0.05204784 0.98961296]  gradients: [-5.107909441741689e-05, -2.7797292037482035e-05, -3.593877793109025e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 199  loss: 0.006808114770349839  params: [0.46243693 0.05207563 0.9896489 ]  gradients: [-5.10801870856422e-05, -2.7797886668575352e-05, -3.593954672233743e-05]\n",
      "epoch: 200  loss: 0.006808298246514423  params: [0.46248801 0.05210343 0.98968484]  gradients: [-5.108127972401596e-05, -2.7798481283423427e-05, -3.594031549258133e-05]\n",
      "epoch: 201  loss: 0.00680848173225626  params: [0.46253909 0.05213123 0.98972078]  gradients: [-5.108237233253344e-05, -2.7799075882023686e-05, -3.594108424181863e-05]\n",
      "epoch: 202  loss: 0.006808665227575245  params: [0.46259018 0.05215903 0.98975673]  gradients: [-5.108346491118991e-05, -2.7799670464373554e-05, -3.5941852970045995e-05]\n",
      "epoch: 203  loss: 0.006808848732471273  params: [0.46264126 0.05218683 0.98979267]  gradients: [-5.108455745998061e-05, -2.780026503047044e-05, -3.5942621677260095e-05]\n",
      "epoch: 204  loss: 0.006809032246944257  params: [0.46269235 0.05221463 0.98982861]  gradients: [-5.108564997890083e-05, -2.7800859580311784e-05, -3.594339036345759e-05]\n",
      "epoch: 205  loss: 0.006809215770994082  params: [0.46274343 0.05224243 0.98986456]  gradients: [-5.108674246794582e-05, -2.7801454113894997e-05, -3.594415902863515e-05]\n",
      "epoch: 206  loss: 0.006809399304620658  params: [0.46279452 0.05227024 0.9899005 ]  gradients: [-5.1087834927110834e-05, -2.7802048631217504e-05, -3.5944927672789436e-05]\n",
      "epoch: 207  loss: 0.006809582847823885  params: [0.46284561 0.05229804 0.98993645]  gradients: [-5.1088927356391164e-05, -2.780264313227673e-05, -3.594569629591714e-05]\n",
      "epoch: 208  loss: 0.006809766400603655  params: [0.4628967  0.05232584 0.98997239]  gradients: [-5.1090019755782046e-05, -2.7803237617070095e-05, -3.59464648980149e-05]\n",
      "epoch: 209  loss: 0.006809949962959876  params: [0.46294779 0.05235365 0.99000834]  gradients: [-5.109111212527876e-05, -2.780383208559502e-05, -3.59472334790794e-05]\n",
      "epoch: 210  loss: 0.006810133534892439  params: [0.46299888 0.05238145 0.99004429]  gradients: [-5.109220446487657e-05, -2.780442653784894e-05, -3.594800203910731e-05]\n",
      "epoch: 211  loss: 0.006810317116401251  params: [0.46304998 0.05240925 0.99008024]  gradients: [-5.1093296774570725e-05, -2.7805020973829257e-05, -3.5948770578095286e-05]\n",
      "epoch: 212  loss: 0.006810500707486208  params: [0.46310107 0.05243706 0.99011619]  gradients: [-5.1094389054356516e-05, -2.7805615393533413e-05, -3.594953909604e-05]\n",
      "epoch: 213  loss: 0.00681068430814721  params: [0.46315217 0.05246487 0.99015214]  gradients: [-5.10954813042292e-05, -2.780620979695883e-05, -3.5950307592938147e-05]\n",
      "epoch: 214  loss: 0.006810867918384161  params: [0.46320326 0.05249267 0.99018809]  gradients: [-5.109657352418402e-05, -2.780680418410292e-05, -3.595107606878636e-05]\n",
      "epoch: 215  loss: 0.0068110515381969525  params: [0.46325436 0.05252048 0.99022404]  gradients: [-5.109766571421627e-05, -2.7807398554963114e-05, -3.5951844523581325e-05]\n",
      "epoch: 216  loss: 0.0068112351675854894  params: [0.46330546 0.05254829 0.99025999]  gradients: [-5.109875787432123e-05, -2.7807992909536847e-05, -3.5952612957319725e-05]\n",
      "epoch: 217  loss: 0.006811418806549666  params: [0.46335656 0.0525761  0.99029595]  gradients: [-5.109985000449413e-05, -2.780858724782153e-05, -3.595338136999821e-05]\n",
      "epoch: 218  loss: 0.0068116024550893915  params: [0.46340766 0.05260391 0.9903319 ]  gradients: [-5.110094210473026e-05, -2.7809181569814594e-05, -3.595414976161346e-05]\n",
      "epoch: 219  loss: 0.006811786113204552  params: [0.46345876 0.05263172 0.99036785]  gradients: [-5.110203417502489e-05, -2.780977587551346e-05, -3.5954918132162144e-05]\n",
      "epoch: 220  loss: 0.006811969780895057  params: [0.46350986 0.05265953 0.99040381]  gradients: [-5.1103126215373285e-05, -2.7810370164915555e-05, -3.595568648164093e-05]\n",
      "epoch: 221  loss: 0.006812153458160798  params: [0.46356097 0.05268734 0.99043977]  gradients: [-5.11042182257707e-05, -2.7810964438018305e-05, -3.5956454810046496e-05]\n",
      "epoch: 222  loss: 0.006812337145001682  params: [0.46361207 0.05271515 0.99047572]  gradients: [-5.110531020621243e-05, -2.7811558694819136e-05, -3.5957223117375515e-05]\n",
      "epoch: 223  loss: 0.0068125208414175995  params: [0.46366318 0.05274296 0.99051168]  gradients: [-5.1106402156693724e-05, -2.7812152935315474e-05, -3.595799140362464e-05]\n",
      "epoch: 224  loss: 0.006812704547408453  params: [0.46371429 0.05277077 0.99054764]  gradients: [-5.110749407720987e-05, -2.7812747159504745e-05, -3.5958759668790564e-05]\n",
      "epoch: 225  loss: 0.006812888262974152  params: [0.4637654  0.05279859 0.9905836 ]  gradients: [-5.110858596775613e-05, -2.781334136738438e-05, -3.595952791286996e-05]\n",
      "epoch: 226  loss: 0.0068130719881145715  params: [0.46381651 0.0528264  0.99061956]  gradients: [-5.1109677828327756e-05, -2.7813935558951787e-05, -3.596029613585949e-05]\n",
      "epoch: 227  loss: 0.006813255722829631  params: [0.46386762 0.05285422 0.99065552]  gradients: [-5.1110769658920056e-05, -2.781452973420442e-05, -3.5961064337755836e-05]\n",
      "epoch: 228  loss: 0.0068134394671192176  params: [0.46391873 0.05288203 0.99069148]  gradients: [-5.111186145952827e-05, -2.7815123893139682e-05, -3.596183251855566e-05]\n",
      "epoch: 229  loss: 0.006813623220983239  params: [0.46396984 0.05290985 0.99072745]  gradients: [-5.11129532301477e-05, -2.781571803575502e-05, -3.5962600678255636e-05]\n",
      "epoch: 230  loss: 0.006813806984421588  params: [0.46402096 0.05293766 0.99076341]  gradients: [-5.111404497077359e-05, -2.781631216204784e-05, -3.596336881685245e-05]\n",
      "epoch: 231  loss: 0.0068139907574341655  params: [0.46407207 0.05296548 0.99079937]  gradients: [-5.111513668140124e-05, -2.7816906272015592e-05, -3.5964136934342774e-05]\n",
      "epoch: 232  loss: 0.00681417454002087  params: [0.46412319 0.0529933  0.99083534]  gradients: [-5.1116228362025893e-05, -2.781750036565569e-05, -3.596490503072327e-05]\n",
      "epoch: 233  loss: 0.006814358332181598  params: [0.4641743  0.05302112 0.9908713 ]  gradients: [-5.111732001264284e-05, -2.7818094442965567e-05, -3.596567310599063e-05]\n",
      "epoch: 234  loss: 0.006814542133916245  params: [0.46422542 0.05304893 0.99090727]  gradients: [-5.111841163324736e-05, -2.7818688503942644e-05, -3.596644116014152e-05]\n",
      "epoch: 235  loss: 0.006814725945224719  params: [0.46427654 0.05307675 0.99094324]  gradients: [-5.1119503223834716e-05, -2.7819282548584355e-05, -3.596720919317261e-05]\n",
      "epoch: 236  loss: 0.00681490976610691  params: [0.46432766 0.05310457 0.99097921]  gradients: [-5.11205947844002e-05, -2.7819876576888132e-05, -3.596797720508058e-05]\n",
      "epoch: 237  loss: 0.006815093596562715  params: [0.46437878 0.05313239 0.99101517]  gradients: [-5.1121686314939065e-05, -2.7820470588851396e-05, -3.596874519586211e-05]\n",
      "epoch: 238  loss: 0.006815277436592041  params: [0.46442991 0.05316021 0.99105114]  gradients: [-5.1122777815446596e-05, -2.782106458447158e-05, -3.596951316551386e-05]\n",
      "epoch: 239  loss: 0.006815461286194777  params: [0.46448103 0.05318804 0.99108711]  gradients: [-5.1123869285918074e-05, -2.7821658563746115e-05, -3.5970281114032534e-05]\n",
      "epoch: 240  loss: 0.00681564514537083  params: [0.46453216 0.05321586 0.99112309]  gradients: [-5.1124960726348763e-05, -2.7822252526672423e-05, -3.597104904141478e-05]\n",
      "epoch: 241  loss: 0.006815829014120089  params: [0.46458328 0.05324368 0.99115906]  gradients: [-5.1126052136733954e-05, -2.7822846473247943e-05, -3.5971816947657294e-05]\n",
      "epoch: 242  loss: 0.006816012892442453  params: [0.46463441 0.05327151 0.99119503]  gradients: [-5.112714351706892e-05, -2.7823440403470105e-05, -3.5972584832756754e-05]\n",
      "epoch: 243  loss: 0.006816196780337829  params: [0.46468554 0.05329933 0.991231  ]  gradients: [-5.112823486734894e-05, -2.782403431733633e-05, -3.597335269670982e-05]\n",
      "epoch: 244  loss: 0.006816380677806104  params: [0.46473667 0.05332715 0.99126698]  gradients: [-5.1129326187569276e-05, -2.7824628214844054e-05, -3.597412053951318e-05]\n",
      "epoch: 245  loss: 0.006816564584847186  params: [0.4647878  0.05335498 0.99130295]  gradients: [-5.113041747772522e-05, -2.7825222095990703e-05, -3.597488836116351e-05]\n",
      "epoch: 246  loss: 0.006816748501460965  params: [0.46483893 0.0533828  0.99133893]  gradients: [-5.1131508737812066e-05, -2.782581596077372e-05, -3.597565616165749e-05]\n",
      "epoch: 247  loss: 0.006816932427647343  params: [0.46489006 0.05341063 0.9913749 ]  gradients: [-5.113259996782506e-05, -2.7826409809190517e-05, -3.5976423940991804e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 248  loss: 0.006817116363406212  params: [0.4649412  0.05343846 0.99141088]  gradients: [-5.11336911677595e-05, -2.7827003641238535e-05, -3.597719169916311e-05]\n",
      "epoch: 249  loss: 0.006817300308737477  params: [0.46499233 0.05346629 0.99144686]  gradients: [-5.1134782337610664e-05, -2.7827597456915213e-05, -3.597795943616811e-05]\n",
      "epoch: 250  loss: 0.006817484263641028  params: [0.46504347 0.05349411 0.99148284]  gradients: [-5.1135873477373836e-05, -2.7828191256217967e-05, -3.597872715200348e-05]\n",
      "epoch: 251  loss: 0.006817668228116767  params: [0.4650946  0.05352194 0.99151882]  gradients: [-5.113696458704429e-05, -2.782878503914424e-05, -3.597949484666589e-05]\n",
      "epoch: 252  loss: 0.006817852202164589  params: [0.46514574 0.05354977 0.9915548 ]  gradients: [-5.11380556666173e-05, -2.7829378805691456e-05, -3.598026252015202e-05]\n",
      "epoch: 253  loss: 0.006818036185784402  params: [0.46519688 0.0535776  0.99159078]  gradients: [-5.113914671608818e-05, -2.782997255585706e-05, -3.598103017245856e-05]\n",
      "epoch: 254  loss: 0.006818220178976088  params: [0.46524802 0.05360543 0.99162676]  gradients: [-5.114023773545217e-05, -2.7830566289638472e-05, -3.5981797803582176e-05]\n",
      "epoch: 255  loss: 0.006818404181739551  params: [0.46529916 0.05363326 0.99166274]  gradients: [-5.114132872470457e-05, -2.7831160007033124e-05, -3.5982565413519555e-05]\n",
      "epoch: 256  loss: 0.006818588194074691  params: [0.4653503  0.0536611  0.99169873]  gradients: [-5.114241968384066e-05, -2.783175370803845e-05, -3.598333300226738e-05]\n",
      "epoch: 257  loss: 0.0068187722159814  params: [0.46540145 0.05368893 0.99173471]  gradients: [-5.114351061285574e-05, -2.7832347392651897e-05, -3.598410056982234e-05]\n",
      "epoch: 258  loss: 0.0068189562474595755  params: [0.46545259 0.05371676 0.9917707 ]  gradients: [-5.1144601511745066e-05, -2.7832941060870875e-05, -3.598486811618109e-05]\n",
      "epoch: 259  loss: 0.006819140288509118  params: [0.46550374 0.05374459 0.99180668]  gradients: [-5.114569238050393e-05, -2.7833534712692834e-05, -3.5985635641340346e-05]\n",
      "epoch: 260  loss: 0.006819324339129924  params: [0.46555488 0.05377243 0.99184267]  gradients: [-5.114678321912763e-05, -2.7834128348115205e-05, -3.598640314529676e-05]\n",
      "epoch: 261  loss: 0.006819508399321889  params: [0.46560603 0.05380026 0.99187865]  gradients: [-5.114787402761144e-05, -2.7834721967135413e-05, -3.598717062804703e-05]\n",
      "epoch: 262  loss: 0.006819692469084912  params: [0.46565718 0.0538281  0.99191464]  gradients: [-5.114896480595064e-05, -2.78353155697509e-05, -3.598793808958784e-05]\n",
      "epoch: 263  loss: 0.006819876548418883  params: [0.46570833 0.05385593 0.99195063]  gradients: [-5.115005555414053e-05, -2.7835909155959105e-05, -3.598870552991587e-05]\n",
      "epoch: 264  loss: 0.00682006063732371  params: [0.46575948 0.05388377 0.99198662]  gradients: [-5.115114627217637e-05, -2.783650272575745e-05, -3.598947294902779e-05]\n",
      "epoch: 265  loss: 0.006820244735799276  params: [0.46581064 0.05391161 0.99202261]  gradients: [-5.115223696005347e-05, -2.783709627914337e-05, -3.59902403469203e-05]\n",
      "epoch: 266  loss: 0.006820428843845493  params: [0.46586179 0.05393945 0.9920586 ]  gradients: [-5.1153327617767106e-05, -2.7837689816114313e-05, -3.599100772359007e-05]\n",
      "epoch: 267  loss: 0.006820612961462248  params: [0.46591294 0.05396728 0.99209459]  gradients: [-5.115441824531257e-05, -2.7838283336667703e-05, -3.5991775079033807e-05]\n",
      "epoch: 268  loss: 0.006820797088649441  params: [0.4659641  0.05399512 0.99213059]  gradients: [-5.115550884268514e-05, -2.783887684080098e-05, -3.5992542413248164e-05]\n",
      "epoch: 269  loss: 0.006820981225406964  params: [0.46601525 0.05402296 0.99216658]  gradients: [-5.115659940988011e-05, -2.783947032851158e-05, -3.599330972622985e-05]\n",
      "epoch: 270  loss: 0.006821165371734719  params: [0.46606641 0.0540508  0.99220257]  gradients: [-5.115768994689277e-05, -2.7840063799796925e-05, -3.5994077017975535e-05]\n",
      "epoch: 271  loss: 0.006821349527632602  params: [0.46611757 0.05407864 0.99223857]  gradients: [-5.11587804537184e-05, -2.784065725465447e-05, -3.599484428848191e-05]\n",
      "epoch: 272  loss: 0.006821533693100505  params: [0.46616873 0.05410648 0.99227456]  gradients: [-5.1159870930352294e-05, -2.7841250693081643e-05, -3.599561153774565e-05]\n",
      "epoch: 273  loss: 0.006821717868138327  params: [0.46621989 0.05413433 0.99231056]  gradients: [-5.116096137678974e-05, -2.7841844115075876e-05, -3.5996378765763456e-05]\n",
      "epoch: 274  loss: 0.0068219020527459625  params: [0.46627105 0.05416217 0.99234656]  gradients: [-5.116205179302604e-05, -2.784243752063462e-05, -3.599714597253201e-05]\n",
      "epoch: 275  loss: 0.006822086246923315  params: [0.46632222 0.05419001 0.99238256]  gradients: [-5.116314217905645e-05, -2.784303090975529e-05, -3.599791315804799e-05]\n",
      "epoch: 276  loss: 0.0068222704506702685  params: [0.46637338 0.05421786 0.99241855]  gradients: [-5.116423253487629e-05, -2.7843624282435335e-05, -3.599868032230809e-05]\n",
      "epoch: 277  loss: 0.006822454663986729  params: [0.46642455 0.0542457  0.99245455]  gradients: [-5.1165322860480845e-05, -2.7844217638672193e-05, -3.599944746530898e-05]\n",
      "epoch: 278  loss: 0.00682263888687259  params: [0.46647571 0.05427354 0.99249055]  gradients: [-5.11664131558654e-05, -2.78448109784633e-05, -3.600021458704737e-05]\n",
      "epoch: 279  loss: 0.006822823119327745  params: [0.46652688 0.05430139 0.99252656]  gradients: [-5.1167503421025244e-05, -2.7845404301806096e-05, -3.600098168751994e-05]\n",
      "epoch: 280  loss: 0.0068230073613520965  params: [0.46657805 0.05432924 0.99256256]  gradients: [-5.116859365595568e-05, -2.7845997608698014e-05, -3.600174876672337e-05]\n",
      "epoch: 281  loss: 0.006823191612945531  params: [0.46662922 0.05435708 0.99259856]  gradients: [-5.1169683860651996e-05, -2.7846590899136493e-05, -3.600251582465435e-05]\n",
      "epoch: 282  loss: 0.006823375874107952  params: [0.46668039 0.05438493 0.99263456]  gradients: [-5.117077403510948e-05, -2.7847184173118975e-05, -3.600328286130957e-05]\n",
      "epoch: 283  loss: 0.0068235601448392484  params: [0.46673156 0.05441278 0.99267057]  gradients: [-5.117186417932342e-05, -2.784777743064289e-05, -3.600404987668571e-05]\n",
      "epoch: 284  loss: 0.006823744425139324  params: [0.46678273 0.05444063 0.99270657]  gradients: [-5.1172954293289116e-05, -2.784837067170568e-05, -3.600481687077947e-05]\n",
      "epoch: 285  loss: 0.006823928715008072  params: [0.46683391 0.05446847 0.99274258]  gradients: [-5.117404437700187e-05, -2.7848963896304792e-05, -3.600558384358754e-05]\n",
      "epoch: 286  loss: 0.006824113014445377  params: [0.46688508 0.05449632 0.99277858]  gradients: [-5.117513443045696e-05, -2.784955710443765e-05, -3.600635079510659e-05]\n",
      "epoch: 287  loss: 0.006824297323451155  params: [0.46693626 0.05452417 0.99281459]  gradients: [-5.1176224453649676e-05, -2.7850150296101698e-05, -3.600711772533333e-05]\n",
      "epoch: 288  loss: 0.006824481642025287  params: [0.46698744 0.05455203 0.9928506 ]  gradients: [-5.117731444657534e-05, -2.785074347129439e-05, -3.6007884634264445e-05]\n",
      "epoch: 289  loss: 0.006824665970167671  params: [0.46703862 0.05457988 0.99288661]  gradients: [-5.117840440922923e-05, -2.7851336630013152e-05, -3.600865152189662e-05]\n",
      "epoch: 290  loss: 0.0068248503078782025  params: [0.4670898  0.05460773 0.99292262]  gradients: [-5.1179494341606636e-05, -2.785192977225542e-05, -3.600941838822654e-05]\n",
      "epoch: 291  loss: 0.006825034655156781  params: [0.46714098 0.05463558 0.99295863]  gradients: [-5.118058424370286e-05, -2.7852522898018644e-05, -3.601018523325089e-05]\n",
      "epoch: 292  loss: 0.006825219012003297  params: [0.46719216 0.05466343 0.99299464]  gradients: [-5.118167411551319e-05, -2.7853116007300255e-05, -3.601095205696639e-05]\n",
      "epoch: 293  loss: 0.006825403378417655  params: [0.46724334 0.05469129 0.99303065]  gradients: [-5.118276395703295e-05, -2.7853709100097707e-05, -3.60117188593697e-05]\n",
      "epoch: 294  loss: 0.00682558775439973  params: [0.46729452 0.05471914 0.99306666]  gradients: [-5.118385376825741e-05, -2.7854302176408422e-05, -3.601248564045753e-05]\n",
      "epoch: 295  loss: 0.006825772139949433  params: [0.46734571 0.054747   0.99310268]  gradients: [-5.118494354918187e-05, -2.785489523622986e-05, -3.601325240022656e-05]\n",
      "epoch: 296  loss: 0.006825956535066662  params: [0.4673969  0.05477485 0.99313869]  gradients: [-5.118603329980163e-05, -2.785548827955944e-05, -3.601401913867348e-05]\n",
      "epoch: 297  loss: 0.006826140939751307  params: [0.46744808 0.05480271 0.9931747 ]  gradients: [-5.118712302011199e-05, -2.7856081306394626e-05, -3.601478585579499e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 298  loss: 0.006826325354003256  params: [0.46749927 0.05483057 0.99321072]  gradients: [-5.118821271010827e-05, -2.7856674316732852e-05, -3.60155525515878e-05]\n",
      "epoch: 299  loss: 0.006826509777822417  params: [0.46755046 0.05485842 0.99324674]  gradients: [-5.118930236978573e-05, -2.785726731057155e-05, -3.6016319226048565e-05]\n",
      "epoch: 300  loss: 0.006826694211208671  params: [0.46760165 0.05488628 0.99328275]  gradients: [-5.1190391999139694e-05, -2.7857860287908173e-05, -3.601708587917399e-05]\n",
      "epoch: 301  loss: 0.006826878654161922  params: [0.46765284 0.05491414 0.99331877]  gradients: [-5.119148159816545e-05, -2.7858453248740162e-05, -3.6017852510960775e-05]\n",
      "epoch: 302  loss: 0.006827063106682067  params: [0.46770403 0.054942   0.99335479]  gradients: [-5.119257116685831e-05, -2.785904619306495e-05, -3.601861912140561e-05]\n",
      "epoch: 303  loss: 0.006827247568768995  params: [0.46775523 0.05496986 0.99339081]  gradients: [-5.119366070521356e-05, -2.7859639120879984e-05, -3.601938571050519e-05]\n",
      "epoch: 304  loss: 0.0068274320404226015  params: [0.46780642 0.05499772 0.99342683]  gradients: [-5.1194750213226514e-05, -2.7860232032182714e-05, -3.602015227825621e-05]\n",
      "epoch: 305  loss: 0.0068276165216427775  params: [0.46785762 0.05502558 0.99346285]  gradients: [-5.119583969089246e-05, -2.7860824926970575e-05, -3.602091882465536e-05]\n",
      "epoch: 306  loss: 0.00682780101242943  params: [0.46790882 0.05505344 0.99349887]  gradients: [-5.1196929138206724e-05, -2.7861417805241016e-05, -3.602168534969934e-05]\n",
      "epoch: 307  loss: 0.0068279855127824415  params: [0.46796001 0.0550813  0.99353489]  gradients: [-5.119801855516457e-05, -2.7862010666991472e-05, -3.6022451853384836e-05]\n",
      "epoch: 308  loss: 0.006828170022701719  params: [0.46801121 0.05510916 0.99357092]  gradients: [-5.119910794176133e-05, -2.786260351221939e-05, -3.602321833570854e-05]\n",
      "epoch: 309  loss: 0.006828354542187139  params: [0.46806241 0.05513703 0.99360694]  gradients: [-5.120019729799231e-05, -2.7863196340922223e-05, -3.602398479666717e-05]\n",
      "epoch: 310  loss: 0.006828539071238609  params: [0.46811361 0.05516489 0.99364297]  gradients: [-5.1201286623852785e-05, -2.78637891530974e-05, -3.602475123625739e-05]\n",
      "epoch: 311  loss: 0.0068287236098560095  params: [0.46816482 0.05519276 0.99367899]  gradients: [-5.120237591933808e-05, -2.786438194874237e-05, -3.602551765447592e-05]\n",
      "epoch: 312  loss: 0.006828908158039256  params: [0.46821602 0.05522062 0.99371502]  gradients: [-5.1203465184443494e-05, -2.7864974727854585e-05, -3.602628405131944e-05]\n",
      "epoch: 313  loss: 0.006829092715788224  params: [0.46826722 0.05524849 0.99375104]  gradients: [-5.120455441916433e-05, -2.7865567490431483e-05, -3.602705042678466e-05]\n",
      "epoch: 314  loss: 0.0068292772831028276  params: [0.46831843 0.05527635 0.99378707]  gradients: [-5.1205643623495905e-05, -2.7866160236470516e-05, -3.602781678086827e-05]\n",
      "epoch: 315  loss: 0.006829461859982942  params: [0.46836964 0.05530422 0.9938231 ]  gradients: [-5.12067327974335e-05, -2.7866752965969117e-05, -3.602858311356696e-05]\n",
      "epoch: 316  loss: 0.006829646446428468  params: [0.46842084 0.05533209 0.99385913]  gradients: [-5.1207821940972444e-05, -2.786734567892474e-05, -3.6029349424877444e-05]\n",
      "epoch: 317  loss: 0.006829831042439299  params: [0.46847205 0.05535995 0.99389516]  gradients: [-5.1208911054108025e-05, -2.7867938375334822e-05, -3.60301157147964e-05]\n",
      "epoch: 318  loss: 0.006830015648015329  params: [0.46852326 0.05538782 0.99393119]  gradients: [-5.121000013683556e-05, -2.786853105519682e-05, -3.603088198332054e-05]\n",
      "epoch: 319  loss: 0.006830200263156463  params: [0.46857447 0.05541569 0.99396722]  gradients: [-5.121108918915035e-05, -2.7869123718508173e-05, -3.603164823044654e-05]\n",
      "epoch: 320  loss: 0.0068303848878625735  params: [0.46862569 0.05544356 0.99400326]  gradients: [-5.12121782110477e-05, -2.786971636526633e-05, -3.603241445617113e-05]\n",
      "epoch: 321  loss: 0.006830569522133563  params: [0.4686769  0.05547143 0.99403929]  gradients: [-5.121326720252293e-05, -2.7870308995468734e-05, -3.603318066049099e-05]\n",
      "epoch: 322  loss: 0.006830754165969336  params: [0.46872811 0.0554993  0.99407532]  gradients: [-5.1214356163571334e-05, -2.7870901609112828e-05, -3.6033946843402815e-05]\n",
      "epoch: 323  loss: 0.006830938819369772  params: [0.46877933 0.05552717 0.99411136]  gradients: [-5.121544509418822e-05, -2.787149420619607e-05, -3.603471300490331e-05]\n",
      "epoch: 324  loss: 0.00683112348233478  params: [0.46883055 0.05555505 0.99414739]  gradients: [-5.121653399436891e-05, -2.7872086786715896e-05, -3.603547914498918e-05]\n",
      "epoch: 325  loss: 0.006831308154864234  params: [0.46888176 0.05558292 0.99418343]  gradients: [-5.12176228641087e-05, -2.787267935066976e-05, -3.6036245263657106e-05]\n",
      "epoch: 326  loss: 0.00683149283695804  params: [0.46893298 0.05561079 0.99421947]  gradients: [-5.1218711703402914e-05, -2.787327189805511e-05, -3.60370113609038e-05]\n",
      "epoch: 327  loss: 0.006831677528616091  params: [0.4689842  0.05563867 0.9942555 ]  gradients: [-5.1219800512246834e-05, -2.7873864428869382e-05, -3.6037777436725966e-05]\n",
      "epoch: 328  loss: 0.006831862229838278  params: [0.46903542 0.05566654 0.99429154]  gradients: [-5.12208892906358e-05, -2.787445694311004e-05, -3.603854349112031e-05]\n",
      "epoch: 329  loss: 0.006832046940624492  params: [0.46908665 0.05569442 0.99432758]  gradients: [-5.1221978038565103e-05, -2.7875049440774523e-05, -3.603930952408351e-05]\n",
      "epoch: 330  loss: 0.006832231660974633  params: [0.46913787 0.05572229 0.99436362]  gradients: [-5.1223066756030066e-05, -2.787564192186028e-05, -3.6040075535612284e-05]\n",
      "epoch: 331  loss: 0.006832416390888591  params: [0.46918909 0.05575017 0.99439966]  gradients: [-5.122415544302601e-05, -2.787623438636476e-05, -3.604084152570333e-05]\n",
      "epoch: 332  loss: 0.006832601130366256  params: [0.46924032 0.05577804 0.9944357 ]  gradients: [-5.122524409954821e-05, -2.7876826834285408e-05, -3.604160749435334e-05]\n",
      "epoch: 333  loss: 0.006832785879407523  params: [0.46929154 0.05580592 0.99447175]  gradients: [-5.1226332725592006e-05, -2.787741926561968e-05, -3.604237344155903e-05]\n",
      "epoch: 334  loss: 0.006832970638012288  params: [0.46934277 0.0558338  0.99450779]  gradients: [-5.122742132115271e-05, -2.787801168036502e-05, -3.604313936731709e-05]\n",
      "epoch: 335  loss: 0.006833155406180439  params: [0.469394   0.05586168 0.99454383]  gradients: [-5.122850988622562e-05, -2.787860407851888e-05, -3.604390527162423e-05]\n",
      "epoch: 336  loss: 0.006833340183911882  params: [0.46944523 0.05588956 0.99457988]  gradients: [-5.122959842080607e-05, -2.787919646007871e-05, -3.604467115447715e-05]\n",
      "epoch: 337  loss: 0.006833524971206494  params: [0.46949646 0.05591744 0.99461592]  gradients: [-5.1230686924889355e-05, -2.787978882504195e-05, -3.604543701587255e-05]\n",
      "epoch: 338  loss: 0.0068337097680641725  params: [0.46954769 0.05594532 0.99465197]  gradients: [-5.123177539847079e-05, -2.788038117340606e-05, -3.604620285580713e-05]\n",
      "epoch: 339  loss: 0.006833894574484813  params: [0.46959893 0.0559732  0.99468802]  gradients: [-5.123286384154571e-05, -2.788097350516849e-05, -3.604696867427762e-05]\n",
      "epoch: 340  loss: 0.0068340793904683094  params: [0.46965016 0.05600108 0.99472406]  gradients: [-5.1233952254109416e-05, -2.788156582032669e-05, -3.604773447128068e-05]\n",
      "epoch: 341  loss: 0.006834264216014554  params: [0.46970139 0.05602896 0.99476011]  gradients: [-5.1235040636157206e-05, -2.78821581188781e-05, -3.604850024681303e-05]\n",
      "epoch: 342  loss: 0.006834449051123435  params: [0.46975263 0.05605685 0.99479616]  gradients: [-5.1236128987684434e-05, -2.788275040082019e-05, -3.60492660008714e-05]\n",
      "epoch: 343  loss: 0.006834633895794847  params: [0.46980387 0.05608473 0.99483221]  gradients: [-5.1237217308686376e-05, -2.7883342666150392e-05, -3.605003173345246e-05]\n",
      "epoch: 344  loss: 0.006834818750028687  params: [0.46985511 0.05611261 0.99486826]  gradients: [-5.1238305599158365e-05, -2.7883934914866163e-05, -3.605079744455294e-05]\n",
      "epoch: 345  loss: 0.006835003613824839  params: [0.46990635 0.0561405  0.99490432]  gradients: [-5.123939385909574e-05, -2.7884527146964967e-05, -3.605156313416953e-05]\n",
      "epoch: 346  loss: 0.0068351884871832025  params: [0.46995759 0.05616838 0.99494037]  gradients: [-5.1240482088493776e-05, -2.7885119362444236e-05, -3.605232880229893e-05]\n",
      "epoch: 347  loss: 0.006835373370103669  params: [0.47000883 0.05619627 0.99497642]  gradients: [-5.1241570287347815e-05, -2.7885711561301436e-05, -3.605309444893787e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 348  loss: 0.006835558262586134  params: [0.47006007 0.05622415 0.99501247]  gradients: [-5.1242658455653176e-05, -2.7886303743534008e-05, -3.605386007408302e-05]\n",
      "epoch: 349  loss: 0.006835743164630482  params: [0.47011131 0.05625204 0.99504853]  gradients: [-5.124374659340517e-05, -2.7886895909139414e-05, -3.605462567773112e-05]\n",
      "epoch: 350  loss: 0.006835928076236606  params: [0.47016256 0.05627993 0.99508458]  gradients: [-5.1244834700599125e-05, -2.78874880581151e-05, -3.605539125987886e-05]\n",
      "epoch: 351  loss: 0.006836112997404407  params: [0.4702138  0.05630782 0.99512064]  gradients: [-5.124592277723035e-05, -2.7888080190458526e-05, -3.6056156820522946e-05]\n",
      "epoch: 352  loss: 0.00683629792813377  params: [0.47026505 0.05633571 0.9951567 ]  gradients: [-5.124701082329416e-05, -2.788867230616713e-05, -3.605692235966008e-05]\n",
      "epoch: 353  loss: 0.006836482868424588  params: [0.4703163  0.0563636  0.99519276]  gradients: [-5.124809883878589e-05, -2.7889264405238382e-05, -3.605768787728699e-05]\n",
      "epoch: 354  loss: 0.006836667818276753  params: [0.47036755 0.05639148 0.99522881]  gradients: [-5.124918682370085e-05, -2.7889856487669728e-05, -3.6058453373400365e-05]\n",
      "epoch: 355  loss: 0.006836852777690159  params: [0.4704188  0.05641938 0.99526487]  gradients: [-5.1250274778034363e-05, -2.7890448553458617e-05, -3.605921884799691e-05]\n",
      "epoch: 356  loss: 0.006837037746664699  params: [0.47047005 0.05644727 0.99530093]  gradients: [-5.125136270178176e-05, -2.7891040602602512e-05, -3.605998430107335e-05]\n",
      "epoch: 357  loss: 0.006837222725200259  params: [0.4705213  0.05647516 0.99533699]  gradients: [-5.1252450594938345e-05, -2.7891632635098863e-05, -3.606074973262638e-05]\n",
      "epoch: 358  loss: 0.006837407713296735  params: [0.47057256 0.05650305 0.99537306]  gradients: [-5.1253538457499446e-05, -2.789222465094512e-05, -3.606151514265271e-05]\n",
      "epoch: 359  loss: 0.006837592710954022  params: [0.47062381 0.05653094 0.99540912]  gradients: [-5.125462628946039e-05, -2.7892816650138745e-05, -3.606228053114905e-05]\n",
      "epoch: 360  loss: 0.00683777771817201  params: [0.47067507 0.05655884 0.99544518]  gradients: [-5.12557140908165e-05, -2.7893408632677184e-05, -3.606304589811211e-05]\n",
      "epoch: 361  loss: 0.006837962734950578  params: [0.47072632 0.05658673 0.99548124]  gradients: [-5.125680186156308e-05, -2.7894000598557896e-05, -3.606381124353861e-05]\n",
      "epoch: 362  loss: 0.006838147761289636  params: [0.47077758 0.05661463 0.99551731]  gradients: [-5.1257889601695484e-05, -2.789459254777834e-05, -3.606457656742523e-05]\n",
      "epoch: 363  loss: 0.006838332797189067  params: [0.47082884 0.05664252 0.99555337]  gradients: [-5.125897731120902e-05, -2.7895184480335965e-05, -3.6065341869768715e-05]\n",
      "epoch: 364  loss: 0.006838517842648764  params: [0.4708801  0.05667042 0.99558944]  gradients: [-5.1260064990099e-05, -2.7895776396228222e-05, -3.606610715056575e-05]\n",
      "epoch: 365  loss: 0.006838702897668621  params: [0.47093136 0.05669831 0.99562551]  gradients: [-5.126115263836077e-05, -2.7896368295452575e-05, -3.606687240981305e-05]\n",
      "epoch: 366  loss: 0.006838887962248525  params: [0.47098262 0.05672621 0.99566157]  gradients: [-5.1262240255989655e-05, -2.7896960178006486e-05, -3.6067637647507345e-05]\n",
      "epoch: 367  loss: 0.006839073036388363  params: [0.47103389 0.05675411 0.99569764]  gradients: [-5.126332784298096e-05, -2.7897552043887393e-05, -3.606840286364532e-05]\n",
      "epoch: 368  loss: 0.006839258120088041  params: [0.47108515 0.05678201 0.99573371]  gradients: [-5.126441539933002e-05, -2.7898143893092773e-05, -3.606916805822369e-05]\n",
      "epoch: 369  loss: 0.0068394432133474375  params: [0.47113642 0.0568099  0.99576978]  gradients: [-5.126550292503218e-05, -2.7898735725620065e-05, -3.606993323123918e-05]\n",
      "epoch: 370  loss: 0.0068396283161664476  params: [0.47118768 0.0568378  0.99580585]  gradients: [-5.1266590420082725e-05, -2.7899327541466733e-05, -3.607069838268849e-05]\n",
      "epoch: 371  loss: 0.006839813428544968  params: [0.47123895 0.0568657  0.99584192]  gradients: [-5.1267677884477015e-05, -2.789991934063023e-05, -3.6071463512568334e-05]\n",
      "epoch: 372  loss: 0.006839998550482883  params: [0.47129022 0.0568936  0.995878  ]  gradients: [-5.126876531821038e-05, -2.790051112310802e-05, -3.607222862087545e-05]\n",
      "epoch: 373  loss: 0.006840183681980082  params: [0.47134149 0.0569215  0.99591407]  gradients: [-5.126985272127812e-05, -2.7901102888897552e-05, -3.6072993707606505e-05]\n",
      "epoch: 374  loss: 0.006840368823036461  params: [0.47139276 0.05694941 0.99595014]  gradients: [-5.1270940093675596e-05, -2.7901694637996293e-05, -3.607375877275824e-05]\n",
      "epoch: 375  loss: 0.006840553973651914  params: [0.47144403 0.05697731 0.99598622]  gradients: [-5.127202743539811e-05, -2.7902286370401694e-05, -3.6074523816327364e-05]\n",
      "epoch: 376  loss: 0.006840739133826321  params: [0.47149531 0.05700521 0.99602229]  gradients: [-5.127311474644101e-05, -2.7902878086111216e-05, -3.607528883831059e-05]\n",
      "epoch: 377  loss: 0.006840924303559585  params: [0.47154658 0.05703311 0.99605837]  gradients: [-5.12742020267996e-05, -2.7903469785122313e-05, -3.6076053838704624e-05]\n",
      "epoch: 378  loss: 0.006841109482851582  params: [0.47159786 0.05706102 0.99609445]  gradients: [-5.127528927646924e-05, -2.7904061467432448e-05, -3.6076818817506194e-05]\n",
      "epoch: 379  loss: 0.006841294671702225  params: [0.47164913 0.05708892 0.99613052]  gradients: [-5.127637649544524e-05, -2.790465313303908e-05, -3.6077583774712016e-05]\n",
      "epoch: 380  loss: 0.00684147987011138  params: [0.47170041 0.05711683 0.9961666 ]  gradients: [-5.127746368372293e-05, -2.7905244781939663e-05, -3.607834871031878e-05]\n",
      "epoch: 381  loss: 0.006841665078078957  params: [0.47175169 0.05714473 0.99620268]  gradients: [-5.127855084129765e-05, -2.7905836414131655e-05, -3.607911362432322e-05]\n",
      "epoch: 382  loss: 0.006841850295604835  params: [0.47180297 0.05717264 0.99623876]  gradients: [-5.1279637968164734e-05, -2.7906428029612524e-05, -3.607987851672205e-05]\n",
      "epoch: 383  loss: 0.006842035522688914  params: [0.47185425 0.05720055 0.99627484]  gradients: [-5.1280725064319515e-05, -2.7907019628379724e-05, -3.608064338751198e-05]\n",
      "epoch: 384  loss: 0.0068422207593310715  params: [0.47190553 0.05722846 0.99631092]  gradients: [-5.12818121297573e-05, -2.7907611210430713e-05, -3.6081408236689735e-05]\n",
      "epoch: 385  loss: 0.006842406005531207  params: [0.47195681 0.05725636 0.99634701]  gradients: [-5.128289916447345e-05, -2.7908202775762955e-05, -3.608217306425202e-05]\n",
      "epoch: 386  loss: 0.006842591261289214  params: [0.4720081  0.05728427 0.99638309]  gradients: [-5.128398616846328e-05, -2.7908794324373906e-05, -3.608293787019555e-05]\n",
      "epoch: 387  loss: 0.00684277652660497  params: [0.47205938 0.05731218 0.99641917]  gradients: [-5.128507314172214e-05, -2.7909385856261032e-05, -3.608370265451705e-05]\n",
      "epoch: 388  loss: 0.006842961801478379  params: [0.47211067 0.05734009 0.99645526]  gradients: [-5.128616008424535e-05, -2.7909977371421788e-05, -3.608446741721325e-05]\n",
      "epoch: 389  loss: 0.006843147085909328  params: [0.47216196 0.057368   0.99649134]  gradients: [-5.128724699602825e-05, -2.7910568869853637e-05, -3.608523215828084e-05]\n",
      "epoch: 390  loss: 0.0068433323798977016  params: [0.47221324 0.05739591 0.99652743]  gradients: [-5.128833387706616e-05, -2.7911160351554037e-05, -3.6085996877716535e-05]\n",
      "epoch: 391  loss: 0.006843517683443396  params: [0.47226453 0.05742383 0.99656351]  gradients: [-5.1289420727354434e-05, -2.791175181652045e-05, -3.608676157551708e-05]\n",
      "epoch: 392  loss: 0.006843702996546294  params: [0.47231582 0.05745174 0.9965996 ]  gradients: [-5.129050754688839e-05, -2.791234326475034e-05, -3.6087526251679165e-05]\n",
      "epoch: 393  loss: 0.006843888319206295  params: [0.47236712 0.05747965 0.99663569]  gradients: [-5.129159433566339e-05, -2.7912934696241175e-05, -3.608829090619953e-05]\n",
      "epoch: 394  loss: 0.0068440736514232825  params: [0.47241841 0.05750756 0.99667178]  gradients: [-5.129268109367475e-05, -2.7913526110990404e-05, -3.608905553907489e-05]\n",
      "epoch: 395  loss: 0.0068442589931971415  params: [0.4724697  0.05753548 0.99670787]  gradients: [-5.12937678209178e-05, -2.7914117508995492e-05, -3.6089820150301954e-05]\n",
      "epoch: 396  loss: 0.006844444344527784  params: [0.472521   0.05756339 0.99674396]  gradients: [-5.1294854517387894e-05, -2.791470889025391e-05, -3.6090584739877444e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 397  loss: 0.006844629705415077  params: [0.47257229 0.05759131 0.99678005]  gradients: [-5.1295941183080354e-05, -2.791530025476311e-05, -3.609134930779809e-05]\n",
      "epoch: 398  loss: 0.006844815075858913  params: [0.47262359 0.05761922 0.99681614]  gradients: [-5.129702781799053e-05, -2.791589160252056e-05, -3.609211385406059e-05]\n",
      "epoch: 399  loss: 0.006845000455859191  params: [0.47267489 0.05764714 0.99685224]  gradients: [-5.129811442211377e-05, -2.7916482933523726e-05, -3.609287837866169e-05]\n",
      "epoch: 400  loss: 0.006845185845415793  params: [0.47272619 0.05767506 0.99688833]  gradients: [-5.1299200995445376e-05, -2.791707424777006e-05, -3.609364288159808e-05]\n",
      "epoch: 401  loss: 0.006845371244528618  params: [0.47277749 0.05770298 0.99692442]  gradients: [-5.1300287537980723e-05, -2.7917665545257036e-05, -3.609440736286651e-05]\n",
      "epoch: 402  loss: 0.006845556653197541  params: [0.47282879 0.05773089 0.99696052]  gradients: [-5.130137404971512e-05, -2.791825682598211e-05, -3.6095171822463686e-05]\n",
      "epoch: 403  loss: 0.006845742071422466  params: [0.47288009 0.05775881 0.99699662]  gradients: [-5.130246053064393e-05, -2.791884808994275e-05, -3.609593626038632e-05]\n",
      "epoch: 404  loss: 0.006845927499203275  params: [0.47293139 0.05778673 0.99703271]  gradients: [-5.1303546980762475e-05, -2.791943933713642e-05, -3.609670067663115e-05]\n",
      "epoch: 405  loss: 0.006846112936539858  params: [0.4729827  0.05781465 0.99706881]  gradients: [-5.130463340006613e-05, -2.7920030567560592e-05, -3.60974650711949e-05]\n",
      "epoch: 406  loss: 0.0068462983834321044  params: [0.473034   0.05784257 0.99710491]  gradients: [-5.1305719788550185e-05, -2.7920621781212716e-05, -3.609822944407427e-05]\n",
      "epoch: 407  loss: 0.006846483839879907  params: [0.47308531 0.05787049 0.99714101]  gradients: [-5.130680614621002e-05, -2.792121297809026e-05, -3.6098993795266e-05]\n",
      "epoch: 408  loss: 0.006846669305883147  params: [0.47313662 0.05789842 0.99717711]  gradients: [-5.1307892473040954e-05, -2.7921804158190693e-05, -3.60997581247668e-05]\n",
      "epoch: 409  loss: 0.006846854781441725  params: [0.47318793 0.05792634 0.99721321]  gradients: [-5.130897876903834e-05, -2.7922395321511483e-05, -3.610052243257341e-05]\n",
      "epoch: 410  loss: 0.006847040266555517  params: [0.47323924 0.05795426 0.99724931]  gradients: [-5.1310065034197525e-05, -2.7922986468050083e-05, -3.6101286718682534e-05]\n",
      "epoch: 411  loss: 0.006847225761224429  params: [0.47329055 0.05798218 0.99728541]  gradients: [-5.131115126851383e-05, -2.7923577597803967e-05, -3.6102050983090905e-05]\n",
      "epoch: 412  loss: 0.006847411265448333  params: [0.47334186 0.05801011 0.99732151]  gradients: [-5.1312237471982616e-05, -2.7924168710770596e-05, -3.610281522579524e-05]\n",
      "epoch: 413  loss: 0.006847596779227125  params: [0.47339318 0.05803803 0.99735762]  gradients: [-5.1313323644599234e-05, -2.7924759806947454e-05, -3.610357944679228e-05]\n",
      "epoch: 414  loss: 0.006847782302560697  params: [0.47344449 0.05806596 0.99739372]  gradients: [-5.131440978635902e-05, -2.7925350886331988e-05, -3.610434364607874e-05]\n",
      "epoch: 415  loss: 0.006847967835448934  params: [0.47349581 0.05809389 0.99742983]  gradients: [-5.1315495897257305e-05, -2.7925941948921665e-05, -3.610510782365133e-05]\n",
      "epoch: 416  loss: 0.00684815337789173  params: [0.47354712 0.05812181 0.99746593]  gradients: [-5.131658197728945e-05, -2.7926532994713957e-05, -3.610587197950679e-05]\n",
      "epoch: 417  loss: 0.006848338929888969  params: [0.47359844 0.05814974 0.99750204]  gradients: [-5.13176680264508e-05, -2.7927124023706333e-05, -3.610663611364184e-05]\n",
      "epoch: 418  loss: 0.006848524491440531  params: [0.47364976 0.05817767 0.99753815]  gradients: [-5.131875404473668e-05, -2.7927715035896255e-05, -3.61074002260532e-05]\n",
      "epoch: 419  loss: 0.006848710062546324  params: [0.47370108 0.05820559 0.99757425]  gradients: [-5.131984003214246e-05, -2.7928306031281192e-05, -3.610816431673761e-05]\n",
      "epoch: 420  loss: 0.006848895643206223  params: [0.4737524  0.05823352 0.99761036]  gradients: [-5.132092598866346e-05, -2.7928897009858604e-05, -3.610892838569177e-05]\n",
      "epoch: 421  loss: 0.006849081233420121  params: [0.47380372 0.05826145 0.99764647]  gradients: [-5.132201191429506e-05, -2.792948797162597e-05, -3.610969243291243e-05]\n",
      "epoch: 422  loss: 0.006849266833187904  params: [0.47385504 0.05828938 0.99768258]  gradients: [-5.1323097809032584e-05, -2.793007891658076e-05, -3.611045645839632e-05]\n",
      "epoch: 423  loss: 0.006849452442509459  params: [0.47390637 0.05831731 0.99771869]  gradients: [-5.1324183672871405e-05, -2.7930669844720437e-05, -3.611122046214015e-05]\n",
      "epoch: 424  loss: 0.006849638061384685  params: [0.47395769 0.05834525 0.99775481]  gradients: [-5.1325269505806825e-05, -2.7931260756042458e-05, -3.611198444414065e-05]\n",
      "epoch: 425  loss: 0.006849823689813457  params: [0.47400902 0.05837318 0.99779092]  gradients: [-5.132635530783423e-05, -2.7931851650544304e-05, -3.6112748404394544e-05]\n",
      "epoch: 426  loss: 0.0068500093277956775  params: [0.47406035 0.05840111 0.99782703]  gradients: [-5.132744107894895e-05, -2.7932442528223443e-05, -3.611351234289856e-05]\n",
      "epoch: 427  loss: 0.00685019497533122  params: [0.47411168 0.05842904 0.99786315]  gradients: [-5.1328526819146366e-05, -2.793303338907735e-05, -3.611427625964945e-05]\n",
      "epoch: 428  loss: 0.006850380632419981  params: [0.47416301 0.05845698 0.99789926]  gradients: [-5.132961252842177e-05, -2.7933624233103476e-05, -3.611504015464391e-05]\n",
      "epoch: 429  loss: 0.006850566299061846  params: [0.47421434 0.05848491 0.99793538]  gradients: [-5.133069820677057e-05, -2.7934215060299302e-05, -3.611580402787868e-05]\n",
      "epoch: 430  loss: 0.00685075197525671  params: [0.47426567 0.05851285 0.99797149]  gradients: [-5.133178385418808e-05, -2.7934805870662303e-05, -3.611656787935049e-05]\n",
      "epoch: 431  loss: 0.006850937661004447  params: [0.474317   0.05854078 0.99800761]  gradients: [-5.133286947066966e-05, -2.7935396664189934e-05, -3.611733170905606e-05]\n",
      "epoch: 432  loss: 0.0068511233563049575  params: [0.47436833 0.05856872 0.99804373]  gradients: [-5.133395505621067e-05, -2.793598744087968e-05, -3.611809551699213e-05]\n",
      "epoch: 433  loss: 0.006851309061158128  params: [0.47441967 0.05859665 0.99807985]  gradients: [-5.133504061080645e-05, -2.7936578200729003e-05, -3.6118859303155435e-05]\n",
      "epoch: 434  loss: 0.006851494775563836  params: [0.47447101 0.05862459 0.99811597]  gradients: [-5.1336126134452354e-05, -2.7937168943735375e-05, -3.611962306754269e-05]\n",
      "epoch: 435  loss: 0.006851680499521979  params: [0.47452234 0.05865253 0.99815209]  gradients: [-5.133721162714373e-05, -2.793775966989626e-05, -3.612038681015062e-05]\n",
      "epoch: 436  loss: 0.006851866233032447  params: [0.47457368 0.05868047 0.99818821]  gradients: [-5.133829708887595e-05, -2.7938350379209142e-05, -3.612115053097598e-05]\n",
      "epoch: 437  loss: 0.006852051976095122  params: [0.47462502 0.05870841 0.99822433]  gradients: [-5.133938251964434e-05, -2.793894107167148e-05, -3.6121914230015465e-05]\n",
      "epoch: 438  loss: 0.006852237728709894  params: [0.47467636 0.05873634 0.99826045]  gradients: [-5.134046791944428e-05, -2.793953174728076e-05, -3.612267790726584e-05]\n",
      "epoch: 439  loss: 0.0068524234908766465  params: [0.4747277  0.05876428 0.99829658]  gradients: [-5.1341553288271104e-05, -2.794012240603444e-05, -3.6123441562723824e-05]\n",
      "epoch: 440  loss: 0.0068526092625952745  params: [0.47477905 0.05879223 0.9983327 ]  gradients: [-5.134263862612018e-05, -2.7940713047929993e-05, -3.612420519638615e-05]\n",
      "epoch: 441  loss: 0.006852795043865664  params: [0.47483039 0.05882017 0.99836883]  gradients: [-5.134372393298685e-05, -2.79413036729649e-05, -3.612496880824955e-05]\n",
      "epoch: 442  loss: 0.00685298083468769  params: [0.47488173 0.05884811 0.99840495]  gradients: [-5.134480920886648e-05, -2.7941894281136626e-05, -3.612573239831074e-05]\n",
      "epoch: 443  loss: 0.006853166635061259  params: [0.47493308 0.05887605 0.99844108]  gradients: [-5.134589445375443e-05, -2.7942484872442647e-05, -3.612649596656647e-05]\n",
      "epoch: 444  loss: 0.006853352444986244  params: [0.47498443 0.05890399 0.99847721]  gradients: [-5.134697966764602e-05, -2.7943075446880426e-05, -3.6127259513013456e-05]\n",
      "epoch: 445  loss: 0.006853538264462538  params: [0.47503577 0.05893194 0.99851333]  gradients: [-5.134806485053665e-05, -2.7943666004447447e-05, -3.6128023037648446e-05]\n",
      "epoch: 446  loss: 0.006853724093490027  params: [0.47508712 0.05895988 0.99854946]  gradients: [-5.134915000242165e-05, -2.7944256545141178e-05, -3.612878654046817e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 447  loss: 0.006853909932068599  params: [0.47513847 0.05898783 0.99858559]  gradients: [-5.135023512329639e-05, -2.7944847068959093e-05, -3.612955002146936e-05]\n",
      "epoch: 448  loss: 0.006854095780198152  params: [0.47518983 0.05901577 0.99862172]  gradients: [-5.1351320213156226e-05, -2.7945437575898673e-05, -3.6130313480648755e-05]\n",
      "epoch: 449  loss: 0.006854281637878549  params: [0.47524118 0.05904372 0.99865785]  gradients: [-5.135240527199651e-05, -2.7946028065957378e-05, -3.613107691800307e-05]\n",
      "epoch: 450  loss: 0.006854467505109699  params: [0.47529253 0.05907167 0.99869399]  gradients: [-5.135349029981262e-05, -2.7946618539132697e-05, -3.613184033352907e-05]\n",
      "epoch: 451  loss: 0.0068546533818914755  params: [0.47534389 0.05909961 0.99873012]  gradients: [-5.135457529659988e-05, -2.794720899542209e-05, -3.613260372722345e-05]\n",
      "epoch: 452  loss: 0.0068548392682237745  params: [0.47539524 0.05912756 0.99876625]  gradients: [-5.135566026235366e-05, -2.7947799434823033e-05, -3.613336709908298e-05]\n",
      "epoch: 453  loss: 0.006855025164106473  params: [0.4754466  0.05915551 0.99880239]  gradients: [-5.135674519706934e-05, -2.7948389857333015e-05, -3.6134130449104376e-05]\n",
      "epoch: 454  loss: 0.006855211069539466  params: [0.47549796 0.05918346 0.99883852]  gradients: [-5.135783010074226e-05, -2.794898026294949e-05, -3.613489377728438e-05]\n",
      "epoch: 455  loss: 0.006855396984522636  params: [0.47554932 0.05921141 0.99887466]  gradients: [-5.1358914973367795e-05, -2.794957065166995e-05, -3.6135657083619725e-05]\n",
      "epoch: 456  loss: 0.006855582909055875  params: [0.47560068 0.05923936 0.99891079]  gradients: [-5.135999981494128e-05, -2.795016102349186e-05, -3.613642036810714e-05]\n",
      "epoch: 457  loss: 0.0068557688431390715  params: [0.47565204 0.05926731 0.99894693]  gradients: [-5.1361084625458115e-05, -2.795075137841271e-05, -3.613718363074338e-05]\n",
      "epoch: 458  loss: 0.006855954786772097  params: [0.4757034  0.05929526 0.99898307]  gradients: [-5.136216940491363e-05, -2.7951341716429957e-05, -3.613794687152517e-05]\n",
      "epoch: 459  loss: 0.006856140739954848  params: [0.47575476 0.05932321 0.99901921]  gradients: [-5.13632541533032e-05, -2.7951932037541086e-05, -3.613871009044924e-05]\n",
      "epoch: 460  loss: 0.006856326702687218  params: [0.47580613 0.05935116 0.99905535]  gradients: [-5.136433887062219e-05, -2.795252234174358e-05, -3.613947328751233e-05]\n",
      "epoch: 461  loss: 0.0068565126749690905  params: [0.47585749 0.05937912 0.99909149]  gradients: [-5.136542355686593e-05, -2.795311262903489e-05, -3.614023646271118e-05]\n",
      "epoch: 462  loss: 0.006856698656800334  params: [0.47590886 0.05940707 0.99912763]  gradients: [-5.136650821202984e-05, -2.795370289941253e-05, -3.614099961604254e-05]\n",
      "epoch: 463  loss: 0.0068568846481808655  params: [0.47596023 0.05943503 0.99916377]  gradients: [-5.1367592836109235e-05, -2.7954293152873945e-05, -3.614176274750312e-05]\n",
      "epoch: 464  loss: 0.006857070649110543  params: [0.47601159 0.05946298 0.99919991]  gradients: [-5.1368677429099504e-05, -2.7954883389416627e-05, -3.614252585708969e-05]\n",
      "epoch: 465  loss: 0.006857256659589269  params: [0.47606296 0.05949094 0.99923606]  gradients: [-5.1369761990996e-05, -2.7955473609038048e-05, -3.614328894479896e-05]\n",
      "epoch: 466  loss: 0.0068574426796169236  params: [0.47611433 0.05951889 0.9992722 ]  gradients: [-5.1370846521794093e-05, -2.795606381173569e-05, -3.614405201062768e-05]\n",
      "epoch: 467  loss: 0.006857628709193396  params: [0.47616571 0.05954685 0.99930834]  gradients: [-5.1371931021489145e-05, -2.7956653997507027e-05, -3.61448150545726e-05]\n",
      "epoch: 468  loss: 0.006857814748318574  params: [0.47621708 0.05957481 0.99934449]  gradients: [-5.137301549007653e-05, -2.7957244166349543e-05, -3.6145578076630435e-05]\n",
      "epoch: 469  loss: 0.006858000796992339  params: [0.47626845 0.05960276 0.99938064]  gradients: [-5.13740999275516e-05, -2.7957834318260705e-05, -3.614634107679795e-05]\n",
      "epoch: 470  loss: 0.006858186855214581  params: [0.47631983 0.05963072 0.99941678]  gradients: [-5.1375184333909724e-05, -2.7958424453237998e-05, -3.614710405507186e-05]\n",
      "epoch: 471  loss: 0.006858372922985181  params: [0.47637121 0.05965868 0.99945293]  gradients: [-5.137626870914629e-05, -2.79590145712789e-05, -3.614786701144893e-05]\n",
      "epoch: 472  loss: 0.006858559000304026  params: [0.47642258 0.05968664 0.99948908]  gradients: [-5.137735305325664e-05, -2.7959604672380896e-05, -3.6148629945925895e-05]\n",
      "epoch: 473  loss: 0.0068587450871710086  params: [0.47647396 0.0597146  0.99952523]  gradients: [-5.1378437366236145e-05, -2.7960194756541454e-05, -3.614939285849948e-05]\n",
      "epoch: 474  loss: 0.006858931183586004  params: [0.47652534 0.05974256 0.99956138]  gradients: [-5.137952164808018e-05, -2.796078482375806e-05, -3.6150155749166424e-05]\n",
      "epoch: 475  loss: 0.006859117289548906  params: [0.47657672 0.05977052 0.99959753]  gradients: [-5.1380605898784113e-05, -2.7961374874028194e-05, -3.6150918617923494e-05]\n",
      "epoch: 476  loss: 0.006859303405059599  params: [0.4766281  0.05979848 0.99963368]  gradients: [-5.1381690118343305e-05, -2.7961964907349326e-05, -3.615168146476741e-05]\n",
      "epoch: 477  loss: 0.006859489530117961  params: [0.47667949 0.05982645 0.99966983]  gradients: [-5.138277430675314e-05, -2.796255492371895e-05, -3.615244428969492e-05]\n",
      "epoch: 478  loss: 0.006859675664723899  params: [0.47673087 0.05985441 0.99970599]  gradients: [-5.138385846400897e-05, -2.7963144923134535e-05, -3.615320709270277e-05]\n",
      "epoch: 479  loss: 0.0068598618088772665  params: [0.47678225 0.05988237 0.99974214]  gradients: [-5.138494259010617e-05, -2.796373490559356e-05, -3.61539698737877e-05]\n",
      "epoch: 480  loss: 0.006860047962577977  params: [0.47683364 0.05991034 0.9997783 ]  gradients: [-5.138602668504012e-05, -2.796432487109352e-05, -3.615473263294644e-05]\n",
      "epoch: 481  loss: 0.006860234125825901  params: [0.47688503 0.0599383  0.99981445]  gradients: [-5.1387110748806184e-05, -2.7964914819631884e-05, -3.615549537017575e-05]\n",
      "epoch: 482  loss: 0.006860420298620928  params: [0.47693642 0.05996627 0.99985061]  gradients: [-5.138819478139973e-05, -2.7965504751206137e-05, -3.615625808547238e-05]\n",
      "epoch: 483  loss: 0.006860606480962945  params: [0.47698781 0.05999423 0.99988676]  gradients: [-5.138927878281614e-05, -2.796609466581376e-05, -3.615702077883304e-05]\n",
      "epoch: 484  loss: 0.006860792672851837  params: [0.4770392  0.0600222  0.99992292]  gradients: [-5.1390362753050764e-05, -2.7966684563452223e-05, -3.615778345025451e-05]\n",
      "epoch: 485  loss: 0.006860978874287485  params: [0.47709059 0.06005017 0.99995908]  gradients: [-5.1391446692099e-05, -2.7967274444119027e-05, -3.61585460997335e-05]\n",
      "epoch: 486  loss: 0.0068611650852697815  params: [0.47714198 0.06007814 0.99999524]  gradients: [-5.139253059995621e-05, -2.7967864307811646e-05, -3.615930872726679e-05]\n",
      "epoch: 487  loss: 0.006861351305798604  params: [0.47719337 0.06010611 1.0000314 ]  gradients: [-5.139361447661776e-05, -2.7968454154527554e-05, -3.6160071332851104e-05]\n",
      "epoch: 488  loss: 0.006861537535873846  params: [0.47724477 0.06013407 1.00006756]  gradients: [-5.139469832207904e-05, -2.7969043984264242e-05, -3.616083391648318e-05]\n",
      "epoch: 489  loss: 0.006861723775495379  params: [0.47729616 0.06016204 1.00010372]  gradients: [-5.1395782136335405e-05, -2.796963379701919e-05, -3.616159647815978e-05]\n",
      "epoch: 490  loss: 0.006861910024663099  params: [0.47734756 0.06019001 1.00013989]  gradients: [-5.139686591938224e-05, -2.7970223592789884e-05, -3.6162359017877646e-05]\n",
      "epoch: 491  loss: 0.006862096283376891  params: [0.47739896 0.06021798 1.00017605]  gradients: [-5.139794967121492e-05, -2.79708133715738e-05, -3.616312153563351e-05]\n",
      "epoch: 492  loss: 0.006862282551636632  params: [0.47745036 0.06024596 1.00021221]  gradients: [-5.139903339182882e-05, -2.7971403133368427e-05, -3.616388403142414e-05]\n",
      "epoch: 493  loss: 0.006862468829442217  params: [0.47750176 0.06027393 1.00024838]  gradients: [-5.140011708121931e-05, -2.7971992878171248e-05, -3.6164646505246264e-05]\n",
      "epoch: 494  loss: 0.006862655116793525  params: [0.47755316 0.0603019  1.00028454]  gradients: [-5.140120073938177e-05, -2.797258260597975e-05, -3.616540895709664e-05]\n",
      "epoch: 495  loss: 0.0068628414136904445  params: [0.47760456 0.06032987 1.00032071]  gradients: [-5.140228436631157e-05, -2.7973172316791398e-05, -3.6166171386972e-05]\n",
      "epoch: 496  loss: 0.006863027720132854  params: [0.47765596 0.06035785 1.00035688]  gradients: [-5.140336796200411e-05, -2.7973762010603703e-05, -3.616693379486911e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 497  loss: 0.006863214036120643  params: [0.47770737 0.06038582 1.00039304]  gradients: [-5.140445152645474e-05, -2.797435168741413e-05, -3.616769618078471e-05]\n",
      "epoch: 498  loss: 0.006863400361653694  params: [0.47775877 0.0604138  1.00042921]  gradients: [-5.140553505965886e-05, -2.797494134722018e-05, -3.616845854471555e-05]\n",
      "epoch: 499  loss: 0.0068635866967318914  params: [0.47781018 0.06044177 1.00046538]  gradients: [-5.140661856161181e-05, -2.7975530990019304e-05, -3.6169220886658355e-05]\n",
      "epoch: 500  loss: 0.006863773041355121  params: [0.47786159 0.06046975 1.00050155]  gradients: [-5.140770203230902e-05, -2.7976120615809032e-05, -3.616998320660992e-05]\n",
      "epoch: 501  loss: 0.006863959395523265  params: [0.477913   0.06049773 1.00053772]  gradients: [-5.1408785471745825e-05, -2.797671022458682e-05, -3.617074550456694e-05]\n",
      "epoch: 502  loss: 0.006864145759236209  params: [0.47796441 0.0605257  1.00057389]  gradients: [-5.1409868879917636e-05, -2.797729981635016e-05, -3.61715077805262e-05]\n",
      "epoch: 503  loss: 0.006864332132493837  params: [0.47801582 0.06055368 1.00061007]  gradients: [-5.141095225681981e-05, -2.7977889391096537e-05, -3.6172270034484436e-05]\n",
      "epoch: 504  loss: 0.006864518515296037  params: [0.47806723 0.06058166 1.00064624]  gradients: [-5.1412035602447736e-05, -2.7978478948823447e-05, -3.6173032266438417e-05]\n",
      "epoch: 505  loss: 0.0068647049076426825  params: [0.47811864 0.06060964 1.00068241]  gradients: [-5.1413118916796803e-05, -2.797906848952836e-05, -3.617379447638486e-05]\n",
      "epoch: 506  loss: 0.0068648913095336744  params: [0.47817006 0.06063762 1.00071859]  gradients: [-5.141420219986237e-05, -2.7979658013208768e-05, -3.617455666432053e-05]\n",
      "epoch: 507  loss: 0.00686507772096888  params: [0.47822147 0.0606656  1.00075476]  gradients: [-5.141528545163984e-05, -2.7980247519862164e-05, -3.617531883024219e-05]\n",
      "epoch: 508  loss: 0.006865264141948198  params: [0.47827289 0.06069358 1.00079094]  gradients: [-5.141636867212459e-05, -2.798083700948603e-05, -3.617608097414658e-05]\n",
      "epoch: 509  loss: 0.006865450572471502  params: [0.47832431 0.06072156 1.00082711]  gradients: [-5.141745186131199e-05, -2.7981426482077852e-05, -3.6176843096030436e-05]\n",
      "epoch: 510  loss: 0.006865637012538676  params: [0.47837573 0.06074954 1.00086329]  gradients: [-5.141853501919744e-05, -2.7982015937635117e-05, -3.617760519589053e-05]\n",
      "epoch: 511  loss: 0.006865823462149612  params: [0.47842715 0.06077752 1.00089947]  gradients: [-5.14196181457763e-05, -2.798260537615531e-05, -3.6178367273723605e-05]\n",
      "epoch: 512  loss: 0.006866009921304188  params: [0.47847857 0.06080551 1.00093565]  gradients: [-5.142070124104398e-05, -2.798319479763592e-05, -3.617912932952641e-05]\n",
      "epoch: 513  loss: 0.006866196390002287  params: [0.47852999 0.06083349 1.00097183]  gradients: [-5.142178430499584e-05, -2.7983784202074445e-05, -3.6179891363295715e-05]\n",
      "epoch: 514  loss: 0.006866382868243791  params: [0.47858141 0.06086148 1.00100801]  gradients: [-5.1422867337627273e-05, -2.7984373589468357e-05, -3.618065337502825e-05]\n",
      "epoch: 515  loss: 0.006866569356028595  params: [0.47863283 0.06088946 1.00104419]  gradients: [-5.142395033893368e-05, -2.798496295981516e-05, -3.618141536472078e-05]\n",
      "epoch: 516  loss: 0.006866755853356572  params: [0.47868426 0.06091745 1.00108037]  gradients: [-5.142503330891042e-05, -2.798555231311233e-05, -3.6182177332370054e-05]\n",
      "epoch: 517  loss: 0.006866942360227601  params: [0.47873569 0.06094543 1.00111656]  gradients: [-5.1426116247552896e-05, -2.798614164935736e-05, -3.6182939277972824e-05]\n",
      "epoch: 518  loss: 0.006867128876641579  params: [0.47878711 0.06097342 1.00115274]  gradients: [-5.1427199154856486e-05, -2.7986730968547738e-05, -3.618370120152585e-05]\n",
      "epoch: 519  loss: 0.006867315402598392  params: [0.47883854 0.06100141 1.00118893]  gradients: [-5.142828203081657e-05, -2.7987320270680952e-05, -3.618446310302587e-05]\n",
      "epoch: 520  loss: 0.006867501938097901  params: [0.47888997 0.06102939 1.00122511]  gradients: [-5.142936487542854e-05, -2.7987909555754493e-05, -3.618522498246965e-05]\n",
      "epoch: 521  loss: 0.006867688483140008  params: [0.4789414  0.06105738 1.0012613 ]  gradients: [-5.143044768868779e-05, -2.7988498823765853e-05, -3.618598683985394e-05]\n",
      "epoch: 522  loss: 0.006867875037724594  params: [0.47899283 0.06108537 1.00129748]  gradients: [-5.1431530470589705e-05, -2.798908807471252e-05, -3.618674867517551e-05]\n",
      "epoch: 523  loss: 0.006868061601851538  params: [0.47904426 0.06111336 1.00133367]  gradients: [-5.143261322112967e-05, -2.7989677308591983e-05, -3.618751048843109e-05]\n",
      "epoch: 524  loss: 0.006868248175520723  params: [0.4790957  0.06114135 1.00136986]  gradients: [-5.1433695940303074e-05, -2.799026652540174e-05, -3.618827227961746e-05]\n",
      "epoch: 525  loss: 0.006868434758732042  params: [0.47914713 0.06116934 1.00140605]  gradients: [-5.14347786281053e-05, -2.799085572513927e-05, -3.618903404873135e-05]\n",
      "epoch: 526  loss: 0.006868621351485358  params: [0.47919857 0.06119733 1.00144224]  gradients: [-5.143586128453174e-05, -2.7991444907802064e-05, -3.618979579576954e-05]\n",
      "epoch: 527  loss: 0.006868807953780571  params: [0.47925001 0.06122533 1.00147843]  gradients: [-5.143694390957779e-05, -2.799203407338762e-05, -3.6190557520728766e-05]\n",
      "epoch: 528  loss: 0.006868994565617565  params: [0.47930144 0.06125332 1.00151462]  gradients: [-5.1438026503238844e-05, -2.799262322189343e-05, -3.619131922360581e-05]\n",
      "epoch: 529  loss: 0.006869181186996211  params: [0.47935288 0.06128131 1.00155081]  gradients: [-5.143910906551026e-05, -2.7993212353316973e-05, -3.619208090439739e-05]\n",
      "epoch: 530  loss: 0.006869367817916401  params: [0.47940432 0.06130931 1.001587  ]  gradients: [-5.1440191596387466e-05, -2.7993801467655756e-05, -3.6192842563100285e-05]\n",
      "epoch: 531  loss: 0.006869554458378011  params: [0.47945576 0.0613373  1.0016232 ]  gradients: [-5.144127409586584e-05, -2.7994390564907262e-05, -3.619360419971126e-05]\n",
      "epoch: 532  loss: 0.006869741108380934  params: [0.47950721 0.0613653  1.00165939]  gradients: [-5.144235656394076e-05, -2.799497964506898e-05, -3.619436581422706e-05]\n",
      "epoch: 533  loss: 0.006869927767925043  params: [0.47955865 0.06139329 1.00169559]  gradients: [-5.144343900060765e-05, -2.7995568708138415e-05, -3.619512740664446e-05]\n",
      "epoch: 534  loss: 0.0068701144370102276  params: [0.4796101  0.06142129 1.00173178]  gradients: [-5.144452140586187e-05, -2.7996157754113047e-05, -3.619588897696019e-05]\n",
      "epoch: 535  loss: 0.006870301115636363  params: [0.47966154 0.06144928 1.00176798]  gradients: [-5.144560377969883e-05, -2.799674678299038e-05, -3.6196650525171034e-05]\n",
      "epoch: 536  loss: 0.006870487803803335  params: [0.47971299 0.06147728 1.00180418]  gradients: [-5.144668612211391e-05, -2.799733579476789e-05, -3.619741205127373e-05]\n",
      "epoch: 537  loss: 0.0068706745015110265  params: [0.47976444 0.06150528 1.00184038]  gradients: [-5.1447768433102527e-05, -2.7997924789443093e-05, -3.619817355526506e-05]\n",
      "epoch: 538  loss: 0.0068708612087593265  params: [0.47981588 0.06153328 1.00187657]  gradients: [-5.1448850712660045e-05, -2.7998513767013462e-05, -3.619893503714175e-05]\n",
      "epoch: 539  loss: 0.006871047925548111  params: [0.47986733 0.06156128 1.00191277]  gradients: [-5.1449932960781874e-05, -2.7999102727476493e-05, -3.619969649690059e-05]\n",
      "epoch: 540  loss: 0.006871234651877256  params: [0.47991878 0.06158928 1.00194897]  gradients: [-5.145101517746342e-05, -2.79996916708297e-05, -3.620045793453834e-05]\n",
      "epoch: 541  loss: 0.0068714213877466585  params: [0.47997024 0.06161728 1.00198518]  gradients: [-5.145209736270005e-05, -2.8000280597070545e-05, -3.620121935005173e-05]\n",
      "epoch: 542  loss: 0.006871608133156187  params: [0.48002169 0.06164528 1.00202138]  gradients: [-5.145317951648718e-05, -2.8000869506196552e-05, -3.620198074343755e-05]\n",
      "epoch: 543  loss: 0.006871794888105732  params: [0.48007314 0.06167328 1.00205758]  gradients: [-5.145426163882021e-05, -2.8001458398205196e-05, -3.620274211469254e-05]\n",
      "epoch: 544  loss: 0.006871981652595176  params: [0.4801246  0.06170128 1.00209378]  gradients: [-5.1455343729694516e-05, -2.8002047273093982e-05, -3.6203503463813485e-05]\n",
      "epoch: 545  loss: 0.006872168426624395  params: [0.48017606 0.06172928 1.00212999]  gradients: [-5.1456425789105515e-05, -2.8002636130860404e-05, -3.620426479079713e-05]\n",
      "epoch: 546  loss: 0.006872355210193275  params: [0.48022751 0.06175729 1.00216619]  gradients: [-5.1457507817048597e-05, -2.8003224971501954e-05, -3.620502609564024e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 547  loss: 0.0068725420033017015  params: [0.48027897 0.06178529 1.0022024 ]  gradients: [-5.145858981351915e-05, -2.8003813795016125e-05, -3.6205787378339574e-05]\n",
      "epoch: 548  loss: 0.006872728805949546  params: [0.48033043 0.0618133  1.00223861]  gradients: [-5.145967177851259e-05, -2.8004402601400417e-05, -3.620654863889189e-05]\n",
      "epoch: 549  loss: 0.006872915618136705  params: [0.48038189 0.0618413  1.00227481]  gradients: [-5.1460753712024296e-05, -2.8004991390652323e-05, -3.6207309877293964e-05]\n",
      "epoch: 550  loss: 0.0068731024398630425  params: [0.48043335 0.06186931 1.00231102]  gradients: [-5.1461835614049676e-05, -2.800558016276934e-05, -3.620807109354255e-05]\n",
      "epoch: 551  loss: 0.00687328927112846  params: [0.48048482 0.06189731 1.00234723]  gradients: [-5.146291748458414e-05, -2.800616891774897e-05, -3.620883228763441e-05]\n",
      "epoch: 552  loss: 0.006873476111932823  params: [0.48053628 0.06192532 1.00238344]  gradients: [-5.146399932362307e-05, -2.8006757655588702e-05, -3.6209593459566305e-05]\n",
      "epoch: 553  loss: 0.006873662962276021  params: [0.48058775 0.06195333 1.00241965]  gradients: [-5.146508113116188e-05, -2.8007346376286036e-05, -3.621035460933501e-05]\n",
      "epoch: 554  loss: 0.006873849822157939  params: [0.48063921 0.06198133 1.00245586]  gradients: [-5.146616290719596e-05, -2.8007935079838465e-05, -3.621111573693728e-05]\n",
      "epoch: 555  loss: 0.006874036691578449  params: [0.48069068 0.06200934 1.00249207]  gradients: [-5.146724465172071e-05, -2.8008523766243488e-05, -3.621187684236988e-05]\n",
      "epoch: 556  loss: 0.00687422357053744  params: [0.48074215 0.06203735 1.00252829]  gradients: [-5.146832636473155e-05, -2.800911243549861e-05, -3.621263792562958e-05]\n",
      "epoch: 557  loss: 0.0068744104590347885  params: [0.48079362 0.06206536 1.0025645 ]  gradients: [-5.1469408046223856e-05, -2.8009701087601323e-05, -3.6213398986713135e-05]\n",
      "epoch: 558  loss: 0.00687459735707038  params: [0.48084509 0.06209337 1.00260071]  gradients: [-5.147048969619303e-05, -2.801028972254911e-05, -3.6214160025617306e-05]\n",
      "epoch: 559  loss: 0.0068747842646440915  params: [0.48089656 0.06212138 1.00263693]  gradients: [-5.147157131463451e-05, -2.8010878340339498e-05, -3.6214921042338876e-05]\n",
      "epoch: 560  loss: 0.006874971181755813  params: [0.48094803 0.06214939 1.00267314]  gradients: [-5.1472652901543666e-05, -2.8011466940969963e-05, -3.62156820368746e-05]\n",
      "epoch: 561  loss: 0.00687515810840542  params: [0.48099951 0.06217741 1.00270936]  gradients: [-5.1473734456915926e-05, -2.8012055524438016e-05, -3.6216443009221247e-05]\n",
      "epoch: 562  loss: 0.006875345044592787  params: [0.48105098 0.06220542 1.00274558]  gradients: [-5.147481598074666e-05, -2.8012644090741146e-05, -3.6217203959375575e-05]\n",
      "epoch: 563  loss: 0.006875531990317804  params: [0.48110246 0.06223343 1.0027818 ]  gradients: [-5.1475897473031294e-05, -2.8013232639876856e-05, -3.621796488733436e-05]\n",
      "epoch: 564  loss: 0.006875718945580354  params: [0.48115393 0.06226145 1.00281801]  gradients: [-5.147697893376523e-05, -2.801382117184265e-05, -3.621872579309438e-05]\n",
      "epoch: 565  loss: 0.006875905910380309  params: [0.48120541 0.06228946 1.00285423]  gradients: [-5.147806036294388e-05, -2.8014409686636022e-05, -3.6219486676652374e-05]\n",
      "epoch: 566  loss: 0.006876092884717564  params: [0.48125689 0.06231748 1.00289045]  gradients: [-5.147914176056264e-05, -2.8014998184254474e-05, -3.6220247538005124e-05]\n",
      "epoch: 567  loss: 0.006876279868591982  params: [0.48130837 0.06234549 1.00292668]  gradients: [-5.1480223126616916e-05, -2.8015586664695504e-05, -3.6221008377149395e-05]\n",
      "epoch: 568  loss: 0.006876466862003456  params: [0.48135985 0.06237351 1.0029629 ]  gradients: [-5.148130446110212e-05, -2.8016175127956608e-05, -3.622176919408196e-05]\n",
      "epoch: 569  loss: 0.006876653864951866  params: [0.48141134 0.06240152 1.00299912]  gradients: [-5.1482385764013655e-05, -2.8016763574035297e-05, -3.6222529988799585e-05]\n",
      "epoch: 570  loss: 0.006876840877437087  params: [0.48146282 0.06242954 1.00303534]  gradients: [-5.1483467035346914e-05, -2.8017352002929063e-05, -3.622329076129904e-05]\n",
      "epoch: 571  loss: 0.006877027899459005  params: [0.4815143  0.06245756 1.00307157]  gradients: [-5.148454827509734e-05, -2.8017940414635412e-05, -3.6224051511577085e-05]\n",
      "epoch: 572  loss: 0.0068772149310175066  params: [0.48156579 0.06248558 1.00310779]  gradients: [-5.1485629483260296e-05, -2.8018528809151834e-05, -3.622481223963049e-05]\n",
      "epoch: 573  loss: 0.0068774019721124565  params: [0.48161728 0.0625136  1.00314402]  gradients: [-5.148671065983122e-05, -2.801911718647585e-05, -3.622557294545604e-05]\n",
      "epoch: 574  loss: 0.006877589022743746  params: [0.48166876 0.06254162 1.00318024]  gradients: [-5.148779180480552e-05, -2.8019705546604942e-05, -3.622633362905049e-05]\n",
      "epoch: 575  loss: 0.006877776082911253  params: [0.48172025 0.06256964 1.00321647]  gradients: [-5.14888729181786e-05, -2.8020293889536624e-05, -3.622709429041061e-05]\n",
      "epoch: 576  loss: 0.00687796315261486  params: [0.48177174 0.06259766 1.0032527 ]  gradients: [-5.1489953999945875e-05, -2.8020882215268388e-05, -3.622785492953318e-05]\n",
      "epoch: 577  loss: 0.006878150231854444  params: [0.48182323 0.06262568 1.00328893]  gradients: [-5.149103505010274e-05, -2.802147052379775e-05, -3.622861554641496e-05]\n",
      "epoch: 578  loss: 0.00687833732062989  params: [0.48187473 0.0626537  1.00332516]  gradients: [-5.149211606864462e-05, -2.80220588151222e-05, -3.622937614105273e-05]\n",
      "epoch: 579  loss: 0.0068785244189410785  params: [0.48192622 0.06268172 1.00336139]  gradients: [-5.149319705556692e-05, -2.8022647089239244e-05, -3.623013671344326e-05]\n",
      "epoch: 580  loss: 0.006878711526787883  params: [0.48197771 0.06270975 1.00339762]  gradients: [-5.1494278010865047e-05, -2.8023235346146382e-05, -3.623089726358331e-05]\n",
      "epoch: 581  loss: 0.006878898644170192  params: [0.48202921 0.06273777 1.00343385]  gradients: [-5.1495358934534426e-05, -2.802382358584113e-05, -3.623165779146966e-05]\n",
      "epoch: 582  loss: 0.006879085771087882  params: [0.4820807  0.0627658  1.00347008]  gradients: [-5.149643982657045e-05, -2.8024411808320972e-05, -3.623241829709908e-05]\n",
      "epoch: 583  loss: 0.006879272907540828  params: [0.4821322  0.06279382 1.00350631]  gradients: [-5.149752068696855e-05, -2.8025000013583427e-05, -3.623317878046835e-05]\n",
      "epoch: 584  loss: 0.006879460053528918  params: [0.4821837  0.06282185 1.00354255]  gradients: [-5.1498601515724126e-05, -2.802558820162599e-05, -3.623393924157424e-05]\n",
      "epoch: 585  loss: 0.006879647209052027  params: [0.4822352  0.06284987 1.00357878]  gradients: [-5.14996823128326e-05, -2.8026176372446168e-05, -3.623469968041351e-05]\n",
      "epoch: 586  loss: 0.00687983437411004  params: [0.4822867  0.0628779  1.00361502]  gradients: [-5.1500763078289384e-05, -2.802676452604147e-05, -3.6235460096982954e-05]\n",
      "epoch: 587  loss: 0.006880021548702836  params: [0.4823382  0.06290593 1.00365126]  gradients: [-5.1501843812089883e-05, -2.8027352662409385e-05, -3.623622049127932e-05]\n",
      "epoch: 588  loss: 0.006880208732830292  params: [0.48238971 0.06293395 1.00368749]  gradients: [-5.150292451422952e-05, -2.8027940781547433e-05, -3.62369808632994e-05]\n",
      "epoch: 589  loss: 0.006880395926492283  params: [0.48244121 0.06296198 1.00372373]  gradients: [-5.150400518470371e-05, -2.8028528883453112e-05, -3.623774121303996e-05]\n",
      "epoch: 590  loss: 0.006880583129688701  params: [0.48249271 0.06299001 1.00375997]  gradients: [-5.150508582350787e-05, -2.8029116968123933e-05, -3.623850154049779e-05]\n",
      "epoch: 591  loss: 0.0068807703424194145  params: [0.48254422 0.06301804 1.00379621]  gradients: [-5.15061664306374e-05, -2.8029705035557393e-05, -3.623926184566964e-05]\n",
      "epoch: 592  loss: 0.006880957564684312  params: [0.48259573 0.06304607 1.00383245]  gradients: [-5.150724700608774e-05, -2.8030293085751002e-05, -3.624002212855231e-05]\n",
      "epoch: 593  loss: 0.00688114479648327  params: [0.48264724 0.0630741  1.00386869]  gradients: [-5.1508327549854294e-05, -2.8030881118702266e-05, -3.624078238914255e-05]\n",
      "epoch: 594  loss: 0.006881332037816163  params: [0.48269875 0.06310213 1.00390493]  gradients: [-5.1509408061932475e-05, -2.803146913440869e-05, -3.624154262743715e-05]\n",
      "epoch: 595  loss: 0.006881519288682877  params: [0.48275026 0.06313017 1.00394117]  gradients: [-5.151048854231772e-05, -2.8032057132867785e-05, -3.624230284343289e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 596  loss: 0.006881706549083285  params: [0.48280177 0.0631582  1.00397742]  gradients: [-5.1511568991005417e-05, -2.8032645114077042e-05, -3.6243063037126536e-05]\n",
      "epoch: 597  loss: 0.006881893819017275  params: [0.48285328 0.06318623 1.00401366]  gradients: [-5.1512649407991e-05, -2.8033233078033982e-05, -3.624382320851487e-05]\n",
      "epoch: 598  loss: 0.006882081098484727  params: [0.48290479 0.06321427 1.0040499 ]  gradients: [-5.15137297932699e-05, -2.8033821024736113e-05, -3.624458335759467e-05]\n",
      "epoch: 599  loss: 0.00688226838748551  params: [0.48295631 0.0632423  1.00408615]  gradients: [-5.151481014683752e-05, -2.803440895418094e-05, -3.624534348436271e-05]\n",
      "epoch: 600  loss: 0.006882455686019505  params: [0.48300783 0.06327034 1.0041224 ]  gradients: [-5.151589046868928e-05, -2.803499686636596e-05, -3.624610358881576e-05]\n",
      "epoch: 601  loss: 0.0068826429940866  params: [0.48305934 0.06329837 1.00415864]  gradients: [-5.1516970758820605e-05, -2.803558476128869e-05, -3.6246863670950606e-05]\n",
      "epoch: 602  loss: 0.00688283031168667  params: [0.48311086 0.06332641 1.00419489]  gradients: [-5.151805101722691e-05, -2.803617263894663e-05, -3.624762373076402e-05]\n",
      "epoch: 603  loss: 0.006883017638819597  params: [0.48316238 0.06335444 1.00423114]  gradients: [-5.151913124390362e-05, -2.80367604993373e-05, -3.6248383768252795e-05]\n",
      "epoch: 604  loss: 0.006883204975485247  params: [0.4832139  0.06338248 1.00426739]  gradients: [-5.152021143884616e-05, -2.8037348342458204e-05, -3.624914378341369e-05]\n",
      "epoch: 605  loss: 0.006883392321683511  params: [0.48326542 0.06341052 1.00430364]  gradients: [-5.152129160204996e-05, -2.803793616830685e-05, -3.62499037762435e-05]\n",
      "epoch: 606  loss: 0.006883579677414264  params: [0.48331694 0.06343856 1.00433989]  gradients: [-5.152237173351041e-05, -2.803852397688074e-05, -3.625066374673898e-05]\n",
      "epoch: 607  loss: 0.006883767042677391  params: [0.48336847 0.0634666  1.00437614]  gradients: [-5.152345183322296e-05, -2.803911176817739e-05, -3.6251423694896946e-05]\n",
      "epoch: 608  loss: 0.006883954417472766  params: [0.48341999 0.06349464 1.00441239]  gradients: [-5.152453190118302e-05, -2.8039699542194306e-05, -3.625218362071415e-05]\n",
      "epoch: 609  loss: 0.0068841418018002615  params: [0.48347152 0.06352268 1.00444864]  gradients: [-5.152561193738602e-05, -2.8040287298928997e-05, -3.625294352418739e-05]\n",
      "epoch: 610  loss: 0.006884329195659773  params: [0.48352304 0.06355072 1.0044849 ]  gradients: [-5.152669194182739e-05, -2.804087503837898e-05, -3.625370340531342e-05]\n",
      "epoch: 611  loss: 0.006884516599051163  params: [0.48357457 0.06357876 1.00452115]  gradients: [-5.152777191450253e-05, -2.804146276054175e-05, -3.625446326408903e-05]\n",
      "epoch: 612  loss: 0.0068847040119743115  params: [0.4836261  0.0636068  1.00455741]  gradients: [-5.152885185540689e-05, -2.8042050465414833e-05, -3.6255223100511024e-05]\n",
      "epoch: 613  loss: 0.006884891434429107  params: [0.48367763 0.06363484 1.00459366]  gradients: [-5.1529931764535896e-05, -2.8042638152995738e-05, -3.6255982914576165e-05]\n",
      "epoch: 614  loss: 0.006885078866415422  params: [0.48372916 0.06366289 1.00462992]  gradients: [-5.153101164188495e-05, -2.804322582328196e-05, -3.625674270628123e-05]\n",
      "epoch: 615  loss: 0.0068852663079331375  params: [0.48378069 0.06369093 1.00466618]  gradients: [-5.1532091487449493e-05, -2.8043813476271024e-05, -3.6257502475623e-05]\n",
      "epoch: 616  loss: 0.006885453758982125  params: [0.48383223 0.06371898 1.00470244]  gradients: [-5.153317130122496e-05, -2.8044401111960434e-05, -3.6258262222598274e-05]\n",
      "epoch: 617  loss: 0.006885641219562278  params: [0.48388376 0.06374702 1.0047387 ]  gradients: [-5.153425108320675e-05, -2.80449887303477e-05, -3.6259021947203806e-05]\n",
      "epoch: 618  loss: 0.006885828689673453  params: [0.4839353  0.06377507 1.00477495]  gradients: [-5.153533083339031e-05, -2.804557633143035e-05, -3.625978164943641e-05]\n",
      "epoch: 619  loss: 0.006886016169315545  params: [0.48398683 0.06380311 1.00481122]  gradients: [-5.153641055177106e-05, -2.8046163915205867e-05, -3.626054132929285e-05]\n",
      "epoch: 620  loss: 0.006886203658488428  params: [0.48403837 0.06383116 1.00484748]  gradients: [-5.1537490238344445e-05, -2.8046751481671787e-05, -3.6261300986769915e-05]\n",
      "epoch: 621  loss: 0.00688639115719198  params: [0.48408991 0.06385921 1.00488374]  gradients: [-5.153856989310587e-05, -2.8047339030825614e-05, -3.626206062186439e-05]\n",
      "epoch: 622  loss: 0.00688657866542608  params: [0.48414145 0.06388725 1.00492   ]  gradients: [-5.1539649516050775e-05, -2.8047926562664855e-05, -3.6262820234573045e-05]\n",
      "epoch: 623  loss: 0.006886766183190601  params: [0.48419299 0.0639153  1.00495627]  gradients: [-5.15407291071746e-05, -2.8048514077187035e-05, -3.6263579824892684e-05]\n",
      "epoch: 624  loss: 0.0068869537104854295  params: [0.48424453 0.06394335 1.00499253]  gradients: [-5.154180866647275e-05, -2.8049101574389652e-05, -3.626433939282007e-05]\n",
      "epoch: 625  loss: 0.006887141247310432  params: [0.48429607 0.0639714  1.00502879]  gradients: [-5.1542888193940666e-05, -2.8049689054270224e-05, -3.626509893835201e-05]\n",
      "epoch: 626  loss: 0.006887328793665501  params: [0.48434762 0.06399945 1.00506506]  gradients: [-5.154396768957378e-05, -2.805027651682627e-05, -3.626585846148527e-05]\n",
      "epoch: 627  loss: 0.006887516349550505  params: [0.48439916 0.0640275  1.00510133]  gradients: [-5.154504715336754e-05, -2.8050863962055303e-05, -3.626661796221665e-05]\n",
      "epoch: 628  loss: 0.006887703914965316  params: [0.48445071 0.06405555 1.00513759]  gradients: [-5.1546126585317346e-05, -2.805145138995483e-05, -3.626737744054293e-05]\n",
      "epoch: 629  loss: 0.006887891489909828  params: [0.48450226 0.06408361 1.00517386]  gradients: [-5.154720598541864e-05, -2.805203880052237e-05, -3.626813689646089e-05]\n",
      "epoch: 630  loss: 0.006888079074383908  params: [0.4845538  0.06411166 1.00521013]  gradients: [-5.1548285353666865e-05, -2.805262619375543e-05, -3.626889632996732e-05]\n",
      "epoch: 631  loss: 0.006888266668387436  params: [0.48460535 0.06413971 1.0052464 ]  gradients: [-5.154936469005745e-05, -2.8053213569651537e-05, -3.6269655741059015e-05]\n",
      "epoch: 632  loss: 0.006888454271920285  params: [0.4846569  0.06416777 1.00528267]  gradients: [-5.155044399458582e-05, -2.8053800928208196e-05, -3.6270415129732744e-05]\n",
      "epoch: 633  loss: 0.006888641884982341  params: [0.48470846 0.06419582 1.00531894]  gradients: [-5.1551523267247405e-05, -2.8054388269422926e-05, -3.627117449598531e-05]\n",
      "epoch: 634  loss: 0.006888829507573476  params: [0.48476001 0.06422388 1.00535521]  gradients: [-5.155260250803766e-05, -2.8054975593293243e-05, -3.627193383981349e-05]\n",
      "epoch: 635  loss: 0.006889017139693574  params: [0.48481156 0.06425193 1.00539149]  gradients: [-5.1553681716952e-05, -2.805556289981665e-05, -3.627269316121408e-05]\n",
      "epoch: 636  loss: 0.0068892047813425  params: [0.48486312 0.06427999 1.00542776]  gradients: [-5.155476089398586e-05, -2.805615018899068e-05, -3.627345246018385e-05]\n",
      "epoch: 637  loss: 0.006889392432520143  params: [0.48491467 0.06430804 1.00546404]  gradients: [-5.1555840039134685e-05, -2.8056737460812835e-05, -3.627421173671962e-05]\n",
      "epoch: 638  loss: 0.006889580093226373  params: [0.48496623 0.0643361  1.00550031]  gradients: [-5.155691915239391e-05, -2.8057324715280644e-05, -3.627497099081815e-05]\n",
      "epoch: 639  loss: 0.006889767763461076  params: [0.48501779 0.06436416 1.00553659]  gradients: [-5.155799823375897e-05, -2.8057911952391613e-05, -3.627573022247624e-05]\n",
      "epoch: 640  loss: 0.0068899554432241186  params: [0.48506935 0.06439222 1.00557286]  gradients: [-5.155907728322528e-05, -2.805849917214326e-05, -3.627648943169067e-05]\n",
      "epoch: 641  loss: 0.006890143132515387  params: [0.48512091 0.06442028 1.00560914]  gradients: [-5.1560156300788306e-05, -2.8059086374533103e-05, -3.6277248618458244e-05]\n",
      "epoch: 642  loss: 0.006890330831334753  params: [0.48517247 0.06444834 1.00564542]  gradients: [-5.156123528644347e-05, -2.805967355955866e-05, -3.627800778277574e-05]\n",
      "epoch: 643  loss: 0.0068905185396820915  params: [0.48522403 0.0644764  1.0056817 ]  gradients: [-5.156231424018622e-05, -2.8060260727217447e-05, -3.627876692463995e-05]\n",
      "epoch: 644  loss: 0.006890706257557282  params: [0.48527559 0.06450446 1.00571798]  gradients: [-5.156339316201199e-05, -2.8060847877506986e-05, -3.627952604404768e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 645  loss: 0.006890893984960204  params: [0.48532716 0.06453252 1.00575426]  gradients: [-5.1564472051916195e-05, -2.8061435010424778e-05, -3.628028514099569e-05]\n",
      "epoch: 646  loss: 0.006891081721890734  params: [0.48537872 0.06456058 1.00579054]  gradients: [-5.1565550909894315e-05, -2.8062022125968365e-05, -3.6281044215480805e-05]\n",
      "epoch: 647  loss: 0.006891269468348749  params: [0.48543029 0.06458864 1.00582682]  gradients: [-5.156662973594176e-05, -2.8062609224135247e-05, -3.6281803267499785e-05]\n",
      "epoch: 648  loss: 0.006891457224334119  params: [0.48548186 0.06461671 1.0058631 ]  gradients: [-5.156770853005398e-05, -2.806319630492295e-05, -3.628256229704944e-05]\n",
      "epoch: 649  loss: 0.006891644989846732  params: [0.48553343 0.06464477 1.00589938]  gradients: [-5.156878729222641e-05, -2.8063783368328988e-05, -3.628332130412656e-05]\n",
      "epoch: 650  loss: 0.006891832764886459  params: [0.485585   0.06467283 1.00593567]  gradients: [-5.15698660224545e-05, -2.8064370414350882e-05, -3.628408028872792e-05]\n",
      "epoch: 651  loss: 0.0068920205494531735  params: [0.48563657 0.0647009  1.00597195]  gradients: [-5.1570944720733685e-05, -2.806495744298616e-05, -3.628483925085035e-05]\n",
      "epoch: 652  loss: 0.006892208343546753  params: [0.48568814 0.06472897 1.00600824]  gradients: [-5.15720233870594e-05, -2.8065544454232322e-05, -3.62855981904906e-05]\n",
      "epoch: 653  loss: 0.006892396147167077  params: [0.48573971 0.06475703 1.00604453]  gradients: [-5.1573102021427096e-05, -2.80661314480869e-05, -3.628635710764549e-05]\n",
      "epoch: 654  loss: 0.006892583960314022  params: [0.48579129 0.0647851  1.00608081]  gradients: [-5.1574180623832214e-05, -2.8066718424547414e-05, -3.62871160023118e-05]\n",
      "epoch: 655  loss: 0.0068927717829874655  params: [0.48584286 0.06481317 1.0061171 ]  gradients: [-5.1575259194270186e-05, -2.8067305383611374e-05, -3.6287874874486326e-05]\n",
      "epoch: 656  loss: 0.006892959615187279  params: [0.48589444 0.06484123 1.00615339]  gradients: [-5.157633773273648e-05, -2.8067892325276323e-05, -3.628863372416587e-05]\n",
      "epoch: 657  loss: 0.006893147456913347  params: [0.48594602 0.0648693  1.00618968]  gradients: [-5.1577416239226515e-05, -2.806847924953976e-05, -3.628939255134722e-05]\n",
      "epoch: 658  loss: 0.006893335308165535  params: [0.48599759 0.06489737 1.00622597]  gradients: [-5.1578494713735736e-05, -2.80690661563992e-05, -3.6290151356027165e-05]\n",
      "epoch: 659  loss: 0.0068935231689437205  params: [0.48604917 0.06492544 1.00626226]  gradients: [-5.1579573156259604e-05, -2.8069653045852185e-05, -3.629091013820251e-05]\n",
      "epoch: 660  loss: 0.006893711039247794  params: [0.48610075 0.06495351 1.00629855]  gradients: [-5.158065156679354e-05, -2.807023991789622e-05, -3.6291668897870045e-05]\n",
      "epoch: 661  loss: 0.0068938989190776186  params: [0.48615234 0.06498158 1.00633484]  gradients: [-5.158172994533302e-05, -2.807082677252884e-05, -3.629242763502656e-05]\n",
      "epoch: 662  loss: 0.00689408680843307  params: [0.48620392 0.06500965 1.00637114]  gradients: [-5.158280829187346e-05, -2.8071413609747552e-05, -3.6293186349668857e-05]\n",
      "epoch: 663  loss: 0.00689427470731403  params: [0.4862555  0.06503772 1.00640743]  gradients: [-5.158388660641033e-05, -2.8072000429549888e-05, -3.629394504179373e-05]\n",
      "epoch: 664  loss: 0.006894462615720371  params: [0.48630709 0.0650658  1.00644373]  gradients: [-5.158496488893906e-05, -2.8072587231933366e-05, -3.629470371139798e-05]\n",
      "epoch: 665  loss: 0.006894650533651969  params: [0.48635867 0.06509387 1.00648002]  gradients: [-5.15860431394551e-05, -2.8073174016895505e-05, -3.62954623584784e-05]\n",
      "epoch: 666  loss: 0.006894838461108706  params: [0.48641026 0.06512194 1.00651632]  gradients: [-5.15871213579539e-05, -2.8073760784433837e-05, -3.629622098303178e-05]\n",
      "epoch: 667  loss: 0.006895026398090449  params: [0.48646185 0.06515002 1.00655261]  gradients: [-5.1588199544430914e-05, -2.8074347534545876e-05, -3.629697958505493e-05]\n",
      "epoch: 668  loss: 0.006895214344597077  params: [0.48651344 0.06517809 1.00658891]  gradients: [-5.158927769888158e-05, -2.807493426722915e-05, -3.6297738164544634e-05]\n",
      "epoch: 669  loss: 0.006895402300628466  params: [0.48656503 0.06520617 1.00662521]  gradients: [-5.159035582130134e-05, -2.8075520982481173e-05, -3.62984967214977e-05]\n",
      "epoch: 670  loss: 0.006895590266184493  params: [0.48661662 0.06523425 1.00666151]  gradients: [-5.159143391168566e-05, -2.8076107680299478e-05, -3.629925525591091e-05]\n",
      "epoch: 671  loss: 0.0068957782412650305  params: [0.48666821 0.06526232 1.00669781]  gradients: [-5.159251197002998e-05, -2.807669436068159e-05, -3.6300013767781086e-05]\n",
      "epoch: 672  loss: 0.006895966225869959  params: [0.48671981 0.0652904  1.00673411]  gradients: [-5.159358999632975e-05, -2.807728102362502e-05, -3.630077225710501e-05]\n",
      "epoch: 673  loss: 0.006896154219999149  params: [0.4867714  0.06531848 1.00677041]  gradients: [-5.159466799058042e-05, -2.8077867669127304e-05, -3.6301530723879486e-05]\n",
      "epoch: 674  loss: 0.006896342223652483  params: [0.486823   0.06534656 1.00680671]  gradients: [-5.159574595277744e-05, -2.8078454297185958e-05, -3.6302289168101315e-05]\n",
      "epoch: 675  loss: 0.006896530236829824  params: [0.48687459 0.06537463 1.00684302]  gradients: [-5.1596823882916275e-05, -2.8079040907798514e-05, -3.6303047589767286e-05]\n",
      "epoch: 676  loss: 0.0068967182595310594  params: [0.48692619 0.06540271 1.00687932]  gradients: [-5.159790178099237e-05, -2.8079627500962504e-05, -3.6303805988874215e-05]\n",
      "epoch: 677  loss: 0.006896906291756058  params: [0.48697779 0.06543079 1.00691563]  gradients: [-5.1598979647001164e-05, -2.808021407667543e-05, -3.63045643654189e-05]\n",
      "epoch: 678  loss: 0.0068970943335046955  params: [0.48702939 0.06545888 1.00695193]  gradients: [-5.1600057480938114e-05, -2.8080800634934834e-05, -3.6305322719398126e-05]\n",
      "epoch: 679  loss: 0.006897282384776859  params: [0.48708099 0.06548696 1.00698824]  gradients: [-5.160113528279868e-05, -2.8081387175738235e-05, -3.63060810508087e-05]\n",
      "epoch: 680  loss: 0.006897470445572401  params: [0.48713259 0.06551504 1.00702454]  gradients: [-5.160221305257831e-05, -2.8081973699083165e-05, -3.630683935964743e-05]\n",
      "epoch: 681  loss: 0.006897658515891219  params: [0.4871842  0.06554312 1.00706085]  gradients: [-5.160329079027246e-05, -2.808256020496714e-05, -3.630759764591111e-05]\n",
      "epoch: 682  loss: 0.006897846595733168  params: [0.4872358  0.0655712  1.00709716]  gradients: [-5.1604368495876576e-05, -2.8083146693387694e-05, -3.6308355909596545e-05]\n",
      "epoch: 683  loss: 0.006898034685098139  params: [0.48728741 0.06559929 1.00713347]  gradients: [-5.160544616938613e-05, -2.8083733164342356e-05, -3.630911415070054e-05]\n",
      "epoch: 684  loss: 0.006898222783985999  params: [0.48733901 0.06562737 1.00716978]  gradients: [-5.160652381079656e-05, -2.8084319617828637e-05, -3.6309872369219886e-05]\n",
      "epoch: 685  loss: 0.0068984108923966225  params: [0.48739062 0.06565546 1.00720609]  gradients: [-5.160760142010333e-05, -2.808490605384408e-05, -3.63106305651514e-05]\n",
      "epoch: 686  loss: 0.006898599010329891  params: [0.48744223 0.06568354 1.0072424 ]  gradients: [-5.160867899730189e-05, -2.8085492472386208e-05, -3.631138873849187e-05]\n",
      "epoch: 687  loss: 0.006898787137785673  params: [0.48749384 0.06571163 1.00727871]  gradients: [-5.16097565423877e-05, -2.8086078873452544e-05, -3.6312146889238105e-05]\n",
      "epoch: 688  loss: 0.006898975274763845  params: [0.48754545 0.06573972 1.00731503]  gradients: [-5.161083405535622e-05, -2.8086665257040616e-05, -3.631290501738691e-05]\n",
      "epoch: 689  loss: 0.006899163421264275  params: [0.48759706 0.0657678  1.00735134]  gradients: [-5.16119115362029e-05, -2.8087251623147954e-05, -3.6313663122935085e-05]\n",
      "epoch: 690  loss: 0.006899351577286855  params: [0.48764868 0.06579589 1.00738765]  gradients: [-5.16129889849232e-05, -2.808783797177209e-05, -3.6314421205879445e-05]\n",
      "epoch: 691  loss: 0.006899539742831443  params: [0.48770029 0.06582398 1.00742397]  gradients: [-5.1614066401512575e-05, -2.808842430291054e-05, -3.631517926621679e-05]\n",
      "epoch: 692  loss: 0.006899727917897924  params: [0.4877519  0.06585207 1.00746029]  gradients: [-5.1615143785966504e-05, -2.808901061656085e-05, -3.631593730394391e-05]\n",
      "epoch: 693  loss: 0.006899916102486165  params: [0.48780352 0.06588016 1.0074966 ]  gradients: [-5.161622113828041e-05, -2.8089596912720537e-05, -3.631669531905762e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 694  loss: 0.006900104296596041  params: [0.48785514 0.06590825 1.00753292]  gradients: [-5.161729845844977e-05, -2.8090183191387123e-05, -3.6317453311554734e-05]\n",
      "epoch: 695  loss: 0.006900292500227432  params: [0.48790676 0.06593634 1.00756924]  gradients: [-5.161837574647006e-05, -2.8090769452558154e-05, -3.631821128143205e-05]\n",
      "epoch: 696  loss: 0.00690048071338021  params: [0.48795838 0.06596443 1.00760556]  gradients: [-5.161945300233672e-05, -2.8091355696231152e-05, -3.6318969228686364e-05]\n",
      "epoch: 697  loss: 0.0069006689360542415  params: [0.48801    0.06599252 1.00764188]  gradients: [-5.16205302260452e-05, -2.8091941922403643e-05, -3.631972715331449e-05]\n",
      "epoch: 698  loss: 0.006900857168249418  params: [0.48806162 0.06602061 1.0076782 ]  gradients: [-5.162160741759099e-05, -2.8092528131073157e-05, -3.632048505531324e-05]\n",
      "epoch: 699  loss: 0.006901045409965601  params: [0.48811324 0.06604871 1.00771452]  gradients: [-5.1622684576969535e-05, -2.8093114322237227e-05, -3.632124293467941e-05]\n",
      "epoch: 700  loss: 0.006901233661202667  params: [0.48816486 0.0660768  1.00775084]  gradients: [-5.162376170417629e-05, -2.8093700495893388e-05, -3.632200079140982e-05]\n",
      "epoch: 701  loss: 0.006901421921960484  params: [0.48821649 0.0661049  1.00778716]  gradients: [-5.1624838799206734e-05, -2.8094286652039157e-05, -3.632275862550127e-05]\n",
      "epoch: 702  loss: 0.006901610192238938  params: [0.48826812 0.06613299 1.00782349]  gradients: [-5.162591586205632e-05, -2.8094872790672077e-05, -3.6323516436950554e-05]\n",
      "epoch: 703  loss: 0.006901798472037891  params: [0.48831974 0.06616109 1.00785981]  gradients: [-5.162699289272051e-05, -2.8095458911789677e-05, -3.6324274225754504e-05]\n",
      "epoch: 704  loss: 0.006901986761357228  params: [0.48837137 0.06618918 1.00789614]  gradients: [-5.162806989119477e-05, -2.8096045015389484e-05, -3.632503199190992e-05]\n",
      "epoch: 705  loss: 0.006902175060196812  params: [0.488423   0.06621728 1.00793246]  gradients: [-5.162914685747457e-05, -2.8096631101469034e-05, -3.632578973541361e-05]\n",
      "epoch: 706  loss: 0.006902363368556525  params: [0.48847463 0.06624538 1.00796879]  gradients: [-5.163022379155536e-05, -2.8097217170025854e-05, -3.632654745626237e-05]\n",
      "epoch: 707  loss: 0.006902551686436242  params: [0.48852626 0.06627347 1.00800512]  gradients: [-5.163130069343262e-05, -2.809780322105748e-05, -3.632730515445302e-05]\n",
      "epoch: 708  loss: 0.006902740013835835  params: [0.48857789 0.06630157 1.00804144]  gradients: [-5.16323775631018e-05, -2.809838925456144e-05, -3.632806282998237e-05]\n",
      "epoch: 709  loss: 0.006902928350755168  params: [0.48862953 0.06632967 1.00807777]  gradients: [-5.163345440055837e-05, -2.8098975270535267e-05, -3.632882048284723e-05]\n",
      "epoch: 710  loss: 0.006903116697194129  params: [0.48868116 0.06635777 1.0081141 ]  gradients: [-5.163453120579781e-05, -2.8099561268976496e-05, -3.6329578113044406e-05]\n",
      "epoch: 711  loss: 0.0069033050531525815  params: [0.4887328  0.06638587 1.00815043]  gradients: [-5.163560797881556e-05, -2.810014724988266e-05, -3.63303357205707e-05]\n",
      "epoch: 712  loss: 0.006903493418630404  params: [0.48878443 0.06641397 1.00818676]  gradients: [-5.1636684719607116e-05, -2.810073321325129e-05, -3.6331093305422946e-05]\n",
      "epoch: 713  loss: 0.006903681793627468  params: [0.48883607 0.06644207 1.0082231 ]  gradients: [-5.163776142816792e-05, -2.8101319159079916e-05, -3.633185086759794e-05]\n",
      "epoch: 714  loss: 0.006903870178143648  params: [0.48888771 0.06647017 1.00825943]  gradients: [-5.163883810449347e-05, -2.8101905087366088e-05, -3.633260840709249e-05]\n",
      "epoch: 715  loss: 0.0069040585721788115  params: [0.48893935 0.06649828 1.00829576]  gradients: [-5.1639914748579196e-05, -2.8102490998107314e-05, -3.6333365923903414e-05]\n",
      "epoch: 716  loss: 0.006904246975732836  params: [0.48899099 0.06652638 1.0083321 ]  gradients: [-5.164099136042058e-05, -2.8103076891301142e-05, -3.6334123418027515e-05]\n",
      "epoch: 717  loss: 0.006904435388805606  params: [0.48904263 0.06655448 1.00836843]  gradients: [-5.164206794001312e-05, -2.810366276694511e-05, -3.633488088946163e-05]\n",
      "epoch: 718  loss: 0.006904623811396974  params: [0.48909428 0.06658259 1.00840477]  gradients: [-5.164314448735225e-05, -2.810424862503675e-05, -3.6335638338202545e-05]\n",
      "epoch: 719  loss: 0.006904812243506836  params: [0.48914592 0.06661069 1.0084411 ]  gradients: [-5.164422100243345e-05, -2.8104834465573596e-05, -3.633639576424707e-05]\n",
      "epoch: 720  loss: 0.006905000685135039  params: [0.48919757 0.0666388  1.00847744]  gradients: [-5.16452974852522e-05, -2.8105420288553183e-05, -3.633715316759204e-05]\n",
      "epoch: 721  loss: 0.006905189136281477  params: [0.48924921 0.0666669  1.00851378]  gradients: [-5.1646373935803965e-05, -2.8106006093973045e-05, -3.633791054823425e-05]\n",
      "epoch: 722  loss: 0.0069053775969460125  params: [0.48930086 0.06669501 1.00855012]  gradients: [-5.1647450354084214e-05, -2.810659188183071e-05, -3.6338667906170526e-05]\n",
      "epoch: 723  loss: 0.006905566067128522  params: [0.48935251 0.06672312 1.00858646]  gradients: [-5.164852674008841e-05, -2.8107177652123722e-05, -3.633942524139767e-05]\n",
      "epoch: 724  loss: 0.006905754546828882  params: [0.48940416 0.06675123 1.0086228 ]  gradients: [-5.164960309381204e-05, -2.8107763404849623e-05, -3.6340182553912506e-05]\n",
      "epoch: 725  loss: 0.00690594303604696  params: [0.48945581 0.06677933 1.00865914]  gradients: [-5.165067941525056e-05, -2.8108349140005934e-05, -3.634093984371184e-05]\n",
      "epoch: 726  loss: 0.006906131534782625  params: [0.48950746 0.06680744 1.00869548]  gradients: [-5.165175570439945e-05, -2.8108934857590197e-05, -3.6341697110792495e-05]\n",
      "epoch: 727  loss: 0.0069063200430357595  params: [0.48955911 0.06683555 1.00873182]  gradients: [-5.165283196125421e-05, -2.810952055759996e-05, -3.634245435515129e-05]\n",
      "epoch: 728  loss: 0.006906508560806227  params: [0.48961077 0.06686366 1.00876816]  gradients: [-5.165390818581027e-05, -2.8110106240032742e-05, -3.634321157678502e-05]\n",
      "epoch: 729  loss: 0.006906697088093914  params: [0.48966242 0.06689177 1.00880451]  gradients: [-5.165498437806313e-05, -2.811069190488609e-05, -3.6343968775690515e-05]\n",
      "epoch: 730  loss: 0.00690688562489868  params: [0.48971408 0.06691988 1.00884085]  gradients: [-5.165606053800827e-05, -2.811127755215755e-05, -3.63447259518646e-05]\n",
      "epoch: 731  loss: 0.006907074171220393  params: [0.48976574 0.066948   1.0088772 ]  gradients: [-5.165713666564113e-05, -2.8111863181844634e-05, -3.6345483105304064e-05]\n",
      "epoch: 732  loss: 0.006907262727058939  params: [0.48981739 0.06697611 1.00891354]  gradients: [-5.165821276095722e-05, -2.81124487939449e-05, -3.6346240236005745e-05]\n",
      "epoch: 733  loss: 0.006907451292414185  params: [0.48986905 0.06700422 1.00894989]  gradients: [-5.165928882395202e-05, -2.811303438845589e-05, -3.6346997343966474e-05]\n",
      "epoch: 734  loss: 0.006907639867286008  params: [0.48992071 0.06703234 1.00898624]  gradients: [-5.1660364854620966e-05, -2.811361996537512e-05, -3.634775442918302e-05]\n",
      "epoch: 735  loss: 0.006907828451674275  params: [0.48997237 0.06706045 1.00902259]  gradients: [-5.166144085295958e-05, -2.811420552470015e-05, -3.6348511491652254e-05]\n",
      "epoch: 736  loss: 0.00690801704557885  params: [0.49002404 0.06708856 1.00905894]  gradients: [-5.166251681896331e-05, -2.811479106642851e-05, -3.6349268531370954e-05]\n",
      "epoch: 737  loss: 0.006908205648999622  params: [0.4900757  0.06711668 1.00909529]  gradients: [-5.1663592752627645e-05, -2.8115376590557737e-05, -3.635002554833596e-05]\n",
      "epoch: 738  loss: 0.006908394261936457  params: [0.49012737 0.0671448  1.00913164]  gradients: [-5.1664668653948046e-05, -2.8115962097085364e-05, -3.635078254254408e-05]\n",
      "epoch: 739  loss: 0.006908582884389219  params: [0.49017903 0.06717291 1.00916799]  gradients: [-5.1665744522920035e-05, -2.8116547586008953e-05, -3.635153951399215e-05]\n",
      "epoch: 740  loss: 0.0069087715163577874  params: [0.4902307  0.06720103 1.00920434]  gradients: [-5.166682035953904e-05, -2.811713305732602e-05, -3.635229646267696e-05]\n",
      "epoch: 741  loss: 0.0069089601578420385  params: [0.49028237 0.06722915 1.00924069]  gradients: [-5.166789616380057e-05, -2.8117718511034116e-05, -3.6353053388595346e-05]\n",
      "epoch: 742  loss: 0.006909148808841833  params: [0.49033403 0.06725727 1.00927705]  gradients: [-5.166897193570008e-05, -2.8118303947130774e-05, -3.6353810291744125e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 743  loss: 0.00690933746935706  params: [0.4903857  0.06728538 1.0093134 ]  gradients: [-5.167004767523308e-05, -2.8118889365613545e-05, -3.635456717212012e-05]\n",
      "epoch: 744  loss: 0.006909526139387571  params: [0.49043738 0.0673135  1.00934976]  gradients: [-5.1671123382395054e-05, -2.8119474766479964e-05, -3.6355324029720156e-05]\n",
      "epoch: 745  loss: 0.006909714818933253  params: [0.49048905 0.06734162 1.00938611]  gradients: [-5.1672199057181446e-05, -2.812006014972757e-05, -3.635608086454104e-05]\n",
      "epoch: 746  loss: 0.006909903507993968  params: [0.49054072 0.06736974 1.00942247]  gradients: [-5.167327469958776e-05, -2.8120645515353906e-05, -3.635683767657959e-05]\n",
      "epoch: 747  loss: 0.006910092206569588  params: [0.4905924  0.06739787 1.00945883]  gradients: [-5.1674350309609486e-05, -2.8121230863356515e-05, -3.635759446583266e-05]\n",
      "epoch: 748  loss: 0.0069102809146599975  params: [0.49064407 0.06742599 1.00949519]  gradients: [-5.167542588724209e-05, -2.812181619373293e-05, -3.635835123229704e-05]\n",
      "epoch: 749  loss: 0.006910469632265053  params: [0.49069575 0.06745411 1.00953155]  gradients: [-5.167650143248105e-05, -2.8122401506480704e-05, -3.6359107975969544e-05]\n",
      "epoch: 750  loss: 0.006910658359384637  params: [0.49074743 0.06748223 1.00956791]  gradients: [-5.167757694532186e-05, -2.8122986801597365e-05, -3.635986469684702e-05]\n",
      "epoch: 751  loss: 0.006910847096018614  params: [0.4907991  0.06751036 1.00960427]  gradients: [-5.167865242576002e-05, -2.8123572079080473e-05, -3.636062139492628e-05]\n",
      "epoch: 752  loss: 0.006911035842166853  params: [0.49085078 0.06753848 1.00964063]  gradients: [-5.167972787379099e-05, -2.812415733892756e-05, -3.6361378070204144e-05]\n",
      "epoch: 753  loss: 0.006911224597829232  params: [0.49090246 0.06756661 1.00967699]  gradients: [-5.168080328941027e-05, -2.8124742581136173e-05, -3.6362134722677453e-05]\n",
      "epoch: 754  loss: 0.00691141336300562  params: [0.49095415 0.06759473 1.00971335]  gradients: [-5.168187867261333e-05, -2.8125327805703845e-05, -3.636289135234299e-05]\n",
      "epoch: 755  loss: 0.006911602137695896  params: [0.49100583 0.06762286 1.00974972]  gradients: [-5.168295402339567e-05, -2.8125913012628133e-05, -3.636364795919763e-05]\n",
      "epoch: 756  loss: 0.006911790921899915  params: [0.49105751 0.06765098 1.00978608]  gradients: [-5.168402934175276e-05, -2.812649820190657e-05, -3.636440454323815e-05]\n",
      "epoch: 757  loss: 0.006911979715617558  params: [0.4911092  0.06767911 1.00982245]  gradients: [-5.1685104627680086e-05, -2.8127083373536695e-05, -3.63651611044614e-05]\n",
      "epoch: 758  loss: 0.006912168518848695  params: [0.49116088 0.06770724 1.00985881]  gradients: [-5.1686179881173155e-05, -2.812766852751607e-05, -3.63659176428642e-05]\n",
      "epoch: 759  loss: 0.006912357331593199  params: [0.49121257 0.06773537 1.00989518]  gradients: [-5.1687255102227437e-05, -2.8128253663842222e-05, -3.636667415844336e-05]\n",
      "epoch: 760  loss: 0.006912546153850944  params: [0.49126426 0.0677635  1.00993155]  gradients: [-5.168833029083843e-05, -2.812883878251271e-05, -3.6367430651195734e-05]\n",
      "epoch: 761  loss: 0.006912734985621786  params: [0.49131595 0.06779162 1.00996791]  gradients: [-5.1689405447001615e-05, -2.8129423883525066e-05, -3.636818712111813e-05]\n",
      "epoch: 762  loss: 0.006912923826905616  params: [0.49136764 0.06781975 1.01000428]  gradients: [-5.169048057071248e-05, -2.8130008966876842e-05, -3.6368943568207375e-05]\n",
      "epoch: 763  loss: 0.006913112677702295  params: [0.49141933 0.06784789 1.01004065]  gradients: [-5.1691555661966516e-05, -2.813059403256558e-05, -3.6369699992460294e-05]\n",
      "epoch: 764  loss: 0.006913301538011683  params: [0.49147102 0.06787602 1.01007702]  gradients: [-5.1692630720759216e-05, -2.8131179080588824e-05, -3.6370456393873715e-05]\n",
      "epoch: 765  loss: 0.006913490407833668  params: [0.49152272 0.06790415 1.01011339]  gradients: [-5.169370574708605e-05, -2.8131764110944117e-05, -3.637121277244446e-05]\n",
      "epoch: 766  loss: 0.006913679287168116  params: [0.49157441 0.06793228 1.01014977]  gradients: [-5.169478074094254e-05, -2.8132349123629018e-05, -3.6371969128169365e-05]\n",
      "epoch: 767  loss: 0.006913868176014898  params: [0.49162611 0.06796041 1.01018614]  gradients: [-5.169585570232415e-05, -2.8132934118641055e-05, -3.637272546104525e-05]\n",
      "epoch: 768  loss: 0.006914057074373882  params: [0.49167781 0.06798855 1.01022251]  gradients: [-5.1696930631226386e-05, -2.813351909597779e-05, -3.637348177106895e-05]\n",
      "epoch: 769  loss: 0.006914245982244932  params: [0.4917295  0.06801668 1.01025889]  gradients: [-5.169800552764473e-05, -2.813410405563676e-05, -3.6374238058237275e-05]\n",
      "epoch: 770  loss: 0.006914434899627935  params: [0.4917812  0.06804482 1.01029526]  gradients: [-5.1699080391574685e-05, -2.813468899761551e-05, -3.637499432254708e-05]\n",
      "epoch: 771  loss: 0.006914623826522746  params: [0.4918329  0.06807295 1.01033164]  gradients: [-5.170015522301174e-05, -2.81352739219116e-05, -3.637575056399517e-05]\n",
      "epoch: 772  loss: 0.006914812762929247  params: [0.4918846  0.06810109 1.01036801]  gradients: [-5.1701230021951375e-05, -2.8135858828522558e-05, -3.637650678257838e-05]\n",
      "epoch: 773  loss: 0.006915001708847299  params: [0.49193631 0.06812922 1.01040439]  gradients: [-5.1702304788389094e-05, -2.8136443717445944e-05, -3.637726297829355e-05]\n",
      "epoch: 774  loss: 0.006915190664276778  params: [0.49198801 0.06815736 1.01044077]  gradients: [-5.17033795223204e-05, -2.8137028588679308e-05, -3.63780191511375e-05]\n",
      "epoch: 775  loss: 0.006915379629217555  params: [0.49203971 0.0681855  1.01047715]  gradients: [-5.1704454223740765e-05, -2.813761344222019e-05, -3.6378775301107056e-05]\n",
      "epoch: 776  loss: 0.006915568603669495  params: [0.49209142 0.06821364 1.01051353]  gradients: [-5.170552889264569e-05, -2.8138198278066136e-05, -3.637953142819905e-05]\n",
      "epoch: 777  loss: 0.0069157575876324685  params: [0.49214313 0.06824177 1.01054991]  gradients: [-5.170660352903068e-05, -2.8138783096214705e-05, -3.638028753241032e-05]\n",
      "epoch: 778  loss: 0.006915946581106354  params: [0.49219483 0.06826991 1.01058629]  gradients: [-5.170767813289123e-05, -2.8139367896663437e-05, -3.638104361373769e-05]\n",
      "epoch: 779  loss: 0.006916135584091013  params: [0.49224654 0.06829805 1.01062267]  gradients: [-5.1708752704222824e-05, -2.8139952679409888e-05, -3.6381799672178e-05]\n",
      "epoch: 780  loss: 0.006916324596586317  params: [0.49229825 0.06832619 1.01065905]  gradients: [-5.170982724302096e-05, -2.814053744445159e-05, -3.638255570772806e-05]\n",
      "epoch: 781  loss: 0.006916513618592141  params: [0.49234996 0.06835434 1.01069544]  gradients: [-5.171090174928114e-05, -2.8141122191786116e-05, -3.6383311720384714e-05]\n",
      "epoch: 782  loss: 0.00691670265010835  params: [0.49240168 0.06838248 1.01073182]  gradients: [-5.171197622299887e-05, -2.8141706921411e-05, -3.6384067710144806e-05]\n",
      "epoch: 783  loss: 0.006916891691134814  params: [0.49245339 0.06841062 1.01076821]  gradients: [-5.171305066416963e-05, -2.8142291633323796e-05, -3.638482367700515e-05]\n",
      "epoch: 784  loss: 0.006917080741671405  params: [0.4925051  0.06843876 1.01080459]  gradients: [-5.171412507278893e-05, -2.8142876327522056e-05, -3.638557962096259e-05]\n",
      "epoch: 785  loss: 0.006917269801717995  params: [0.49255682 0.06846691 1.01084098]  gradients: [-5.171519944885226e-05, -2.8143461004003328e-05, -3.638633554201395e-05]\n",
      "epoch: 786  loss: 0.006917458871274446  params: [0.49260853 0.06849505 1.01087736]  gradients: [-5.171627379235512e-05, -2.8144045662765158e-05, -3.638709144015606e-05]\n",
      "epoch: 787  loss: 0.0069176479503406324  params: [0.49266025 0.06852319 1.01091375]  gradients: [-5.1717348103293023e-05, -2.814463030380511e-05, -3.638784731538577e-05]\n",
      "epoch: 788  loss: 0.006917837038916417  params: [0.49271197 0.06855134 1.01095014]  gradients: [-5.171842238166144e-05, -2.814521492712072e-05, -3.63886031676999e-05]\n",
      "epoch: 789  loss: 0.006918026137001691  params: [0.49276369 0.06857949 1.01098653]  gradients: [-5.17194966274559e-05, -2.8145799532709545e-05, -3.638935899709529e-05]\n",
      "epoch: 790  loss: 0.006918215244596298  params: [0.49281541 0.06860763 1.01102292]  gradients: [-5.172057084067189e-05, -2.8146384120569138e-05, -3.639011480356877e-05]\n",
      "epoch: 791  loss: 0.006918404361700121  params: [0.49286713 0.06863578 1.01105931]  gradients: [-5.172164502130491e-05, -2.8146968690697048e-05, -3.639087058711717e-05]\n",
      "epoch: 792  loss: 0.006918593488313027  params: [0.49291885 0.06866393 1.0110957 ]  gradients: [-5.1722719169350466e-05, -2.8147553243090825e-05, -3.639162634773733e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 793  loss: 0.006918782624434882  params: [0.49297058 0.06869208 1.0111321 ]  gradients: [-5.1723793284804056e-05, -2.8148137777748024e-05, -3.6392382085426097e-05]\n",
      "epoch: 794  loss: 0.006918971770065562  params: [0.4930223  0.06872022 1.01116849]  gradients: [-5.1724867367661196e-05, -2.814872229466621e-05, -3.639313780018029e-05]\n",
      "epoch: 795  loss: 0.006919160925204929  params: [0.49307403 0.06874837 1.01120488]  gradients: [-5.172594141791737e-05, -2.814930679384291e-05, -3.639389349199675e-05]\n",
      "epoch: 796  loss: 0.00691935008985286  params: [0.49312576 0.06877652 1.01124128]  gradients: [-5.172701543556809e-05, -2.8149891275275698e-05, -3.639464916087232e-05]\n",
      "epoch: 797  loss: 0.006919539264009217  params: [0.49317748 0.06880467 1.01127767]  gradients: [-5.172808942060885e-05, -2.8150475738962113e-05, -3.6395404806803813e-05]\n",
      "epoch: 798  loss: 0.006919728447673873  params: [0.49322921 0.06883282 1.01131407]  gradients: [-5.172916337303517e-05, -2.8151060184899716e-05, -3.63961604297881e-05]\n",
      "epoch: 799  loss: 0.006919917640846695  params: [0.49328094 0.06886098 1.01135047]  gradients: [-5.173023729284254e-05, -2.815164461308606e-05, -3.639691602982199e-05]\n",
      "epoch: 800  loss: 0.0069201068435275495  params: [0.49333267 0.06888913 1.01138686]  gradients: [-5.1731311180026486e-05, -2.8152229023518697e-05, -3.639767160690234e-05]\n",
      "epoch: 801  loss: 0.006920296055716317  params: [0.49338441 0.06891728 1.01142326]  gradients: [-5.173238503458247e-05, -2.8152813416195176e-05, -3.639842716102596e-05]\n",
      "epoch: 802  loss: 0.006920485277412855  params: [0.49343614 0.06894543 1.01145966]  gradients: [-5.173345885650604e-05, -2.815339779111306e-05, -3.639918269218971e-05]\n",
      "epoch: 803  loss: 0.006920674508617033  params: [0.49348787 0.06897359 1.01149606]  gradients: [-5.17345326457927e-05, -2.8153982148269905e-05, -3.639993820039044e-05]\n",
      "epoch: 804  loss: 0.006920863749328721  params: [0.49353961 0.06900174 1.01153246]  gradients: [-5.1735606402437946e-05, -2.8154566487663258e-05, -3.640069368562497e-05]\n",
      "epoch: 805  loss: 0.006921052999547793  params: [0.49359135 0.0690299  1.01156886]  gradients: [-5.1736680126437266e-05, -2.815515080929067e-05, -3.640144914789013e-05]\n",
      "epoch: 806  loss: 0.006921242259274116  params: [0.49364308 0.06905805 1.01160527]  gradients: [-5.1737753817786197e-05, -2.8155735113149715e-05, -3.6402204587182774e-05]\n",
      "epoch: 807  loss: 0.006921431528507551  params: [0.49369482 0.06908621 1.01164167]  gradients: [-5.173882747648024e-05, -2.815631939923793e-05, -3.6402960003499744e-05]\n",
      "epoch: 808  loss: 0.006921620807247978  params: [0.49374656 0.06911437 1.01167807]  gradients: [-5.173990110251489e-05, -2.8156903667552876e-05, -3.640371539683787e-05]\n",
      "epoch: 809  loss: 0.006921810095495256  params: [0.4937983  0.06914252 1.01171448]  gradients: [-5.1740974695885664e-05, -2.815748791809211e-05, -3.640447076719399e-05]\n",
      "epoch: 810  loss: 0.006921999393249261  params: [0.49385005 0.06917068 1.01175088]  gradients: [-5.174204825658808e-05, -2.8158072150853187e-05, -3.640522611456496e-05]\n",
      "epoch: 811  loss: 0.0069221887005098594  params: [0.49390179 0.06919884 1.01178729]  gradients: [-5.174312178461764e-05, -2.815865636583367e-05, -3.640598143894761e-05]\n",
      "epoch: 812  loss: 0.006922378017276906  params: [0.49395353 0.069227   1.01182369]  gradients: [-5.1744195279969845e-05, -2.8159240563031107e-05, -3.6406736740338777e-05]\n",
      "epoch: 813  loss: 0.006922567343550289  params: [0.49400528 0.06925516 1.0118601 ]  gradients: [-5.1745268742640216e-05, -2.8159824742443057e-05, -3.640749201873531e-05]\n",
      "epoch: 814  loss: 0.006922756679329868  params: [0.49405703 0.06928332 1.01189651]  gradients: [-5.174634217262427e-05, -2.8160408904067078e-05, -3.6408247274134045e-05]\n",
      "epoch: 815  loss: 0.006922946024615509  params: [0.49410877 0.06931148 1.01193292]  gradients: [-5.1747415569917506e-05, -2.8160993047900727e-05, -3.640900250653183e-05]\n",
      "epoch: 816  loss: 0.006923135379407091  params: [0.49416052 0.06933964 1.01196933]  gradients: [-5.174848893451544e-05, -2.8161577173941562e-05, -3.64097577159255e-05]\n",
      "epoch: 817  loss: 0.006923324743704462  params: [0.49421227 0.06936781 1.01200574]  gradients: [-5.1749562266413586e-05, -2.8162161282187142e-05, -3.641051290231191e-05]\n",
      "epoch: 818  loss: 0.006923514117507506  params: [0.49426402 0.06939597 1.01204215]  gradients: [-5.175063556560746e-05, -2.8162745372635025e-05, -3.641126806568788e-05]\n",
      "epoch: 819  loss: 0.006923703500816092  params: [0.49431577 0.06942413 1.01207856]  gradients: [-5.1751708832092556e-05, -2.816332944528276e-05, -3.641202320605027e-05]\n",
      "epoch: 820  loss: 0.006923892893630084  params: [0.49436753 0.0694523  1.01211498]  gradients: [-5.1752782065864416e-05, -2.8163913500127922e-05, -3.641277832339593e-05]\n",
      "epoch: 821  loss: 0.006924082295949336  params: [0.49441928 0.06948046 1.01215139]  gradients: [-5.175385526691854e-05, -2.8164497537168057e-05, -3.641353341772169e-05]\n",
      "epoch: 822  loss: 0.006924271707773743  params: [0.49447103 0.06950863 1.0121878 ]  gradients: [-5.175492843525044e-05, -2.816508155640073e-05, -3.64142884890244e-05]\n",
      "epoch: 823  loss: 0.006924461129103157  params: [0.49452279 0.06953679 1.01222422]  gradients: [-5.175600157085563e-05, -2.8165665557823496e-05, -3.64150435373009e-05]\n",
      "epoch: 824  loss: 0.006924650559937442  params: [0.49457455 0.06956496 1.01226063]  gradients: [-5.175707467372964e-05, -2.8166249541433918e-05, -3.6415798562548034e-05]\n",
      "epoch: 825  loss: 0.00692484000027648  params: [0.49462631 0.06959312 1.01229705]  gradients: [-5.175814774386798e-05, -2.816683350722956e-05, -3.641655356476266e-05]\n",
      "epoch: 826  loss: 0.006925029450120122  params: [0.49467807 0.06962129 1.01233347]  gradients: [-5.175922078126615e-05, -2.816741745520797e-05, -3.641730854394161e-05]\n",
      "epoch: 827  loss: 0.006925218909468248  params: [0.49472983 0.06964946 1.01236989]  gradients: [-5.176029378591968e-05, -2.8168001385366714e-05, -3.641806350008173e-05]\n",
      "epoch: 828  loss: 0.006925408378320719  params: [0.49478159 0.06967763 1.0124063 ]  gradients: [-5.17613667578241e-05, -2.8168585297703362e-05, -3.641881843317987e-05]\n",
      "epoch: 829  loss: 0.006925597856677405  params: [0.49483335 0.0697058  1.01244272]  gradients: [-5.176243969697489e-05, -2.8169169192215455e-05, -3.6419573343232874e-05]\n",
      "epoch: 830  loss: 0.00692578734453817  params: [0.49488511 0.06973397 1.01247914]  gradients: [-5.17635126033676e-05, -2.8169753068900568e-05, -3.642032823023759e-05]\n",
      "epoch: 831  loss: 0.0069259768419028854  params: [0.49493688 0.06976214 1.01251557]  gradients: [-5.176458547699775e-05, -2.8170336927756267e-05, -3.642108309419088e-05]\n",
      "epoch: 832  loss: 0.006926166348771418  params: [0.49498864 0.06979031 1.01255199]  gradients: [-5.176565831786083e-05, -2.8170920768780096e-05, -3.642183793508956e-05]\n",
      "epoch: 833  loss: 0.00692635586514364  params: [0.49504041 0.06981848 1.01258841]  gradients: [-5.17667311259524e-05, -2.817150459196963e-05, -3.64225927529305e-05]\n",
      "epoch: 834  loss: 0.00692654539101941  params: [0.49509218 0.06984665 1.01262483]  gradients: [-5.176780390126794e-05, -2.8172088397322425e-05, -3.6423347547710534e-05]\n",
      "epoch: 835  loss: 0.006926734926398594  params: [0.49514395 0.06987482 1.01266126]  gradients: [-5.1768876643803e-05, -2.817267218483605e-05, -3.642410231942653e-05]\n",
      "epoch: 836  loss: 0.006926924471281071  params: [0.49519572 0.069903   1.01269768]  gradients: [-5.1769949353553085e-05, -2.8173255954508052e-05, -3.642485706807531e-05]\n",
      "epoch: 837  loss: 0.0069271140256667  params: [0.49524749 0.06993117 1.01273411]  gradients: [-5.177102203051372e-05, -2.817383970633602e-05, -3.642561179365375e-05]\n",
      "epoch: 838  loss: 0.006927303589555347  params: [0.49529926 0.06995935 1.01277053]  gradients: [-5.177209467468043e-05, -2.817442344031749e-05, -3.642636649615868e-05]\n",
      "epoch: 839  loss: 0.0069274931629468815  params: [0.49535103 0.06998752 1.01280696]  gradients: [-5.177316728604873e-05, -2.8175007156450034e-05, -3.6427121175586955e-05]\n",
      "epoch: 840  loss: 0.00692768274584117  params: [0.49540281 0.0700157  1.01284339]  gradients: [-5.177423986461414e-05, -2.8175590854731224e-05, -3.6427875831935414e-05]\n",
      "epoch: 841  loss: 0.006927872338238078  params: [0.49545458 0.07004387 1.01287982]  gradients: [-5.1775312410372194e-05, -2.817617453515861e-05, -3.642863046520093e-05]\n",
      "epoch: 842  loss: 0.006928061940137476  params: [0.49550636 0.07007205 1.01291625]  gradients: [-5.177638492331841e-05, -2.8176758197729765e-05, -3.642938507538034e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 843  loss: 0.006928251551539227  params: [0.49555814 0.07010023 1.01295268]  gradients: [-5.177745740344831e-05, -2.8177341842442246e-05, -3.643013966247049e-05]\n",
      "epoch: 844  loss: 0.006928441172443197  params: [0.49560991 0.0701284  1.01298911]  gradients: [-5.1778529850757424e-05, -2.8177925469293632e-05, -3.643089422646824e-05]\n",
      "epoch: 845  loss: 0.006928630802849261  params: [0.49566169 0.07015658 1.01302554]  gradients: [-5.177960226524127e-05, -2.8178509078281466e-05, -3.643164876737043e-05]\n",
      "epoch: 846  loss: 0.006928820442757275  params: [0.49571348 0.07018476 1.01306197]  gradients: [-5.178067464689538e-05, -2.8179092669403332e-05, -3.643240328517393e-05]\n",
      "epoch: 847  loss: 0.006929010092167107  params: [0.49576526 0.07021294 1.01309841]  gradients: [-5.178174699571527e-05, -2.8179676242656784e-05, -3.643315777987557e-05]\n",
      "epoch: 848  loss: 0.006929199751078632  params: [0.49581704 0.07024112 1.01313484]  gradients: [-5.178281931169647e-05, -2.8180259798039392e-05, -3.643391225147222e-05]\n",
      "epoch: 849  loss: 0.00692938941949171  params: [0.49586882 0.0702693  1.01317127]  gradients: [-5.17838915948345e-05, -2.8180843335548708e-05, -3.643466669996071e-05]\n",
      "epoch: 850  loss: 0.006929579097406203  params: [0.49592061 0.07029748 1.01320771]  gradients: [-5.1784963845124906e-05, -2.8181426855182324e-05, -3.643542112533792e-05]\n",
      "epoch: 851  loss: 0.006929768784821992  params: [0.49597239 0.07032567 1.01324415]  gradients: [-5.178603606256321e-05, -2.818201035693779e-05, -3.64361755276007e-05]\n",
      "epoch: 852  loss: 0.006929958481738932  params: [0.49602418 0.07035385 1.01328058]  gradients: [-5.178710824714492e-05, -2.818259384081267e-05, -3.6436929906745886e-05]\n",
      "epoch: 853  loss: 0.006930148188156896  params: [0.49607597 0.07038203 1.01331702]  gradients: [-5.178818039886558e-05, -2.8183177306804527e-05, -3.643768426277033e-05]\n",
      "epoch: 854  loss: 0.006930337904075734  params: [0.49612776 0.07041022 1.01335346]  gradients: [-5.178925251772072e-05, -2.818376075491094e-05, -3.64384385956709e-05]\n",
      "epoch: 855  loss: 0.006930527629495334  params: [0.49617955 0.0704384  1.0133899 ]  gradients: [-5.1790324603705864e-05, -2.8184344185129473e-05, -3.643919290544445e-05]\n",
      "epoch: 856  loss: 0.006930717364415552  params: [0.49623134 0.07046659 1.01342634]  gradients: [-5.179139665681655e-05, -2.8184927597457697e-05, -3.643994719208784e-05]\n",
      "epoch: 857  loss: 0.006930907108836242  params: [0.49628313 0.07049477 1.01346278]  gradients: [-5.1792468677048304e-05, -2.8185510991893166e-05, -3.644070145559791e-05]\n",
      "epoch: 858  loss: 0.006931096862757297  params: [0.49633493 0.07052296 1.01349922]  gradients: [-5.179354066439663e-05, -2.818609436843345e-05, -3.644145569597151e-05]\n",
      "epoch: 859  loss: 0.0069312866261785665  params: [0.49638672 0.07055114 1.01353566]  gradients: [-5.17946126188571e-05, -2.8186677727076124e-05, -3.644220991320551e-05]\n",
      "epoch: 860  loss: 0.006931476399099914  params: [0.49643852 0.07057933 1.01357211]  gradients: [-5.179568454042522e-05, -2.8187261067818757e-05, -3.644296410729677e-05]\n",
      "epoch: 861  loss: 0.0069316661815212064  params: [0.49649031 0.07060752 1.01360855]  gradients: [-5.1796756429096535e-05, -2.8187844390658915e-05, -3.644371827824214e-05]\n",
      "epoch: 862  loss: 0.006931855973442316  params: [0.49654211 0.07063571 1.01364499]  gradients: [-5.179782828486657e-05, -2.8188427695594163e-05, -3.644447242603847e-05]\n",
      "epoch: 863  loss: 0.006932045774863108  params: [0.49659391 0.0706639  1.01368144]  gradients: [-5.179890010773085e-05, -2.818901098262207e-05, -3.644522655068262e-05]\n",
      "epoch: 864  loss: 0.006932235585783448  params: [0.49664571 0.07069209 1.01371788]  gradients: [-5.179997189768493e-05, -2.818959425174021e-05, -3.644598065217145e-05]\n",
      "epoch: 865  loss: 0.006932425406203194  params: [0.49669751 0.07072028 1.01375433]  gradients: [-5.180104365472432e-05, -2.819017750294615e-05, -3.644673473050181e-05]\n",
      "epoch: 866  loss: 0.00693261523612222  params: [0.49674931 0.07074847 1.01379078]  gradients: [-5.180211537884457e-05, -2.8190760736237458e-05, -3.644748878567057e-05]\n",
      "epoch: 867  loss: 0.0069328050755403815  params: [0.49680112 0.07077666 1.01382723]  gradients: [-5.18031870700412e-05, -2.8191343951611704e-05, -3.644824281767458e-05]\n",
      "epoch: 868  loss: 0.006932994924457555  params: [0.49685292 0.07080485 1.01386368]  gradients: [-5.1804258728309767e-05, -2.8191927149066466e-05, -3.6448996826510706e-05]\n",
      "epoch: 869  loss: 0.006933184782873605  params: [0.49690473 0.07083304 1.01390013]  gradients: [-5.180533035364579e-05, -2.8192510328599302e-05, -3.6449750812175794e-05]\n",
      "epoch: 870  loss: 0.006933374650788397  params: [0.49695653 0.07086124 1.01393658]  gradients: [-5.180640194604479e-05, -2.8193093490207786e-05, -3.645050477466671e-05]\n",
      "epoch: 871  loss: 0.006933564528201779  params: [0.49700834 0.07088943 1.01397303]  gradients: [-5.1807473505502345e-05, -2.8193676633889498e-05, -3.645125871398032e-05]\n",
      "epoch: 872  loss: 0.0069337544151136426  params: [0.49706015 0.07091762 1.01400948]  gradients: [-5.180854503201395e-05, -2.819425975964199e-05, -3.645201263011346e-05]\n",
      "epoch: 873  loss: 0.006933944311523836  params: [0.49711196 0.07094582 1.01404593]  gradients: [-5.1809616525575165e-05, -2.8194842867462855e-05, -3.645276652306302e-05]\n",
      "epoch: 874  loss: 0.006934134217432237  params: [0.49716377 0.07097401 1.01408239]  gradients: [-5.181068798618153e-05, -2.8195425957349654e-05, -3.645352039282585e-05]\n",
      "epoch: 875  loss: 0.006934324132838699  params: [0.49721558 0.07100221 1.01411884]  gradients: [-5.181175941382856e-05, -2.8196009029299958e-05, -3.64542742393988e-05]\n",
      "epoch: 876  loss: 0.006934514057743084  params: [0.49726739 0.07103041 1.0141553 ]  gradients: [-5.181283080851181e-05, -2.819659208331134e-05, -3.645502806277874e-05]\n",
      "epoch: 877  loss: 0.006934703992145275  params: [0.49731921 0.0710586  1.01419175]  gradients: [-5.181390217022682e-05, -2.819717511938137e-05, -3.645578186296253e-05]\n",
      "epoch: 878  loss: 0.00693489393604512  params: [0.49737102 0.0710868  1.01422821]  gradients: [-5.1814973498969136e-05, -2.8197758137507628e-05, -3.645653563994703e-05]\n",
      "epoch: 879  loss: 0.006935083889442491  params: [0.49742284 0.071115   1.01426467]  gradients: [-5.1816044794734274e-05, -2.819834113768768e-05, -3.645728939372911e-05]\n",
      "epoch: 880  loss: 0.0069352738523372525  params: [0.49747466 0.0711432  1.01430112]  gradients: [-5.1817116057517785e-05, -2.8198924119919094e-05, -3.645804312430561e-05]\n",
      "epoch: 881  loss: 0.006935463824729273  params: [0.49752647 0.0711714  1.01433758]  gradients: [-5.1818187287315225e-05, -2.8199507084199455e-05, -3.6458796831673414e-05]\n",
      "epoch: 882  loss: 0.0069356538066184  params: [0.49757829 0.0711996  1.01437404]  gradients: [-5.1819258484122113e-05, -2.820009003052633e-05, -3.645955051582937e-05]\n",
      "epoch: 883  loss: 0.006935843798004531  params: [0.49763011 0.0712278  1.0144105 ]  gradients: [-5.182032964793401e-05, -2.82006729588973e-05, -3.646030417677036e-05]\n",
      "epoch: 884  loss: 0.006936033798887497  params: [0.49768194 0.071256   1.01444696]  gradients: [-5.1821400778746444e-05, -2.8201255869309928e-05, -3.646105781449324e-05]\n",
      "epoch: 885  loss: 0.0069362238092671846  params: [0.49773376 0.0712842  1.01448342]  gradients: [-5.182247187655495e-05, -2.8201838761761787e-05, -3.646181142899485e-05]\n",
      "epoch: 886  loss: 0.0069364138291434525  params: [0.49778558 0.0713124  1.01451989]  gradients: [-5.1823542941355105e-05, -2.8202421636250472e-05, -3.64625650202721e-05]\n",
      "epoch: 887  loss: 0.00693660385851616  params: [0.49783741 0.07134061 1.01455635]  gradients: [-5.1824613973142405e-05, -2.8203004492773528e-05, -3.646331858832181e-05]\n",
      "epoch: 888  loss: 0.006936793897385177  params: [0.49788923 0.07136881 1.01459281]  gradients: [-5.182568497191243e-05, -2.820358733132855e-05, -3.646407213314086e-05]\n",
      "epoch: 889  loss: 0.0069369839457503624  params: [0.49794106 0.07139702 1.01462928]  gradients: [-5.182675593766071e-05, -2.8204170151913107e-05, -3.646482565472613e-05]\n",
      "epoch: 890  loss: 0.006937174003611587  params: [0.49799289 0.07142522 1.01466575]  gradients: [-5.182782687038279e-05, -2.820475295452478e-05, -3.646557915307446e-05]\n",
      "epoch: 891  loss: 0.006937364070968715  params: [0.49804471 0.07145343 1.01470221]  gradients: [-5.182889777007422e-05, -2.8205335739161138e-05, -3.646633262818275e-05]\n",
      "epoch: 892  loss: 0.006937554147821601  params: [0.49809654 0.07148163 1.01473868]  gradients: [-5.182996863673054e-05, -2.8205918505819754e-05, -3.6467086080047816e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 893  loss: 0.006937744234170122  params: [0.49814838 0.07150984 1.01477515]  gradients: [-5.1831039470347305e-05, -2.8206501254498216e-05, -3.6467839508666565e-05]\n",
      "epoch: 894  loss: 0.006937934330014134  params: [0.49820021 0.07153804 1.01481161]  gradients: [-5.183211027092004e-05, -2.8207083985194087e-05, -3.6468592914035845e-05]\n",
      "epoch: 895  loss: 0.006938124435353505  params: [0.49825204 0.07156625 1.01484808]  gradients: [-5.1833181038444323e-05, -2.8207666697904955e-05, -3.6469346296152547e-05]\n",
      "epoch: 896  loss: 0.0069383145501881035  params: [0.49830388 0.07159446 1.01488455]  gradients: [-5.183425177291568e-05, -2.820824939262839e-05, -3.6470099655013504e-05]\n",
      "epoch: 897  loss: 0.0069385046745177835  params: [0.49835571 0.07162267 1.01492103]  gradients: [-5.183532247432966e-05, -2.820883206936197e-05, -3.64708529906156e-05]\n",
      "epoch: 898  loss: 0.006938694808342412  params: [0.49840755 0.07165088 1.0149575 ]  gradients: [-5.183639314268181e-05, -2.8209414728103273e-05, -3.64716063029557e-05]\n",
      "epoch: 899  loss: 0.006938884951661855  params: [0.49845938 0.07167909 1.01499397]  gradients: [-5.183746377796769e-05, -2.8209997368849876e-05, -3.647235959203068e-05]\n",
      "epoch: 900  loss: 0.006939075104475983  params: [0.49851122 0.0717073  1.01503044]  gradients: [-5.183853438018284e-05, -2.8210579991599357e-05, -3.6473112857837396e-05]\n",
      "epoch: 901  loss: 0.006939265266784648  params: [0.49856306 0.07173551 1.01506692]  gradients: [-5.1839604949322816e-05, -2.82111625963493e-05, -3.647386610037273e-05]\n",
      "epoch: 902  loss: 0.006939455438587713  params: [0.4986149  0.07176372 1.01510339]  gradients: [-5.184067548538316e-05, -2.821174518309727e-05, -3.647461931963354e-05]\n",
      "epoch: 903  loss: 0.00693964561988505  params: [0.49866675 0.07179193 1.01513987]  gradients: [-5.184174598835942e-05, -2.8212327751840852e-05, -3.6475372515616693e-05]\n",
      "epoch: 904  loss: 0.006939835810676524  params: [0.49871859 0.07182015 1.01517634]  gradients: [-5.184281645824716e-05, -2.821291030257763e-05, -3.647612568831907e-05]\n",
      "epoch: 905  loss: 0.006940026010961991  params: [0.49877043 0.07184836 1.01521282]  gradients: [-5.184388689504192e-05, -2.821349283530518e-05, -3.6476878837737536e-05]\n",
      "epoch: 906  loss: 0.00694021622074132  params: [0.49882228 0.07187658 1.0152493 ]  gradients: [-5.184495729873926e-05, -2.8214075350021078e-05, -3.647763196386897e-05]\n",
      "epoch: 907  loss: 0.006940406440014374  params: [0.49887412 0.07190479 1.01528578]  gradients: [-5.184602766933473e-05, -2.8214657846722908e-05, -3.647838506671022e-05]\n",
      "epoch: 908  loss: 0.006940596668781015  params: [0.49892597 0.07193301 1.01532225]  gradients: [-5.184709800682388e-05, -2.8215240325408245e-05, -3.647913814625818e-05]\n",
      "epoch: 909  loss: 0.006940786907041105  params: [0.49897782 0.07196122 1.01535873]  gradients: [-5.184816831120226e-05, -2.821582278607467e-05, -3.6479891202509706e-05]\n",
      "epoch: 910  loss: 0.006940977154794516  params: [0.49902967 0.07198944 1.01539521]  gradients: [-5.1849238582465434e-05, -2.8216405228719774e-05, -3.648064423546168e-05]\n",
      "epoch: 911  loss: 0.0069411674120410955  params: [0.49908152 0.07201765 1.0154317 ]  gradients: [-5.185030882060895e-05, -2.8216987653341122e-05, -3.648139724511096e-05]\n",
      "epoch: 912  loss: 0.0069413576787807196  params: [0.49913337 0.07204587 1.01546818]  gradients: [-5.1851379025628356e-05, -2.8217570059936302e-05, -3.6482150231454434e-05]\n",
      "epoch: 913  loss: 0.006941547955013247  params: [0.49918522 0.07207409 1.01550466]  gradients: [-5.1852449197519206e-05, -2.8218152448502887e-05, -3.648290319448896e-05]\n",
      "epoch: 914  loss: 0.0069417382407385405  params: [0.49923707 0.07210231 1.01554114]  gradients: [-5.1853519336277074e-05, -2.8218734819038472e-05, -3.648365613421143e-05]\n",
      "epoch: 915  loss: 0.006941928535956463  params: [0.49928893 0.07213053 1.01557763]  gradients: [-5.18545894418975e-05, -2.821931717154063e-05, -3.648440905061869e-05]\n",
      "epoch: 916  loss: 0.006942118840666887  params: [0.49934079 0.07215875 1.01561411]  gradients: [-5.185565951437604e-05, -2.821989950600694e-05, -3.648516194370764e-05]\n",
      "epoch: 917  loss: 0.00694230915486966  params: [0.49939264 0.07218697 1.0156506 ]  gradients: [-5.1856729553708264e-05, -2.8220481822434992e-05, -3.648591481347514e-05]\n",
      "epoch: 918  loss: 0.006942499478564656  params: [0.4994445  0.07221519 1.01568709]  gradients: [-5.1857799559889714e-05, -2.8221064120822367e-05, -3.648666765991807e-05]\n",
      "epoch: 919  loss: 0.006942689811751734  params: [0.49949636 0.07224341 1.01572357]  gradients: [-5.185886953291596e-05, -2.8221646401166645e-05, -3.64874204830333e-05]\n",
      "epoch: 920  loss: 0.006942880154430752  params: [0.49954822 0.07227163 1.01576006]  gradients: [-5.185993947278254e-05, -2.82222286634654e-05, -3.64881732828177e-05]\n",
      "epoch: 921  loss: 0.00694307050660159  params: [0.49960008 0.07229986 1.01579655]  gradients: [-5.186100937948504e-05, -2.822281090771623e-05, -3.6488926059268156e-05]\n",
      "epoch: 922  loss: 0.006943260868264081  params: [0.49965194 0.07232808 1.01583304]  gradients: [-5.1862079253018996e-05, -2.8223393133916714e-05, -3.648967881238153e-05]\n",
      "epoch: 923  loss: 0.006943451239418123  params: [0.4997038  0.0723563  1.01586953]  gradients: [-5.186314909337997e-05, -2.8223975342064424e-05, -3.649043154215471e-05]\n",
      "epoch: 924  loss: 0.00694364162006355  params: [0.49975567 0.07238453 1.01590602]  gradients: [-5.1864218900563546e-05, -2.822455753215696e-05, -3.649118424858457e-05]\n",
      "epoch: 925  loss: 0.006943832010200242  params: [0.49980753 0.07241275 1.01594251]  gradients: [-5.186528867456524e-05, -2.822513970419189e-05, -3.649193693166797e-05]\n",
      "epoch: 926  loss: 0.006944022409828052  params: [0.4998594  0.07244098 1.01597901]  gradients: [-5.186635841538066e-05, -2.8225721858166813e-05, -3.649268959140182e-05]\n",
      "epoch: 927  loss: 0.0069442128189468495  params: [0.49991127 0.07246921 1.0160155 ]  gradients: [-5.186742812300535e-05, -2.822630399407931e-05, -3.649344222778297e-05]\n",
      "epoch: 928  loss: 0.00694440323755649  params: [0.49996314 0.07249743 1.016052  ]  gradients: [-5.186849779743486e-05, -2.8226886111926958e-05, -3.6494194840808296e-05]\n",
      "epoch: 929  loss: 0.006944593665656844  params: [0.50001501 0.07252566 1.01608849]  gradients: [-5.1869567438664755e-05, -2.8227468211707347e-05, -3.64949474304747e-05]\n",
      "epoch: 930  loss: 0.006944784103247768  params: [0.50006688 0.07255389 1.01612499]  gradients: [-5.18706370466906e-05, -2.822805029341806e-05, -3.649569999677902e-05]\n",
      "epoch: 931  loss: 0.006944974550329122  params: [0.50011875 0.07258212 1.01616148]  gradients: [-5.187170662150797e-05, -2.8228632357056683e-05, -3.6496452539718166e-05]\n",
      "epoch: 932  loss: 0.006945165006900774  params: [0.50017062 0.07261035 1.01619798]  gradients: [-5.187277616311241e-05, -2.8229214402620803e-05, -3.649720505928902e-05]\n",
      "epoch: 933  loss: 0.0069453554729625765  params: [0.5002225  0.07263858 1.01623448]  gradients: [-5.187384567149951e-05, -2.8229796430108014e-05, -3.6497957555488446e-05]\n",
      "epoch: 934  loss: 0.0069455459485144155  params: [0.50027437 0.07266681 1.01627098]  gradients: [-5.18749151466648e-05, -2.8230378439515882e-05, -3.649871002831331e-05]\n",
      "epoch: 935  loss: 0.006945736433556126  params: [0.50032625 0.07269504 1.01630748]  gradients: [-5.187598458860387e-05, -2.823096043084201e-05, -3.6499462477760526e-05]\n",
      "epoch: 936  loss: 0.006945926928087583  params: [0.50037812 0.07272327 1.01634398]  gradients: [-5.187705399731227e-05, -2.8231542404083976e-05, -3.650021490382695e-05]\n",
      "epoch: 937  loss: 0.0069461174321086465  params: [0.50043    0.0727515  1.01638048]  gradients: [-5.1878123372785567e-05, -2.823212435923937e-05, -3.6500967306509455e-05]\n",
      "epoch: 938  loss: 0.006946307945619184  params: [0.50048188 0.07277973 1.01641698]  gradients: [-5.187919271501935e-05, -2.8232706296305787e-05, -3.650171968580495e-05]\n",
      "epoch: 939  loss: 0.006946498468619045  params: [0.50053376 0.07280797 1.01645348]  gradients: [-5.188026202400915e-05, -2.8233288215280796e-05, -3.650247204171029e-05]\n",
      "epoch: 940  loss: 0.006946689001108101  params: [0.50058564 0.0728362  1.01648998]  gradients: [-5.188133129975056e-05, -2.8233870116162e-05, -3.650322437422236e-05]\n",
      "epoch: 941  loss: 0.006946879543086209  params: [0.50063752 0.07286443 1.01652649]  gradients: [-5.188240054223915e-05, -2.8234451998946983e-05, -3.650397668333805e-05]\n",
      "epoch: 942  loss: 0.006947070094553241  params: [0.50068941 0.07289267 1.01656299]  gradients: [-5.1883469751470454e-05, -2.8235033863633326e-05, -3.650472896905424e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 943  loss: 0.00694726065550904  params: [0.50074129 0.07292091 1.0165995 ]  gradients: [-5.188453892744009e-05, -2.8235615710218633e-05, -3.650548123136782e-05]\n",
      "epoch: 944  loss: 0.006947451225953487  params: [0.50079318 0.07294914 1.016636  ]  gradients: [-5.188560807014358e-05, -2.8236197538700477e-05, -3.650623347027565e-05]\n",
      "epoch: 945  loss: 0.0069476418058864306  params: [0.50084506 0.07297738 1.01667251]  gradients: [-5.1886677179576524e-05, -2.823677934907645e-05, -3.6506985685774625e-05]\n",
      "epoch: 946  loss: 0.006947832395307733  params: [0.50089695 0.07300562 1.01670902]  gradients: [-5.188774625573448e-05, -2.8237361141344146e-05, -3.650773787786164e-05]\n",
      "epoch: 947  loss: 0.0069480229942172666  params: [0.50094884 0.07303385 1.01674553]  gradients: [-5.188881529861302e-05, -2.8237942915501156e-05, -3.650849004653355e-05]\n",
      "epoch: 948  loss: 0.006948213602614879  params: [0.50100073 0.07306209 1.01678204]  gradients: [-5.1889884308207706e-05, -2.8238524671545057e-05, -3.6509242191787254e-05]\n",
      "epoch: 949  loss: 0.006948404220500445  params: [0.50105262 0.07309033 1.01681855]  gradients: [-5.189095328451413e-05, -2.8239106409473455e-05, -3.6509994313619646e-05]\n",
      "epoch: 950  loss: 0.0069485948478738135  params: [0.50110451 0.07311857 1.01685506]  gradients: [-5.1892022227527846e-05, -2.823968812928393e-05, -3.65107464120276e-05]\n",
      "epoch: 951  loss: 0.006948785484734851  params: [0.50115641 0.07314681 1.01689157]  gradients: [-5.1893091137244425e-05, -2.8240269830974066e-05, -3.6511498487007995e-05]\n",
      "epoch: 952  loss: 0.006948976131083425  params: [0.5012083  0.07317505 1.01692808]  gradients: [-5.1894160013659456e-05, -2.8240851514541475e-05, -3.6512250538557726e-05]\n",
      "epoch: 953  loss: 0.0069491667869193904  params: [0.5012602  0.07320329 1.01696459]  gradients: [-5.189522885676849e-05, -2.8241433179983728e-05, -3.651300256667367e-05]\n",
      "epoch: 954  loss: 0.006949357452242604  params: [0.50131209 0.07323154 1.01700111]  gradients: [-5.1896297666567114e-05, -2.824201482729842e-05, -3.6513754571352715e-05]\n",
      "epoch: 955  loss: 0.006949548127052937  params: [0.50136399 0.07325978 1.01703762]  gradients: [-5.1897366443050906e-05, -2.824259645648315e-05, -3.651450655259175e-05]\n",
      "epoch: 956  loss: 0.006949738811350241  params: [0.50141589 0.07328802 1.01707414]  gradients: [-5.189843518621543e-05, -2.8243178067535503e-05, -3.651525851038766e-05]\n",
      "epoch: 957  loss: 0.006949929505134385  params: [0.50146779 0.07331626 1.01711065]  gradients: [-5.189950389605627e-05, -2.8243759660453073e-05, -3.6516010444737334e-05]\n",
      "epoch: 958  loss: 0.006950120208405224  params: [0.50151969 0.07334451 1.01714717]  gradients: [-5.190057257256898e-05, -2.824434123523345e-05, -3.651676235563765e-05]\n",
      "epoch: 959  loss: 0.006950310921162628  params: [0.50157159 0.07337275 1.01718369]  gradients: [-5.190164121574916e-05, -2.8244922791874224e-05, -3.65175142430855e-05]\n",
      "epoch: 960  loss: 0.00695050164340644  params: [0.50162349 0.073401   1.01722021]  gradients: [-5.190270982559238e-05, -2.8245504330372994e-05, -3.651826610707777e-05]\n",
      "epoch: 961  loss: 0.006950692375136536  params: [0.5016754  0.07342925 1.01725673]  gradients: [-5.190377840209421e-05, -2.8246085850727355e-05, -3.651901794761136e-05]\n",
      "epoch: 962  loss: 0.006950883116352774  params: [0.5017273  0.07345749 1.01729325]  gradients: [-5.190484694525024e-05, -2.824666735293489e-05, -3.651976976468314e-05]\n",
      "epoch: 963  loss: 0.006951073867055016  params: [0.50177921 0.07348574 1.01732977]  gradients: [-5.1905915455056025e-05, -2.8247248836993192e-05, -3.652052155829e-05]\n",
      "epoch: 964  loss: 0.006951264627243117  params: [0.50183111 0.07351399 1.01736629]  gradients: [-5.190698393150715e-05, -2.8247830302899862e-05, -3.652127332842884e-05]\n",
      "epoch: 965  loss: 0.00695145539691694  params: [0.50188302 0.07354224 1.01740281]  gradients: [-5.1908052374599224e-05, -2.8248411750652496e-05, -3.652202507509655e-05]\n",
      "epoch: 966  loss: 0.006951646176076345  params: [0.50193493 0.07357049 1.01743933]  gradients: [-5.190912078432778e-05, -2.8248993180248676e-05, -3.652277679828999e-05]\n",
      "epoch: 967  loss: 0.0069518369647212025  params: [0.50198684 0.07359873 1.01747586]  gradients: [-5.1910189160688425e-05, -2.824957459168601e-05, -3.6523528498006096e-05]\n",
      "epoch: 968  loss: 0.006952027762851357  params: [0.50203875 0.07362698 1.01751238]  gradients: [-5.1911257503676725e-05, -2.8250155984962078e-05, -3.652428017424172e-05]\n",
      "epoch: 969  loss: 0.006952218570466675  params: [0.50209067 0.07365524 1.0175489 ]  gradients: [-5.1912325813288275e-05, -2.8250737360074487e-05, -3.6525031826993765e-05]\n",
      "epoch: 970  loss: 0.006952409387567019  params: [0.50214258 0.07368349 1.01758543]  gradients: [-5.191339408951864e-05, -2.825131871702082e-05, -3.652578345625913e-05]\n",
      "epoch: 971  loss: 0.0069526002141522495  params: [0.50219449 0.07371174 1.01762196]  gradients: [-5.191446233236341e-05, -2.8251900055798688e-05, -3.652653506203469e-05]\n",
      "epoch: 972  loss: 0.006952791050222222  params: [0.50224641 0.07373999 1.01765848]  gradients: [-5.191553054181817e-05, -2.8252481376405676e-05, -3.652728664431735e-05]\n",
      "epoch: 973  loss: 0.006952981895776807  params: [0.50229833 0.07376824 1.01769501]  gradients: [-5.19165987178785e-05, -2.8253062678839382e-05, -3.6528038203104e-05]\n",
      "epoch: 974  loss: 0.006953172750815855  params: [0.50235024 0.0737965  1.01773154]  gradients: [-5.1917666860539965e-05, -2.8253643963097397e-05, -3.652878973839152e-05]\n",
      "epoch: 975  loss: 0.0069533636153392255  params: [0.50240216 0.07382475 1.01776807]  gradients: [-5.191873496979817e-05, -2.8254225229177327e-05, -3.652954125017681e-05]\n",
      "epoch: 976  loss: 0.006953554489346786  params: [0.50245408 0.07385301 1.0178046 ]  gradients: [-5.1919803045648685e-05, -2.825480647707675e-05, -3.6530292738456764e-05]\n",
      "epoch: 977  loss: 0.006953745372838389  params: [0.502506   0.07388126 1.01784113]  gradients: [-5.1920871088087103e-05, -2.8255387706793292e-05, -3.6531044203228276e-05]\n",
      "epoch: 978  loss: 0.0069539362658139  params: [0.50255792 0.07390952 1.01787766]  gradients: [-5.192193909710901e-05, -2.8255968918324527e-05, -3.6531795644488235e-05]\n",
      "epoch: 979  loss: 0.006954127168273173  params: [0.50260985 0.07393777 1.0179142 ]  gradients: [-5.192300707270997e-05, -2.8256550111668056e-05, -3.653254706223352e-05]\n",
      "epoch: 980  loss: 0.006954318080216077  params: [0.50266177 0.07396603 1.01795073]  gradients: [-5.19240750148856e-05, -2.825713128682149e-05, -3.653329845646107e-05]\n",
      "epoch: 981  loss: 0.006954509001642464  params: [0.5027137  0.07399429 1.01798726]  gradients: [-5.1925142923631455e-05, -2.8257712443782403e-05, -3.653404982716773e-05]\n",
      "epoch: 982  loss: 0.006954699932552203  params: [0.50276562 0.07402255 1.0180238 ]  gradients: [-5.192621079894314e-05, -2.8258293582548408e-05, -3.653480117435041e-05]\n",
      "epoch: 983  loss: 0.006954890872945139  params: [0.50281755 0.07405081 1.01806033]  gradients: [-5.192727864081624e-05, -2.8258874703117103e-05, -3.653555249800602e-05]\n",
      "epoch: 984  loss: 0.006955081822821136  params: [0.50286948 0.07407907 1.01809687]  gradients: [-5.192834644924633e-05, -2.8259455805486086e-05, -3.653630379813144e-05]\n",
      "epoch: 985  loss: 0.006955272782180058  params: [0.50292141 0.07410733 1.01813341]  gradients: [-5.192941422422901e-05, -2.8260036889652957e-05, -3.653705507472357e-05]\n",
      "epoch: 986  loss: 0.006955463751021775  params: [0.50297334 0.07413559 1.01816995]  gradients: [-5.193048196575986e-05, -2.8260617955615305e-05, -3.65378063277793e-05]\n",
      "epoch: 987  loss: 0.006955654729346129  params: [0.50302527 0.07416385 1.01820648]  gradients: [-5.1931549673834466e-05, -2.8261199003370734e-05, -3.653855755729552e-05]\n",
      "epoch: 988  loss: 0.006955845717152984  params: [0.5030772  0.07419211 1.01824302]  gradients: [-5.193261734844844e-05, -2.8261780032916852e-05, -3.6539308763269154e-05]\n",
      "epoch: 989  loss: 0.006956036714442203  params: [0.50312914 0.07422037 1.01827956]  gradients: [-5.1933684989597344e-05, -2.826236104425125e-05, -3.6540059945697073e-05]\n",
      "epoch: 990  loss: 0.006956227721213639  params: [0.50318107 0.07424864 1.0183161 ]  gradients: [-5.193475259727678e-05, -2.8262942037371532e-05, -3.654081110457619e-05]\n",
      "epoch: 991  loss: 0.006956418737467159  params: [0.50323301 0.0742769  1.01835265]  gradients: [-5.193582017148233e-05, -2.8263523012275294e-05, -3.654156223990339e-05]\n",
      "epoch: 992  loss: 0.006956609763202624  params: [0.50328494 0.07430516 1.01838919]  gradients: [-5.193688771220959e-05, -2.8264103968960136e-05, -3.654231335167557e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 993  loss: 0.006956800798419881  params: [0.50333688 0.07433343 1.01842573]  gradients: [-5.193795521945414e-05, -2.826468490742366e-05, -3.654306443988964e-05]\n",
      "epoch: 994  loss: 0.0069569918431188  params: [0.50338882 0.07436169 1.01846227]  gradients: [-5.193902269321159e-05, -2.826526582766347e-05, -3.654381550454248e-05]\n",
      "epoch: 995  loss: 0.006957182897299227  params: [0.50344076 0.07438996 1.01849882]  gradients: [-5.194009013347754e-05, -2.826584672967717e-05, -3.6544566545631014e-05]\n",
      "epoch: 996  loss: 0.0069573739609610444  params: [0.5034927  0.07441823 1.01853536]  gradients: [-5.194115754024755e-05, -2.8266427613462346e-05, -3.6545317563152115e-05]\n",
      "epoch: 997  loss: 0.006957565034104087  params: [0.50354464 0.07444649 1.01857191]  gradients: [-5.1942224913517225e-05, -2.8267008479016618e-05, -3.6546068557102695e-05]\n",
      "epoch: 998  loss: 0.00695775611672823  params: [0.50359659 0.07447476 1.01860846]  gradients: [-5.1943292253282176e-05, -2.8267589326337576e-05, -3.6546819527479664e-05]\n",
      "epoch: 999  loss: 0.006957947208833322  params: [0.50364853 0.07450303 1.01864501]  gradients: [-5.194435955953799e-05, -2.8268170155422834e-05, -3.6547570474279904e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50364853, 0.07450303, 1.01864501])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.01, num_epoch=1000, tolerance = 0.00001, model = 'logistic',batch_size=len(X_train))\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 128.21976796767524  params: [0.44677413 0.36806029 0.77485095]  gradients: [-0.007646876743192717, -0.004161437637639053, -0.005380271680116438]\n",
      "epoch: 1  loss: 170.4052663231031  params: [0.95509471 0.49901766 1.25764628]  gradients: [-0.00890472716044321, -0.0048459610378005545, -0.006265283588250042]\n",
      "epoch: 2  loss: 229.25240652598927  params: [1.53975367 0.67171237 1.78785461]  gradients: [-0.009587526008535841, -0.005217540824007874, -0.006745694536272606]\n",
      "epoch: 3  loss: 299.5030412092467  params: [2.17472324 0.86138208 2.33764975]  gradients: [-0.009862187854146207, -0.005367012063094324, -0.0069389440679668984]\n",
      "epoch: 4  loss: 376.6362895977714  params: [2.8446597  1.05412179 2.89323345]  gradients: [-0.009956625932825432, -0.005418405355838567, -0.007005389825797997]\n",
      "epoch: 5  loss: 458.0139851805435  params: [3.54097991 1.24308791 3.44829577]  gradients: [-0.009986759964875915, -0.005434804324902682, -0.007026591851761508]\n",
      "epoch: 6  loss: 542.1962492536114  params: [4.25833474 1.42519499 3.99988848]  gradients: [-0.009996026544739965, -0.0054398472065278845, -0.007033111731561907]\n",
      "epoch: 7  loss: 628.3466100305501  params: [4.99299079 1.59925149 4.54654766]  gradients: [-0.009998819523305773, -0.005441367148134792, -0.007035076845422825]\n",
      "epoch: 8  loss: 715.9361866593971  params: [5.74224311 1.76491551 5.08745556]  gradients: [-0.009999651452794812, -0.005441819884958441, -0.007035662183309136]\n",
      "epoch: 9  loss: 804.6136209400069  params: [6.50419773 1.92211261 5.6220452 ]  gradients: [-0.009999897469023645, -0.00544195376722548, -0.007035835278050073]\n",
      "epoch: 10  loss: 894.1453226561525  params: [7.27764138 2.07074257 6.14980669]  gradients: [-0.00999996990027575, -0.005441993184382153, -0.00703588624000847]\n",
      "epoch: 11  loss: 984.382803221934  params: [8.06191669 2.21056482 6.67019561]  gradients: [-0.009999991171274704, -0.0054420047600801285, -0.007035901206086402]\n",
      "epoch: 12  loss: 1075.2418739527395  params: [8.85679397 2.34118526 7.18259834]  gradients: [-0.009999997410335274, -0.005442008155382858, -0.007035905595831658]\n",
      "epoch: 13  loss: 1166.696957500531  params: [9.6623461  2.4620958  7.68633213]  gradients: [-0.009999999239854633, -0.005442009151009043, -0.00703590688306454]\n",
      "epoch: 14  loss: 1258.7856425079287  params: [10.47883187  2.57273757  8.18066747]  gradients: [-0.009999999776591297, -0.0054420094431016485, -0.007035907260707488]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-76882a74ec97>:3: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(y*np.log(p)+(1-y)*np.log(1-p))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15  loss: inf  params: [11.30659196  2.67256983  8.66486358]  gradients: [-0.009999999934225116, -0.0054420095288861235, -0.007035907371617184]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12.14596255,  2.76113136,  9.13820805])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.01, num_epoch=1000, tolerance = 0.00001, model = 'logistic',batch_size=1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6393037421493166  params: [0.45738292 0.6258771  0.79269147]  gradients: [-0.0004968234688635318, -0.0002703718069513273, -0.00034956039299616383]\n",
      "epoch: 100  loss: 0.7372950738316901  params: [0.53173027 0.69334317 1.01864512]  gradients: [-0.0005221619416945328, -0.0002841610281013987, -0.0003673883078911429]\n",
      "epoch: 200  loss: 0.8514100464389709  params: [0.61613774 0.78067015 1.26290055]  gradients: [-0.0005456816952607084, -0.00029696050048800513, -0.0003839365887494764]\n",
      "epoch: 300  loss: 0.9798907318786775  params: [0.70926701 0.88563585 1.52244612]  gradients: [-0.0005660181315143608, -0.00030802760854839505, -0.00039824511701854013]\n",
      "epoch: 400  loss: 1.1203703235943987  params: [0.80960186 1.00535116 1.79403112]  gradients: [-0.0005825043614152328, -0.00031699943062900245, -0.00040984467574380816]\n",
      "epoch: 500  loss: 1.270391037645301  params: [0.91569737 1.1368042  2.0746623 ]  gradients: [-0.000595160396005828, -0.00032388685675829756, -0.00041874934450951976]\n",
      "epoch: 600  loss: 1.4277383702633806  params: [1.02630089 1.27724065 2.36185913]  gradients: [-0.000604462278161134, -0.00032894894992416854, -0.00042529406267521067]\n",
      "epoch: 700  loss: 1.5905826794849895  params: [1.1403831  1.42435207 2.65370758]  gradients: [-0.0006110751697228062, -0.00033254769183705025, -0.00042994683195446165]\n",
      "epoch: 800  loss: 1.7574925410324265  params: [1.25712457 1.57631733 2.94880011]  gradients: [-0.0006156619783024637, -0.00033504383745323586, -0.00043317406800546957]\n",
      "epoch: 900  loss: 1.927385735295626  params: [1.37588606 1.73175937 3.24613397]  gradients: [-0.0006187872175500577, -0.0003367445956409682, -0.0004353729574059542]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.4949665 , 1.88807835, 3.54201435])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.01, num_epoch=1000, tolerance = 0.00001, model = 'logistic',batch_size=16)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 29],\n",
       "       [ 3,  7]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49679769, 2.72087602])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.3981069868592975  params: [0.79243276 0.43519711]  gradients: [-1.8357427857806395e-05, -2.9537913226702327e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.79245112, 0.43520007])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate = 0.01, num_epoch = 1000, tolerance = 0.00001, model= 'linear',batch_size=len(X))\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
